[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Work",
    "section": "",
    "text": "Data Visualization Examples in Python\n\n\n\n\n\n\n\nData Visualization\n\n\nPython\n\n\n\n\nUsing Python libraries to visualize data\n\n\n\n\n\n\nAug 19, 2023\n\n\nElke Windschitl\n\n\n\n\n\n\n  \n\n\n\n\nDeveloping a Data Pipeline for Kelp Forest Modeling\n\n\n\n\n\n\n\nMEDS\n\n\n\n\nA Bren capstone project\n\n\n\n\n\n\nJul 2, 2023\n\n\nErika Egg, Jessica French, Javier Patrón, Elke Windschitl\n\n\n\n\n\n\n  \n\n\n\n\nLearning Flourish to Visualize Ocean Depth Data\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\nA brief tutorial on how to use Flourish to visualize depth data\n\n\n\n\n\n\nJun 25, 2023\n\n\nElke Windschitl\n\n\n\n\n\n\n  \n\n\n\n\nIntegrating Art into Science and Conservation\n\n\n\n\n\n\n\nArt\n\n\n\n\nHow I use creative skills in parallel with quantitative tasks\n\n\n\n\n\n\nMar 29, 2023\n\n\nElke Windschitl\n\n\n\n\n\n\n  \n\n\n\n\nDebating Nudging and AI for Climate\n\n\n\n\n\n\n\nMEDS\n\n\n\n\nA short podcast debate by Lewis White and Elke Windschitl\n\n\n\n\n\n\nDec 19, 2022\n\n\nElke Windschitl, Lewis White\n\n\n\n\n\n\n  \n\n\n\n\nIdentifying Key Traits in Hawaiian Fish that Predict Risk of Extinction\n\n\n\n\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\n\n\nA logistic regression example with fish\n\n\n\n\n\n\nDec 2, 2022\n\n\nElke Windschitl\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating the Use of Autonomous Recording Units for Monitoring Northern Bobwhite Coveys\n\n\n\n\n\n\n\nISU\n\n\n\n\nA research poster created for an undergraduate Honors capstone project\n\n\n\n\n\n\nDec 1, 2019\n\n\nElke Windschitl, Adam Janke, Kyla Yuza-Pate\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "",
    "text": "Description:\nIn this post, I investigate Hawaiian fish ecological traits – such as size, endemism, and reef-association – to find their probability of being threatened as ranked by the IUCN Red List. For my full analysis, check out my github repository."
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html#introduction",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html#introduction",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "Introduction",
    "text": "Introduction\nGlobal human activity threatens many species with extinction. According to the International Union and Conservation of Nature (IUCN), “More than 41,000 species are threatened with extinction. That is still 28% of all assessed species.” [1]. Increased extinction and loss of biodiversity can have severe ecological, economic, and cultural impacts. Cardinale et al.’s deep dive into biodiversity and ecosystem services research conclude that biodiversity loss reduces ecological communities’ efficiency, stability, and productivity. Decreased productivity from ecosystem services can have a negative impact on ecosystem economics [2]. Additionally, cultures worldwide have strong ties to local flora and fauna, much of which now face extinction risk. Improving understanding of extinction risk is ecologically, economically, and culturally important.\nWildlife scientists have been working to understand what ecological traits of vertebrates predict threat level, and what common risk factors drive those threat level rates. Munstermann et al. investigate what terrestrial vertebrate functional groups are most at risk of extinction threat and find that cave dwelling amphibian, arboreal quadrupedal mammals, aerial and scavenging birds, and pedal squamates are at high risk [3]. This knowledge can help inform policies and practices with the goal to decrease threats of extinction of wildlife. However, less comprehensive research has been done to conduct similar analyses on marine species.\nIn recent years, the waters surrounding the Hawaiian Islands have been exposed to ecological changes due to mass coral bleaching events, El Niño events, and pollution. Rapidly changing marine ecosystems may pose a threat to Hawaiian fish. Fish hold significant cultural value in Hawaiʻi, and many local people rely on seafood as a major source of protein. However, approximately 72% of fish in Hawaiʻi present in FishBase have been evaluated by the IUCN and have sufficient data to be assessed. Here I run a small-scale analysis to investigate Hawaiian fish ecological traits – such as endemism, size, and reef-association – to predict a binary status on the IUCN red list and predict which unevaluated fish species in Hawaiʻi may be threatened.\n\n\n\nVarious fish found in Hawaiʻi. Elke Windschitl 2018."
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html#data",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html#data",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "Data",
    "text": "Data\nFor my analyses I use the IUCN Red List data accessed via the IUCN Red List API [1] and package rredlist [4]. Consistent with Munstermann et al., living species listed as ‘Vulnerable’, ‘Endangered’, or ‘Critically Endangered’ were categorized as ‘Threatened’. Living species listed as ‘Least Concern’ and ‘Near Threatened’ were categorized as ‘Nonthreatened’ [3]. Extinct species were not evaluated in this analysis. The IUCN Red List data are limited in that many marine species have not been listed yet or have been identified as too data deficient to be evaluated. The lack of data on elusive fish may introduce bias into the model.\nFish ecological data were accessed from FishBase [5] via package rfishbase [6]. Different species in the FishBase data were originally described by different people, possibly leading to errors or biases. Measurement errors in length may be present, as there are various common ways to measure the length of a fish. The species recorded in FishBase may be biased towards fish with commercial value. Data were wrangled in R and formatted in a tidy data table (Table 1.)\n\n\nCode\ntab_df(tidy_fish_data[1:5,],\n       title = \"Tbl 1. Hawaii Fish Data\",\n       col.header = c(\"Genus species\", \"Length (cm)\", \"IUCN Category\", \n                          \"Common Name\", \"Reef Association\", \"Endemic\",\n                          \"Threatened\", \"Threatened Binary\"))\n\n\n\nTbl 1. Hawaii Fish Data\n\n\nGenus species\nLength (cm)\nIUCN Category\nCommon Name\nReef Association\nEndemic\nThreatened\nThreatened Binary\n\n\nOreochromis mossambicus\n39.00\nVU\nMozambique Tilapia\nno\nno\nyes\n1\n\n\nCoryphaena hippurus\n210.00\nLC\nCommon Dolphinfish\nyes\nno\nno\n0\n\n\nCoryphaena equiselis\n145.70\nLC\nPompano Dolphinfish\nno\nno\nno\n0\n\n\nAlectis indica\n165.00\nLC\nIndian Threadfish\nyes\nno\nno\n0\n\n\nArgyropelecus affinis\n8.40\nLC\nPacific Hatchet Fish\nno\nno\nno\n0"
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html#methods",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html#methods",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "Methods",
    "text": "Methods\nHere I run a logistic regression with a categorical binary response variable of ‘Threatened’ or ‘Nonthreatened’ on species length, coral reef association, and endemism.\nI included length in the model because previous research show that species of extreme sizes have higher risk of extinction. Ripple et al. “found that the probability of being threatened was positively and significantly related to body mass for birds, cartilaginous fishes, and mammals” [7]. While body mass is not the same as length, the FishBase data set had few weight entries and many length entries, and sample size was already limited. Several mass coral bleaching events have occurred in Hawaiʻi in recent decades causing ecosystem disruption [8]. Here I consider if reef-associated fish are more likely to be threatened than fish that are not reef-associated. Last, endemic species – species that are native to a region and occur only in that region – are known to be at high risk of extinction."
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html#results-discussion",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html#results-discussion",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "Results & Discussion",
    "text": "Results & Discussion\n\nData exploration:\n\n1. IUCN Red List Threatened Status\n\n\nCode\nthreat_tab &lt;- table(tidy_fish_data$is_threatened)\ntab_df(threat_tab,\n       title  = \"Tbl 2. Counts of threatened species in the data frame\",\n       col.header = \"Nonthreatened\", \"Threatened\")\n\n\n\nTbl 2. Counts of threatened species in the data frame\n\n\nno\nyes\n\n\n880\n34\n\n\n\n\n\n\n\n\n\n2. Fish Length\n\n\nCode\nggplot(tidy_fish_data, aes(x = length_cm)) +\n  geom_histogram(fill = \"#38b6ba\", bins = 60) +\n  theme_minimal() +\n  labs(title = \"Fig 1. Histogram of species length\") +\n  theme(panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(colour = '#c9c9c9'),\n        panel.border = element_rect(colour = \"black\", fill=NA, size=1)) +\n  xlab(\"Length (cm)\") +\n  ylab(\"Count\")\n\n\n\n\n\n\n\n3. Reef-Association\n\n\nCode\nreef_tab &lt;- table(tidy_fish_data$reef_associated)\ntab_df(reef_tab,\n       title = \"Tbl 3. Counts of reef-associated species in the data frame\")\n\n\n\nTbl 3. Counts of reef-associated species in the data frame\n\n\nno\nyes\n\n\n353\n312\n\n\n\n\n\n\n\n\n\n4. Endemism\n\n\nCode\nend_tab &lt;- table(tidy_fish_data$is_endemic)\ntab_df(end_tab,\n       title = \"Tbl 4. Counts of endemic species in the data frame\")\n\n\n\nTbl 4. Counts of endemic species in the data frame\n\n\nno\nyes\n\n\n895\n19\n\n\n\n\n\n\n\nAfter aligning the FishBase data with the IUCN Red List data, the data are disproportionate in both the threat level (Table 2) and endemism (Table 4). Fish length is skewed right (Figure 1), and reef-association is well balanced (Table 3).\n\n\n\nAnalysis:\n\nLength \\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Length)  +\\varepsilon \\]\n\n\nCode\nrm_len_na &lt;- tidy_fish_data %&gt;% \n  filter(!length_cm == \"NA\") # Remove NA length values\n# Plot threat prob. vs. length\ngg_len &lt;- ggplot(data = rm_len_na, aes(x = length_cm, \n                             y = is_of_concern)) +\n  geom_jitter(width = 0, height = 0.05, \n              alpha = 0.8, col = \"#38b6ba\") +\n  theme_minimal() +\n  labs(x = \"Species length (cm)\", \n       y = \"Listed as threatened\", \n       title = \"Fig 2. Probability of being threatened by species length\") +\n  theme(panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(colour = '#c9c9c9'),\n        panel.border = element_rect(colour = \"black\", fill=NA, size=1))\ngg_len + geom_smooth(method = \"glm\", \n              se = FALSE, color = \"#545454\", \n              method.args = list(family = \"binomial\"))\n\n\n\n\n\nCode\n# Log regression length\nmod_length &lt;- glm(is_of_concern ~ length_cm, \n                  data = rm_len_na, \n                  family = \"binomial\")\n# Model output table format\ntab_model(mod_length,\n          transform = NULL,\n          pred.labels = c(\"Intercept\", \"Length (cm)\"),\n          dv.labels = c(\"log Threat Pobability\"),\n          show.p = TRUE,\n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          string.p = \"P-value\",\n          show.r2 = FALSE,\n          title = \"Tbl 5. Logisitc Regression Model Results for Length\",\n          digits = 3)\n\n\n\nTbl 5. Logisitc Regression Model Results for Length\n\n\n \nlog Threat Pobability\n\n\nPredictors\nLog-Odds\nCI\nP-value\n\n\nIntercept\n-4.505 ***\n-5.149 – -3.958\n&lt;0.001\n\n\nLength (cm)\n0.011 ***\n0.008 – 0.014\n&lt;0.001\n\n\nObservations\n874\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\nCode\n# Remove large values to evaluate robustness\nrm_outliers &lt;- rm_len_na %&gt;% \n  filter(length_cm &lt;= 1000)\ngg_rm_out &lt;- ggplot(data = rm_outliers, aes(x = length_cm, \n                                            y = is_of_concern)) +\n  geom_jitter(width = 0, height = 0.05, \n              alpha = 0.8, col = \"#38b6ba\") +\n  labs(x = \"Species length (cm)\", y = \"Listed as threatened\", title = \"Fig 3. Probability of being threatened by species length \\n (excluding outliers)\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(colour = '#c9c9c9'),\n        panel.border = element_rect(colour = \"black\", fill=NA, size=1))\nlen_rm_out_plot &lt;- gg_rm_out +\n  geom_smooth(method = \"glm\", \n              se = FALSE, color = \"#545454\", \n              method.args = list(family = \"binomial\"))\nlen_rm_out_plot\n\n\n\n\n\nCode\n# Log regression length removed outliers\nmod_rm_out &lt;- glm(is_of_concern ~ length_cm, \n                  data = rm_outliers, \n                  family = \"binomial\")\n# Model output table format\ntab_model(mod_rm_out,\n          transform = NULL,\n          pred.labels = c(\"Intercept\", \"Length (cm)\"),\n          dv.labels = c(\"log Threat Pobability\"),\n          show.p = TRUE,\n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          string.p = \"P-value\",\n          show.r2 = FALSE,\n          title = \"Tbl 6. Logisitc Regression Model Results for Length with Outliers Removed\",\n          digits = 3)\n\n\n\nTbl 6. Logisitc Regression Model Results for Length with Outliers Removed\n\n\n \nlog Threat Pobability\n\n\nPredictors\nLog-Odds\nCI\nP-value\n\n\nIntercept\n-4.505 ***\n-5.149 – -3.958\n&lt;0.001\n\n\nLength (cm)\n0.011 ***\n0.008 – 0.014\n&lt;0.001\n\n\nObservations\n872\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\n\n\nCode\n# Compute fitted probabilities\nlength_plus &lt;- mod_length %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  mutate(y_hat = .fitted)\n# Compute odds scale\nlength_plus &lt;- length_plus %&gt;% \n  mutate(odds_hat = y_hat / (1 - y_hat)) %&gt;% \n  filter(length_cm &lt;= 1000) # remove outliers for graphing\n# Graph odds scale\nlen_odds_plot &lt;- ggplot(length_plus, aes(x = length_cm, \n                                         y = odds_hat)) +\n  geom_point() + \n  geom_line() + \n  scale_y_continuous(\"Odds of being threatened\") +\n  labs(x = \"Species length (cm)\", \n       title = \"Fig 4. Odds of being threatened by species length\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major = element_line(colour = '#c9c9c9'),\n        panel.border = element_rect(colour = \"black\", fill=NA, size=1))\nlen_odds_plot\n\n\n\n\n\nHere we see that longer fish are associated with higher probability of being threatened (Table 5, p-value = &lt;0.001), but two outliers may be driving this significance (Figure 2). However, when the outliers are removed, we still see a significant positive correlation between fish length and probability of threat (Table 6, p-value = &lt;0.001) with the 50% probability mark remaining just over 400 cm (Figure 3). When we compute the odds ratio, we see that there is an exponential relationship between the length and the odds of a species being threatened. Odds of being threatened increase more at large lengths (Figure 4).\nConfusion Matrix:\n\n\nCode\nlength_plus &lt;- augment(mod_length, type.predict = \"response\") %&gt;%\n  mutate(threatened_hat = round(.fitted)) %&gt;%\n  select(is_of_concern, length_cm, .fitted, threatened_hat)\nl_con_matrix &lt;- length_plus %&gt;%\n  select(is_of_concern, threatened_hat) %&gt;%\n  table()\nrownames &lt;- c(\"Actually Nonthreatened\", \"Actually Threatend\")\ncolnames &lt;- c(\"Predicted Nonthreatend\", \"Predicted Threatened\")\nl_con_matrix &lt;- as.data.frame(matrix(c(l_con_matrix[1], \n                                       l_con_matrix[2], \n                                       l_con_matrix[3], \n                                       l_con_matrix[4]), \n                                     ncol = 2, \n                                     nrow = 2),\n                              row.names = rownames)\ncolnames(l_con_matrix) &lt;- colnames\ntab_df(l_con_matrix,\n       title = \"Tbl 7. Confusion Matrix Displaying Lenght Model Performance\",\n       show.rownames = TRUE)\n\n\n\nTbl 7. Confusion Matrix Displaying Lenght Model Performance\n\n\nRow\nPredicted.Nonthreatend\nPredicted.Threatened\n\n\nActually Nonthreatened\n835\n5\n\n\nActually Threatend\n22\n12\n\n\n\n\n\n\n\nThe accuracy of the model is 97% with 847 out of 874 predicted observations being correct (Table 7). However, the model seems to be more accurate in predicting species that are not actually of concern (true negative rate = 0.99). Species that are threatened have poorer prediction rates (true positive rate = 0.38).\n\n\nFull Model \\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0+\\beta_1  (Length) + \\beta_2  (Reef) + \\beta_3  (Endemic) +\\varepsilon \\]\n\n\nCode\ntab_model(mod,\n          transform = NULL,\n          pred.labels = c(\"Intercept\", \"Length (cm)\", \n                          \"Reef Association\", \"Endemic\"),\n          dv.labels = c(\"log Threat Pobability\"),\n          p.style = c(\"numeric_stars\"),\n          p.threshold = c(0.10, 0.05, 0.01),\n          show.p = TRUE,\n          string.p = \"P-value\",\n          show.r2 = FALSE,\n          title = \"Tbl 8. Logisitc Regression Model Results for Length, Reef Association, and Endemism\",\n          digits = 3)\n\n\n\nTbl 8. Logisitc Regression Model Results for Length, Reef Association, and Endemism\n\n\n \nlog Threat Pobability\n\n\nPredictors\nLog-Odds\nCI\nP-value\n\n\nIntercept\n-4.431 ***\n-5.324 – -3.694\n&lt;0.001\n\n\nLength (cm)\n0.011 ***\n0.008 – 0.014\n&lt;0.001\n\n\nReef Association\n-0.048 \n-0.972 – 0.869\n0.918\n\n\nEndemic\n1.989 *\n-0.972 – 3.798\n0.071\n\n\nObservations\n650\n\n\n* p&lt;0.1   ** p&lt;0.05   *** p&lt;0.01\n\n\n\n\n\n\n\nWe can see from the output above that length remains significant (Table 8, p-value = &lt;0.001) even when additional variables are added, making length robust. Endemicity is a significant predictor of threat probability at a significance level of 0.10 (p-value = 0.0707). Coral reef association is not significantly impacting the model (p-value = 0.9175).\n\n\n\nPredicting Probability – Solving for p:\nIn this model, the smallest fish in the data set – Melamphaes danae (bigscale) at 2.30 cm – has a probability of being threatened of 0.012. The largest fish in the data set – Rhincodon typus (whale shark) at 1700 cm – has a probability of being threatened of 0.99. Here I use the model to predict the probabilities of being threatened for unlisted fish.\n\n\nCode\ntidy_pred_rank &lt;- tidy_pred_rank %&gt;% \n  select(\"genus_species\", \"length_cm\", \"coral_reefs\", \"main_common_name\", \"reef_associated\", \"endemic\", \"y_hat\")\ntab_df(tidy_pred_rank[1:5,],\n       title = \"Tbl 9. Top 5 Most Vulnerable Unranked Fish in Hawaii\",\n       col.header = c(\"Genus species\", \"Length (cm)\", \"Reef Association\",\n                          \"Common Name\", \"Reef Association\", \"Endemic\",\n                          \"Threatened\"))\n\n\n\nTbl 9. Top 5 Most Vulnerable Unranked Fish in Hawaii\n\n\nGenus species\nLength (cm)\nReef Association\nCommon Name\nReef Association\nEndemic\nThreatened\n\n\nMakaira mazara\n500\n0\nNA\nno\nno\n0.74\n\n\nIstiompax indica\n465\n1\nBlack Marlin\nyes\nno\n0.65\n\n\nEchinorhinus cookei\n400\n0\nPrickly Shark\nno\nno\n0.49\n\n\nEpinephelus lanceolatus\n270\n1\nGiant Grouper\nyes\nno\n0.18\n\n\nAssurger anzac\n250\n0\nNA\nno\nno\n0.15\n\n\n\n\n\n\n\nThe top five most vulnerable unranked species under this model are Makaira mazara (Indo-Pacific blue marlin), Istiompax indica (black marlin), Echinorhinus cookei (prickly shark), Epinephelus lanceolatus (giant grouper), and Assurger anzac (razorback scabbardfish) (Table 9.) Makaira mazara and Istiompax indica have probabilities of being threatened over 50%. I would recommend the IUCN evaluate these two species before others. However, there are limitations to this analysis and more research should be done before allocating resources to evaluate these species."
  },
  {
    "objectID": "posts/2022-12-02-hawaiian-fish-analysis/index.html#limitations-next-steps",
    "href": "posts/2022-12-02-hawaiian-fish-analysis/index.html#limitations-next-steps",
    "title": "Identifying Key Traits in Hawaiian Fish that Predict Risk of Extinction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nAs mentioned above, both the IUCN Red List data and the FishBase data have potential biases and don’t evaluate all species. Additionally, not all listed species had every piece of information, and missing data were removed. With missing data removed, the sample became small. It is possible that endemism plays a bigger role in threat risk than illustrated here by the model. Points of concern with this model include disproportionate data in both the threat level and endemism, and high false negative rates in binary prediction. Future analyses could investigate other potential explanatory variables such as metrics measuring fishing pressure, life cycle characteristics, and range. Future analyses should also incorporate indigenous knowledge as Native People in Hawaiʻi know fish history around the islands and are frequently in the water for spearfishing and other recreation. Additionally, future analyses could expand the area of interest to all of the tropics.\n\nReference\n\n[1] “IUCN,” IUCN Red List of Threatened Species. Version 2022-1, 2022. https://www.iucnredlist.org/ (accessed Dec. 02, 2022).\n[2] B. J. Cardinale et al., “Biodiversity loss and its impact on humanity,” Nature, vol. 486, no. 7401, Art. no. 7401, Jun. 2012, doi: 10.1038/nature11148. [3] M. J. Munstermann et al., “A global ecological signal of extinction risk in terrestrial vertebrates,” Conserv. Biol., vol. 36, no. 3, p. e13852, 2022, doi: 10.1111/cobi.13852.\n[4] “IUCN,” IUCN Red List of Threatened Species. Version 2022-1, 2015. www.iucnredlist.org\n[5] R. Froese and D. Pauly, “FishBase,” 2022. www.fishbase.org\n[6] C. Boettiger, D. Temple Lang, and P. Wainwright, “rfishbase: exploring, manipulating and visualizing FishBase data from R.,” J. Fish Biol., 2012, doi: https://doi.org/10.1111/j.1095-8649.2012.03464.x.\n[7] W. J. Ripple, C. Wolf, T. M. Newsome, M. Hoffmann, A. J. Wirsing, and D. J. McCauley, “Extinction risk is most acute for the world’s largest and smallest vertebrates,” Proc. Natl. Acad. Sci. U. S. A., vol. 114, no. 40, pp. 10678–10683, Oct. 2017, doi: 10.1073/pnas.1702078114.\n[8] K. D. Bahr, P. L. Jokiel, and K. S. Rodgers, “The 2014 coral bleaching and freshwater flood events in Kāneʻohe Bay, Hawaiʻi,” PeerJ, vol. 3, p. e1136, Aug. 2015, doi: 10.7717/peerj.1136."
  },
  {
    "objectID": "posts/2019-12-01-bobwhite-aru-research/index.html",
    "href": "posts/2019-12-01-bobwhite-aru-research/index.html",
    "title": "Evaluating the Use of Autonomous Recording Units for Monitoring Northern Bobwhite Coveys",
    "section": "",
    "text": "This is a 2019 research poster on evaluating the use of autonomous recording units (ARUs) for monitoring Northern bobwhite coveys. This research was done in the Janke lab in collaboration with the Iowa DNR with the goal of furthering the National Bobwhite Conservation Initiative. I presented this research at the Iowa State University College of Agriculture and Life Sciences ‘Science with Practice’ Poster Presentation (April 2019), the Iowa State University Honors Poster Presentation (December 2019), and the National Conference on Undergraduate Research (April 2021).\n\n\n\n\n\n\n\n\nProcessing Audio in Raven Pro\n\n\n\n\n\nCitationBibTeX citation:@online{windschitl, adam janke, kyla yuza-pate2019,\n  author = {Windschitl, Adam Janke, Kyla Yuza-Pate, Elke},\n  title = {Evaluating the {Use} of {Autonomous} {Recording} {Units} for\n    {Monitoring} {Northern} {Bobwhite} {Coveys}},\n  date = {2019-12-01},\n  url = {https://elkewind.github.io/posts/2019-12-01-bobwhite-aru-research},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nWindschitl, Adam Janke, Kyla Yuza-Pate, Elke. 2019. “Evaluating\nthe Use of Autonomous Recording Units for Monitoring Northern Bobwhite\nCoveys.” December 1, 2019. https://elkewind.github.io/posts/2019-12-01-bobwhite-aru-research."
  },
  {
    "objectID": "posts/2023-08-19-python-viz/index.html",
    "href": "posts/2023-08-19-python-viz/index.html",
    "title": "Data Visualization Examples in Python",
    "section": "",
    "text": "This post uses data from the kelpGeoMod data repository and provides examples of data visualization in Python using Folium, Plotly, Matplotlib, and Rasterio.\n\n\nCode\n# Import necessary libraries\n#| warning: false\nimport os\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nimport geopandas as gpd\nimport folium\nfrom folium import DivIcon\nfrom IPython.display import display\nimport rasterio\nimport rasterio.plot\nimport matplotlib.pyplot as plt\nfrom rasterio.warp import transform_geom\nfrom matplotlib import rcParams\n\n\n\n\nData for this notebook come from the kelpGeoMod Google Drive data repository. This data repository was created as a Bren Master of Environmental Data Science capstone project by Erika Egg, Jessica French, Javier Patrón, and Elke Windschitl.\n\n\nCode\n# Setting the data directory path\ndata_dir = \"/Users/elkewindschitl/Documents/MEDS/kelpGeoMod/final-data\"\n\n# Reading in the area of interest shapefile\naoi_path = os.path.join(data_dir, \"02-intermediate-data/02-aoi-sbchannel-shapes-intermediate/aoi-sbchannel.shp\")\naoi = gpd.read_file(aoi_path)\n\n# Reading in the \"full synthesized\" data set\nfull_synth_path = os.path.join(data_dir, \"03-analysis-data/03-data-synthesization-analysis/full-synthesized.csv\")\n# Read the CSV file into a dataframe\nfull_synth_df = pd.read_csv(full_synth_path)\n\n# Reading in the \"observed nutrients\" data set\nobs_nutr_path = os.path.join(data_dir, \"03-analysis-data/03-data-synthesization-analysis/observed-nutrients-synthesized.csv\")\n# Read the CSV file into a dataframe\nobs_nutr_df = pd.read_csv(obs_nutr_path)\n\n# Setting path to depth raster\nraster_path = os.path.join(data_dir, \"02-intermediate-data/06-depth-intermediate/depth.tif\")\n\n\n\n\n\nThese data come from the Santa Barbara Channel between 2014-2022.\n\n\nCode\n# Reproject geometries to WGS84\naoi_84 = aoi.to_crs(epsg=4326)\n\n# Create a Folium map centered around the AOI\nm = folium.Map(location=[aoi_84['geometry'].centroid.y.mean(), aoi_84['geometry'].centroid.x.mean()], zoom_start=9, tiles='Stamen Terrain')\n\n# Define a function to set shape color based on properties\ndef style_function(feature):\n    return {\n        'fillColor': '#93C2E2', \n        'color': '#326587',\n        'weight': 4,\n        'fillOpacity': 0.6\n    }\n\n# Add GeoJSON data to the map with custom style\nfolium.GeoJson(aoi_84.to_json(), style_function=style_function).add_to(m)\n\n# Display the map\ndisplay(m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\nHere I use the “full synthesized data set” to visualize how kelp area in the region changes through time. First, I want to check out the data set.\n\n\nCode\n# Check the data frame\nprint(full_synth_df.head())\n\n\n   year  quarter     lat      lon      depth        sst  kelp_area  \\\n0  2014        1  34.590 -120.646 -46.320492  14.016689        NaN   \n1  2014        1  34.582 -120.646 -42.287880  14.040511        NaN   \n2  2014        1  34.574 -120.646 -45.276779  14.059811        NaN   \n3  2014        1  34.566 -120.646 -54.458694  14.074700        NaN   \n4  2014        1  34.566 -120.638 -23.270111  14.067266        NaN   \n\n   kelp_biomass  nitrate_nitrite  phosphate  ammonium  \n0           NaN              NaN        NaN       NaN  \n1           NaN              NaN        NaN       NaN  \n2           NaN              NaN        NaN       NaN  \n3           NaN              NaN        NaN       NaN  \n4           NaN              NaN        NaN       NaN  \n\n\nI need to do a little wrangling to get the sum of the kelp area for each year/quarter combination. Each row in this data set represents one grid cell at one quarter in one year originating from raster data (for more information on the data, see the kelpGeoMod metadata throughout the Google Drive).\n\n\nCode\n# Combine year and quarter columns into a single datetime column\nfull_synth_df['Date'] = pd.to_datetime(full_synth_df['year'].astype(str) + '-Q' + full_synth_df['quarter'].astype(str))\n\n# Group by date and calculate the mean for specific columns and the sum for kelp_area\naggregation = {\n    'sst': 'mean',\n    'year': 'mean',\n    'quarter': 'mean',\n    'kelp_area': 'sum'  # Sum the kelp_area column\n}\nsum_kelp = full_synth_df.groupby('Date').agg(aggregation)\n\n# Reset index to move \"Season\" from index to a column\nsum_kelp = sum_kelp.reset_index()\n\n# Convert from m^2 to km^2 and round values\nsum_kelp['kelp_area'] = (sum_kelp['kelp_area'] / 1000000).round(2)\nsum_kelp['year'] = sum_kelp['year'].astype(int)\n\n# Define a custom function to generate the new column based on \"quarter\" and \"year\"\ndef generate_season(row):\n    quarter = row[\"quarter\"]\n    year = row[\"year\"]\n    \n    if quarter == 1:\n        return f\"Winter {year}\"\n    elif quarter == 2:\n        return f\"Spring {year}\"\n    elif quarter == 3:\n        return f\"Summer {year}\"\n    elif quarter == 4:\n        return f\"Fall {year}\"\n    else:\n        return \"Invalid Quarter\"\n    \n# Apply the custom function to create the new \"Season\" column\nsum_kelp[\"Season\"] = sum_kelp.apply(generate_season, axis=1)\n\n# Print the summarized dataframe\nprint(sum_kelp.head())\n\n\n        Date        sst  year  quarter  kelp_area       Season\n0 2014-01-01  14.916278  2014      1.0       1.23  Winter 2014\n1 2014-04-01  16.053795  2014      2.0       2.43  Spring 2014\n2 2014-07-01  19.865393  2014      3.0       2.15  Summer 2014\n3 2014-10-01  18.694880  2014      4.0       0.37    Fall 2014\n4 2015-01-01  16.293984  2015      1.0       0.40  Winter 2015\n\n\nHere I show the kelp area over time with the help of Plotly!\n\n\nCode\n# Calculate the overall range for y-axis based on kelp area data\ny_axis_range = [0, sum_kelp['kelp_area'].max() + 1]\n\n# Create the figure\nfig = go.Figure()\n\n# Plotting the Kelp Area data with custom color and line style\nfig.add_trace(go.Scatter(\n    x=sum_kelp.Date,\n    y=sum_kelp['kelp_area'],\n    mode='lines+markers',\n    name='',\n    line=dict(color='#BCD79D'),\n    marker=dict(size=8),\n    hovertemplate='Season: %{text}&lt;br&gt;Kelp Area: %{y} km²'\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    title='Kelp area is highly variable in the Santa Barbara Channel',\n    title_font=dict(family='Arial', size=22, color='white'),\n    title_x=0.5,\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(title='Date', showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=True, linewidth=1, linecolor='white'),\n    yaxis=dict(title='Total Kelp Area (km²)', showgrid=False, showline=False, linewidth=1, linecolor='white', range=y_axis_range, tickmode='linear', dtick=1),\n    legend=dict(font=dict(size=14, color='white')),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600,\n    margin=dict(b=60)\n)\n\n# Update hover text with 'Season'\nfig.update_traces(\n    text=sum_kelp['Season'])\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\n\n\n\nNext I visualize the ocean nutrient data based on averages of the seasonal values over time with Plotly. Similarly, I needed to do a little wrangling first.\n\n\nCode\n# Check the data frame\nprint(obs_nutr_df.head())\n\n\n   year  quarter       lat        lon      temp  nitrate  nitrite  \\\n0  2014        1  34.01032 -118.84232  14.90600   0.4000  0.08300   \n1  2015        1  34.45118 -120.52470  15.87580   0.3125  0.06225   \n2  2015        1  34.40263 -119.80203  16.32375   0.0500  0.03950   \n3  2015        1  34.27690 -120.02423  16.35700   0.0300  0.00900   \n4  2015        1  34.25833 -119.32373  16.01060   0.1525  0.12875   \n\n   nitrate_nitrite  phosphate  ammonium        sst nutrient_source  \\\n0          0.48300   0.376667  0.066667  15.950767         CalCOFI   \n1          0.37475   0.385000  0.125000  14.712589         CalCOFI   \n2          0.08950   0.385000  0.060000  14.730477         CalCOFI   \n3          0.03900   0.310000  0.010000  14.923833         CalCOFI   \n4          0.28125   0.567500  0.227500  14.923833         CalCOFI   \n\n        depth  kelp_area  kelp_biomass  \n0  -87.118073        NaN           NaN  \n1 -112.089943        NaN           NaN  \n2 -109.903297        NaN           NaN  \n3 -481.472626        NaN           NaN  \n4 -481.472626        NaN           NaN  \n\n\n\n\nCode\n# Combine year and quarter columns into a single datetime column\nobs_nutr_df['Date'] = pd.to_datetime(obs_nutr_df['year'].astype(str) + '-Q' + obs_nutr_df['quarter'].astype(str))\n\n# Define a custom function to generate the new column based on \"quarter\" and \"year\"\ndef generate_season(row):\n    quarter = row[\"quarter\"]\n    year = row[\"year\"]\n    \n    if quarter == 1:\n        return f\"Winter {year}\"\n    elif quarter == 2:\n        return f\"Spring {year}\"\n    elif quarter == 3:\n        return f\"Summer {year}\"\n    elif quarter == 4:\n        return f\"Fall {year}\"\n    else:\n        return \"Invalid Quarter\"\n    \n# Apply the custom function to create the new \"Season\" column\nobs_nutr_df[\"Season\"] = obs_nutr_df.apply(generate_season, axis=1)\n\n# Group by date and season, and calculate the mean for each column\nmean_nutr = obs_nutr_df.groupby(['Date', 'Season']).mean(numeric_only=True)\n\n# Reset index to move \"Season\" from index to a column\nmean_nutr = mean_nutr.reset_index()\n\n# Drop unused columns\nmean_nutr = mean_nutr.drop(['lat', 'lon', 'depth'], axis=1)\n\n# Print the summarized dataframe\nprint(mean_nutr.head())\n\n\n        Date       Season    year  quarter       temp   nitrate   nitrite  \\\n0 2014-01-01  Winter 2014  2014.0      1.0  14.312260  0.588876  0.119150   \n1 2014-04-01  Spring 2014  2014.0      2.0  13.987199  2.369880  0.104520   \n2 2014-07-01  Summer 2014  2014.0      3.0  18.531475  0.092371  0.060429   \n3 2014-10-01    Fall 2014  2014.0      4.0  18.146190  0.106062  0.074262   \n4 2015-01-01  Winter 2015  2015.0      1.0  15.701622  0.131579  0.071911   \n\n   nitrate_nitrite  phosphate  ammonium        sst     kelp_area  \\\n0         1.264658   0.339080  0.387688  18.450691   7879.809524   \n1         2.200509   0.349068  0.370103  15.902271  10447.044944   \n2         0.178743   0.178004  0.169410  15.868604   8951.882353   \n3         0.237076   0.251334  0.285495  14.606426  16152.435644   \n4         0.519048   0.334001  0.193615  17.157000  10833.866667   \n\n    kelp_biomass  \n0   61283.412698  \n1   83235.258427  \n2   71077.823529  \n3  121647.386139  \n4   81012.066667  \n\n\n\n\nCode\n# Calculate the overall range for y-axis that covers all nutrient data\ny_axis_range = [0, mean_nutr[['nitrate', 'nitrite', 'phosphate', 'ammonium']].max().max() + 0.5]\n\n# Create the figure\nfig = go.Figure()\n\n# Plotting the data with custom colors and line styles\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['nitrate'],\n    mode='lines+markers',\n    name='Nitrate',\n    line=dict(color='#D28077'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L',\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['nitrite'],\n    mode='lines+markers',\n    name='Nitrite',\n    line=dict(color='#93C2E2'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['phosphate'],\n    mode='lines+markers',\n    name='Phosphate',\n    line=dict(color='#BCD79D'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['ammonium'],\n    mode='lines+markers',\n    name='Ammonium',\n    line=dict(color='#036554'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    title='Nutrient concentrations trend higher in winter and spring and lower in summer and fall',\n    title_font=dict(family='Arial', size=22, color='white'),\n    title_x=0.5,\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(title='Time', showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=True, linewidth=1, linecolor='white'),\n    yaxis=dict(title='Average Concentration (μmol/L)', showgrid=False, showline=False, linewidth=1, linecolor='white', range=y_axis_range, tickmode='linear', dtick=1),\n    legend=dict(font=dict(size=14, color='white')),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600\n)\n\n# Update hover text with 'Season'\nfig.update_traces(\n    text=mean_nutr['Season']\n)\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\nI want to more closely look at average nutrient concentrations during the el Niño year 2016.\n\n\nCode\n# Filter for the year 2016\nfiltered_2016 = mean_nutr[mean_nutr['year'] == 2016]\n\n# Calculate the average nutrient concentrations\naverage_2016 = filtered_2016[['nitrate', 'nitrite', 'ammonium', 'phosphate']].agg('mean')\n\n# Create a DataFrame with 'Nutrient' and 'Concentration' columns\naverage_2016 = pd.DataFrame({'Nutrient': average_2016.index, 'Concentration': average_2016.values})\n\n# Create the bar chart\nfig, ax = plt.subplots(figsize=(9.3, 6))  # Adjusted figsize\nax.set_facecolor('#333333')  # Set the background color for the plotting area\n\ncolors = ['#D28077', '#93C2E2', '#036554', '#BCD79D']\n\nbars = ax.bar(average_2016['Nutrient'], average_2016['Concentration'], color=colors)\nax.set_xlabel('Nutrient', color='white', fontname='Arial', size = 11, labelpad = 10)\nax.set_ylabel('Average Concentration (μmol/L)', color='white', fontname='Arial', size = 11, labelpad = 10)\n\n# Adjusted title font size (no bold)\nax.set_title('Average Nutrient Concentrations in 2016', color='white', fontname='Arial', fontsize=16)\n\nax.tick_params(axis='x', rotation=0, colors='white')\nax.tick_params(axis='y', colors='white')\n\n# Add value labels on top of the bars\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', color='white', fontsize=10, fontname='Arial')\n\n# Adding white grid lines\nax.yaxis.grid(color='white', linestyle='--', linewidth=0.5)\n\n# Moving grid lines behind the data\nax.set_axisbelow(True)\n\n# Adding white spines (lines along the axes)\nax.spines['bottom'].set_color('white')\nax.spines['left'].set_color('white')\n\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nNext, I want to get a better understanding of ocean depth in the channel. Here I create a histogram of depths. First, though, I will need to average the depth over all time periods grouped by lat and lon. This is because depth remains constant over all years and is thus duplicated in the data set. However, I do not want duplicates in the histogram.\n\n\nCode\n# Group by \"lat\" and \"lon\" and calculate the average of the \"depth\" column\ngrouped_data = full_synth_df.groupby(['lat', 'lon'])['depth'].mean().round()\n\n# Because some grid cells overlap with land, the value is greater than zero, but I want to ceiling the data at zero.\n# Convert the grouped data back to a DataFrame\ngrouped_df = grouped_data.reset_index()\n# Replace values greater than zero with zero in the \"depth\" column\ngrouped_df['depth'] = -1 * grouped_df['depth'].apply(lambda x: 0 if x &gt; 0 else x)\n\n# Print the modified DataFrame\nprint(grouped_df)\n\n\n          lat      lon   depth\n0      33.854 -120.646  1971.0\n1      33.854 -120.638  1954.0\n2      33.854 -120.630  1942.0\n3      33.854 -120.622  1931.0\n4      33.854 -120.614  1923.0\n...       ...      ...     ...\n13511  34.566 -120.646    54.0\n13512  34.566 -120.638    23.0\n13513  34.574 -120.646    45.0\n13514  34.582 -120.646    42.0\n13515  34.590 -120.646    46.0\n\n[13516 rows x 3 columns]\n\n\nHere I visualize the depth data in a histogram with plotly! Again, this data was originally in the form of a raster, so each measurement of depth represents a 0.008° x 0.008° grid cell.\n\n\nCode\n# Calculate the histogram manually\nhist, bins = np.histogram(grouped_df.depth, bins=range(0, int(grouped_df['depth'].max()) + 1, 50))\nbin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\nbin_ranges = [f'Range: {bins[i]}-{bins[i + 1] - 1} m' for i in range(len(bins) - 1)]\nhover_text = [f'{bin_ranges[i]}&lt;br&gt;Count: {hist[i]}' for i in range(len(bins) - 1)]\n\n\n# Create the plot\nfig = go.Figure()\n\n# Plotting the data with custom colors and line styles\nfig.add_trace(go.Bar(\n    x=bin_centers,\n    y=hist,\n    hovertext=hover_text,\n    hoverinfo='text',\n    width=bins[1] - bins[0],\n    marker_color='#02a8c9',\n    marker_line=dict(color='rgba(211, 211, 211, 0.5)', width=1)\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    xaxis_title='Average Depth (m)',\n    yaxis_title='Frequency',\n    title='Histogram of Depths in the Santa Barbara Channel',\n    title_font=dict(family='Arial', size=22, color='white'),\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(showgrid=False, showline=False, linewidth=1, linecolor='white'),\n    yaxis=dict(showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=False, linewidth=1, linecolor='white'),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600,\n    title_x=0.5,\n    annotations=[\n        dict(\n            x=0.5,\n            y=1.08,\n            showarrow=False,\n            text=\"where every data point represents 0.008° x 0.008° (approximately 1 km)\",\n            xref=\"paper\",\n            yref=\"paper\",\n            font=dict(family='Arial', size=14, color='white')\n        )\n    ]\n)\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\nHere is the depth data as a raster layer.\n\n\nCode\n# Set the font family for the entire plot\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = ['Arial']  # Use Arial font or another available sans-serif font\n\n# Customize the figure background color, axes background color, and text color\nrcParams['figure.figsize'] = (10, 8)  # Set the figure size (width, height) in inches\nrcParams['figure.facecolor'] = '#333333'   # Set background color of the figure\nrcParams['axes.edgecolor'] = '#333333'       # Set color of axes lines to white\nrcParams['axes.labelcolor'] = 'white'      # Set color of axes labels to white\nrcParams['xtick.color'] = 'white'          # Set color of x-axis ticks to white\nrcParams['ytick.color'] = 'white'          # Set color of y-axis ticks to white\nrcParams['text.color'] = 'white'           # Set text color to white\n\n# Open the raster file using rasterio\nwith rasterio.open(raster_path) as src:\n    # Set up colormap and normalization\n    cmap = plt.cm.Blues_r # Reverse the Blues colormap\n    cmap.set_bad(color='#333333')  # Set NaN values to be white\n    norm = plt.Normalize(vmin=-1000, vmax=20)\n\n    # Create a larger figure\n    plt.figure(figsize=(10.5, 8.5))\n    # Add \"Latitude\" and \"Longitude\" labels using plt.text\n    plt.text(0.5, -0.16, 'Longitude', transform=plt.gca().transAxes,\n             ha='center', color='white')\n    plt.text(-0.1, 0.5, 'Latitude', transform=plt.gca().transAxes,\n             va='center', rotation='vertical', color='white')\n\n    # Plot the raster data using rasterio's show function\n    rasterio.plot.show(src,\n                       cmap=cmap,\n                       norm=norm,\n                       title='Depth in the Santa Barbara Channel',\n                       origin='upper')\n                      \n\nplt.show()"
  },
  {
    "objectID": "posts/2023-08-19-python-viz/index.html#visualizing-various-data-types-from-the-kelpgeomod-data-repository",
    "href": "posts/2023-08-19-python-viz/index.html#visualizing-various-data-types-from-the-kelpgeomod-data-repository",
    "title": "Data Visualization Examples in Python",
    "section": "",
    "text": "This post uses data from the kelpGeoMod data repository and provides examples of data visualization in Python using Folium, Plotly, Matplotlib, and Rasterio.\n\n\nCode\n# Import necessary libraries\n#| warning: false\nimport os\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nimport geopandas as gpd\nimport folium\nfrom folium import DivIcon\nfrom IPython.display import display\nimport rasterio\nimport rasterio.plot\nimport matplotlib.pyplot as plt\nfrom rasterio.warp import transform_geom\nfrom matplotlib import rcParams\n\n\n\n\nData for this notebook come from the kelpGeoMod Google Drive data repository. This data repository was created as a Bren Master of Environmental Data Science capstone project by Erika Egg, Jessica French, Javier Patrón, and Elke Windschitl.\n\n\nCode\n# Setting the data directory path\ndata_dir = \"/Users/elkewindschitl/Documents/MEDS/kelpGeoMod/final-data\"\n\n# Reading in the area of interest shapefile\naoi_path = os.path.join(data_dir, \"02-intermediate-data/02-aoi-sbchannel-shapes-intermediate/aoi-sbchannel.shp\")\naoi = gpd.read_file(aoi_path)\n\n# Reading in the \"full synthesized\" data set\nfull_synth_path = os.path.join(data_dir, \"03-analysis-data/03-data-synthesization-analysis/full-synthesized.csv\")\n# Read the CSV file into a dataframe\nfull_synth_df = pd.read_csv(full_synth_path)\n\n# Reading in the \"observed nutrients\" data set\nobs_nutr_path = os.path.join(data_dir, \"03-analysis-data/03-data-synthesization-analysis/observed-nutrients-synthesized.csv\")\n# Read the CSV file into a dataframe\nobs_nutr_df = pd.read_csv(obs_nutr_path)\n\n# Setting path to depth raster\nraster_path = os.path.join(data_dir, \"02-intermediate-data/06-depth-intermediate/depth.tif\")\n\n\n\n\n\nThese data come from the Santa Barbara Channel between 2014-2022.\n\n\nCode\n# Reproject geometries to WGS84\naoi_84 = aoi.to_crs(epsg=4326)\n\n# Create a Folium map centered around the AOI\nm = folium.Map(location=[aoi_84['geometry'].centroid.y.mean(), aoi_84['geometry'].centroid.x.mean()], zoom_start=9, tiles='Stamen Terrain')\n\n# Define a function to set shape color based on properties\ndef style_function(feature):\n    return {\n        'fillColor': '#93C2E2', \n        'color': '#326587',\n        'weight': 4,\n        'fillOpacity': 0.6\n    }\n\n# Add GeoJSON data to the map with custom style\nfolium.GeoJson(aoi_84.to_json(), style_function=style_function).add_to(m)\n\n# Display the map\ndisplay(m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\nHere I use the “full synthesized data set” to visualize how kelp area in the region changes through time. First, I want to check out the data set.\n\n\nCode\n# Check the data frame\nprint(full_synth_df.head())\n\n\n   year  quarter     lat      lon      depth        sst  kelp_area  \\\n0  2014        1  34.590 -120.646 -46.320492  14.016689        NaN   \n1  2014        1  34.582 -120.646 -42.287880  14.040511        NaN   \n2  2014        1  34.574 -120.646 -45.276779  14.059811        NaN   \n3  2014        1  34.566 -120.646 -54.458694  14.074700        NaN   \n4  2014        1  34.566 -120.638 -23.270111  14.067266        NaN   \n\n   kelp_biomass  nitrate_nitrite  phosphate  ammonium  \n0           NaN              NaN        NaN       NaN  \n1           NaN              NaN        NaN       NaN  \n2           NaN              NaN        NaN       NaN  \n3           NaN              NaN        NaN       NaN  \n4           NaN              NaN        NaN       NaN  \n\n\nI need to do a little wrangling to get the sum of the kelp area for each year/quarter combination. Each row in this data set represents one grid cell at one quarter in one year originating from raster data (for more information on the data, see the kelpGeoMod metadata throughout the Google Drive).\n\n\nCode\n# Combine year and quarter columns into a single datetime column\nfull_synth_df['Date'] = pd.to_datetime(full_synth_df['year'].astype(str) + '-Q' + full_synth_df['quarter'].astype(str))\n\n# Group by date and calculate the mean for specific columns and the sum for kelp_area\naggregation = {\n    'sst': 'mean',\n    'year': 'mean',\n    'quarter': 'mean',\n    'kelp_area': 'sum'  # Sum the kelp_area column\n}\nsum_kelp = full_synth_df.groupby('Date').agg(aggregation)\n\n# Reset index to move \"Season\" from index to a column\nsum_kelp = sum_kelp.reset_index()\n\n# Convert from m^2 to km^2 and round values\nsum_kelp['kelp_area'] = (sum_kelp['kelp_area'] / 1000000).round(2)\nsum_kelp['year'] = sum_kelp['year'].astype(int)\n\n# Define a custom function to generate the new column based on \"quarter\" and \"year\"\ndef generate_season(row):\n    quarter = row[\"quarter\"]\n    year = row[\"year\"]\n    \n    if quarter == 1:\n        return f\"Winter {year}\"\n    elif quarter == 2:\n        return f\"Spring {year}\"\n    elif quarter == 3:\n        return f\"Summer {year}\"\n    elif quarter == 4:\n        return f\"Fall {year}\"\n    else:\n        return \"Invalid Quarter\"\n    \n# Apply the custom function to create the new \"Season\" column\nsum_kelp[\"Season\"] = sum_kelp.apply(generate_season, axis=1)\n\n# Print the summarized dataframe\nprint(sum_kelp.head())\n\n\n        Date        sst  year  quarter  kelp_area       Season\n0 2014-01-01  14.916278  2014      1.0       1.23  Winter 2014\n1 2014-04-01  16.053795  2014      2.0       2.43  Spring 2014\n2 2014-07-01  19.865393  2014      3.0       2.15  Summer 2014\n3 2014-10-01  18.694880  2014      4.0       0.37    Fall 2014\n4 2015-01-01  16.293984  2015      1.0       0.40  Winter 2015\n\n\nHere I show the kelp area over time with the help of Plotly!\n\n\nCode\n# Calculate the overall range for y-axis based on kelp area data\ny_axis_range = [0, sum_kelp['kelp_area'].max() + 1]\n\n# Create the figure\nfig = go.Figure()\n\n# Plotting the Kelp Area data with custom color and line style\nfig.add_trace(go.Scatter(\n    x=sum_kelp.Date,\n    y=sum_kelp['kelp_area'],\n    mode='lines+markers',\n    name='',\n    line=dict(color='#BCD79D'),\n    marker=dict(size=8),\n    hovertemplate='Season: %{text}&lt;br&gt;Kelp Area: %{y} km²'\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    title='Kelp area is highly variable in the Santa Barbara Channel',\n    title_font=dict(family='Arial', size=22, color='white'),\n    title_x=0.5,\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(title='Date', showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=True, linewidth=1, linecolor='white'),\n    yaxis=dict(title='Total Kelp Area (km²)', showgrid=False, showline=False, linewidth=1, linecolor='white', range=y_axis_range, tickmode='linear', dtick=1),\n    legend=dict(font=dict(size=14, color='white')),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600,\n    margin=dict(b=60)\n)\n\n# Update hover text with 'Season'\nfig.update_traces(\n    text=sum_kelp['Season'])\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\n\n\n\nNext I visualize the ocean nutrient data based on averages of the seasonal values over time with Plotly. Similarly, I needed to do a little wrangling first.\n\n\nCode\n# Check the data frame\nprint(obs_nutr_df.head())\n\n\n   year  quarter       lat        lon      temp  nitrate  nitrite  \\\n0  2014        1  34.01032 -118.84232  14.90600   0.4000  0.08300   \n1  2015        1  34.45118 -120.52470  15.87580   0.3125  0.06225   \n2  2015        1  34.40263 -119.80203  16.32375   0.0500  0.03950   \n3  2015        1  34.27690 -120.02423  16.35700   0.0300  0.00900   \n4  2015        1  34.25833 -119.32373  16.01060   0.1525  0.12875   \n\n   nitrate_nitrite  phosphate  ammonium        sst nutrient_source  \\\n0          0.48300   0.376667  0.066667  15.950767         CalCOFI   \n1          0.37475   0.385000  0.125000  14.712589         CalCOFI   \n2          0.08950   0.385000  0.060000  14.730477         CalCOFI   \n3          0.03900   0.310000  0.010000  14.923833         CalCOFI   \n4          0.28125   0.567500  0.227500  14.923833         CalCOFI   \n\n        depth  kelp_area  kelp_biomass  \n0  -87.118073        NaN           NaN  \n1 -112.089943        NaN           NaN  \n2 -109.903297        NaN           NaN  \n3 -481.472626        NaN           NaN  \n4 -481.472626        NaN           NaN  \n\n\n\n\nCode\n# Combine year and quarter columns into a single datetime column\nobs_nutr_df['Date'] = pd.to_datetime(obs_nutr_df['year'].astype(str) + '-Q' + obs_nutr_df['quarter'].astype(str))\n\n# Define a custom function to generate the new column based on \"quarter\" and \"year\"\ndef generate_season(row):\n    quarter = row[\"quarter\"]\n    year = row[\"year\"]\n    \n    if quarter == 1:\n        return f\"Winter {year}\"\n    elif quarter == 2:\n        return f\"Spring {year}\"\n    elif quarter == 3:\n        return f\"Summer {year}\"\n    elif quarter == 4:\n        return f\"Fall {year}\"\n    else:\n        return \"Invalid Quarter\"\n    \n# Apply the custom function to create the new \"Season\" column\nobs_nutr_df[\"Season\"] = obs_nutr_df.apply(generate_season, axis=1)\n\n# Group by date and season, and calculate the mean for each column\nmean_nutr = obs_nutr_df.groupby(['Date', 'Season']).mean(numeric_only=True)\n\n# Reset index to move \"Season\" from index to a column\nmean_nutr = mean_nutr.reset_index()\n\n# Drop unused columns\nmean_nutr = mean_nutr.drop(['lat', 'lon', 'depth'], axis=1)\n\n# Print the summarized dataframe\nprint(mean_nutr.head())\n\n\n        Date       Season    year  quarter       temp   nitrate   nitrite  \\\n0 2014-01-01  Winter 2014  2014.0      1.0  14.312260  0.588876  0.119150   \n1 2014-04-01  Spring 2014  2014.0      2.0  13.987199  2.369880  0.104520   \n2 2014-07-01  Summer 2014  2014.0      3.0  18.531475  0.092371  0.060429   \n3 2014-10-01    Fall 2014  2014.0      4.0  18.146190  0.106062  0.074262   \n4 2015-01-01  Winter 2015  2015.0      1.0  15.701622  0.131579  0.071911   \n\n   nitrate_nitrite  phosphate  ammonium        sst     kelp_area  \\\n0         1.264658   0.339080  0.387688  18.450691   7879.809524   \n1         2.200509   0.349068  0.370103  15.902271  10447.044944   \n2         0.178743   0.178004  0.169410  15.868604   8951.882353   \n3         0.237076   0.251334  0.285495  14.606426  16152.435644   \n4         0.519048   0.334001  0.193615  17.157000  10833.866667   \n\n    kelp_biomass  \n0   61283.412698  \n1   83235.258427  \n2   71077.823529  \n3  121647.386139  \n4   81012.066667  \n\n\n\n\nCode\n# Calculate the overall range for y-axis that covers all nutrient data\ny_axis_range = [0, mean_nutr[['nitrate', 'nitrite', 'phosphate', 'ammonium']].max().max() + 0.5]\n\n# Create the figure\nfig = go.Figure()\n\n# Plotting the data with custom colors and line styles\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['nitrate'],\n    mode='lines+markers',\n    name='Nitrate',\n    line=dict(color='#D28077'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L',\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['nitrite'],\n    mode='lines+markers',\n    name='Nitrite',\n    line=dict(color='#93C2E2'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['phosphate'],\n    mode='lines+markers',\n    name='Phosphate',\n    line=dict(color='#BCD79D'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\nfig.add_trace(go.Scatter(\n    x=mean_nutr.Date,\n    y=mean_nutr['ammonium'],\n    mode='lines+markers',\n    name='Ammonium',\n    line=dict(color='#036554'),\n    hovertemplate='Season: %{text}&lt;br&gt;Concentration: %{y:.2f} μmol/L'\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    title='Nutrient concentrations trend higher in winter and spring and lower in summer and fall',\n    title_font=dict(family='Arial', size=22, color='white'),\n    title_x=0.5,\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(title='Time', showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=True, linewidth=1, linecolor='white'),\n    yaxis=dict(title='Average Concentration (μmol/L)', showgrid=False, showline=False, linewidth=1, linecolor='white', range=y_axis_range, tickmode='linear', dtick=1),\n    legend=dict(font=dict(size=14, color='white')),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600\n)\n\n# Update hover text with 'Season'\nfig.update_traces(\n    text=mean_nutr['Season']\n)\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\nI want to more closely look at average nutrient concentrations during the el Niño year 2016.\n\n\nCode\n# Filter for the year 2016\nfiltered_2016 = mean_nutr[mean_nutr['year'] == 2016]\n\n# Calculate the average nutrient concentrations\naverage_2016 = filtered_2016[['nitrate', 'nitrite', 'ammonium', 'phosphate']].agg('mean')\n\n# Create a DataFrame with 'Nutrient' and 'Concentration' columns\naverage_2016 = pd.DataFrame({'Nutrient': average_2016.index, 'Concentration': average_2016.values})\n\n# Create the bar chart\nfig, ax = plt.subplots(figsize=(9.3, 6))  # Adjusted figsize\nax.set_facecolor('#333333')  # Set the background color for the plotting area\n\ncolors = ['#D28077', '#93C2E2', '#036554', '#BCD79D']\n\nbars = ax.bar(average_2016['Nutrient'], average_2016['Concentration'], color=colors)\nax.set_xlabel('Nutrient', color='white', fontname='Arial', size = 11, labelpad = 10)\nax.set_ylabel('Average Concentration (μmol/L)', color='white', fontname='Arial', size = 11, labelpad = 10)\n\n# Adjusted title font size (no bold)\nax.set_title('Average Nutrient Concentrations in 2016', color='white', fontname='Arial', fontsize=16)\n\nax.tick_params(axis='x', rotation=0, colors='white')\nax.tick_params(axis='y', colors='white')\n\n# Add value labels on top of the bars\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', color='white', fontsize=10, fontname='Arial')\n\n# Adding white grid lines\nax.yaxis.grid(color='white', linestyle='--', linewidth=0.5)\n\n# Moving grid lines behind the data\nax.set_axisbelow(True)\n\n# Adding white spines (lines along the axes)\nax.spines['bottom'].set_color('white')\nax.spines['left'].set_color('white')\n\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nNext, I want to get a better understanding of ocean depth in the channel. Here I create a histogram of depths. First, though, I will need to average the depth over all time periods grouped by lat and lon. This is because depth remains constant over all years and is thus duplicated in the data set. However, I do not want duplicates in the histogram.\n\n\nCode\n# Group by \"lat\" and \"lon\" and calculate the average of the \"depth\" column\ngrouped_data = full_synth_df.groupby(['lat', 'lon'])['depth'].mean().round()\n\n# Because some grid cells overlap with land, the value is greater than zero, but I want to ceiling the data at zero.\n# Convert the grouped data back to a DataFrame\ngrouped_df = grouped_data.reset_index()\n# Replace values greater than zero with zero in the \"depth\" column\ngrouped_df['depth'] = -1 * grouped_df['depth'].apply(lambda x: 0 if x &gt; 0 else x)\n\n# Print the modified DataFrame\nprint(grouped_df)\n\n\n          lat      lon   depth\n0      33.854 -120.646  1971.0\n1      33.854 -120.638  1954.0\n2      33.854 -120.630  1942.0\n3      33.854 -120.622  1931.0\n4      33.854 -120.614  1923.0\n...       ...      ...     ...\n13511  34.566 -120.646    54.0\n13512  34.566 -120.638    23.0\n13513  34.574 -120.646    45.0\n13514  34.582 -120.646    42.0\n13515  34.590 -120.646    46.0\n\n[13516 rows x 3 columns]\n\n\nHere I visualize the depth data in a histogram with plotly! Again, this data was originally in the form of a raster, so each measurement of depth represents a 0.008° x 0.008° grid cell.\n\n\nCode\n# Calculate the histogram manually\nhist, bins = np.histogram(grouped_df.depth, bins=range(0, int(grouped_df['depth'].max()) + 1, 50))\nbin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\nbin_ranges = [f'Range: {bins[i]}-{bins[i + 1] - 1} m' for i in range(len(bins) - 1)]\nhover_text = [f'{bin_ranges[i]}&lt;br&gt;Count: {hist[i]}' for i in range(len(bins) - 1)]\n\n\n# Create the plot\nfig = go.Figure()\n\n# Plotting the data with custom colors and line styles\nfig.add_trace(go.Bar(\n    x=bin_centers,\n    y=hist,\n    hovertext=hover_text,\n    hoverinfo='text',\n    width=bins[1] - bins[0],\n    marker_color='#02a8c9',\n    marker_line=dict(color='rgba(211, 211, 211, 0.5)', width=1)\n))\n\n# Update layout for interactivity\nfig.update_layout(\n    xaxis_title='Average Depth (m)',\n    yaxis_title='Frequency',\n    title='Histogram of Depths in the Santa Barbara Channel',\n    title_font=dict(family='Arial', size=22, color='white'),\n    font=dict(family='Arial', size=14, color='white'),\n    xaxis=dict(showgrid=False, showline=False, linewidth=1, linecolor='white'),\n    yaxis=dict(showgrid=True, gridcolor='rgba(211, 211, 211, 0.5)', showline=False, linewidth=1, linecolor='white'),\n    plot_bgcolor='#333333',\n    paper_bgcolor='#333333',\n    height=600,\n    title_x=0.5,\n    annotations=[\n        dict(\n            x=0.5,\n            y=1.08,\n            showarrow=False,\n            text=\"where every data point represents 0.008° x 0.008° (approximately 1 km)\",\n            xref=\"paper\",\n            yref=\"paper\",\n            font=dict(family='Arial', size=14, color='white')\n        )\n    ]\n)\n\n# Show the interactive plot\nfig.show()\n\n\n\n                                                \n\n\nHere is the depth data as a raster layer.\n\n\nCode\n# Set the font family for the entire plot\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = ['Arial']  # Use Arial font or another available sans-serif font\n\n# Customize the figure background color, axes background color, and text color\nrcParams['figure.figsize'] = (10, 8)  # Set the figure size (width, height) in inches\nrcParams['figure.facecolor'] = '#333333'   # Set background color of the figure\nrcParams['axes.edgecolor'] = '#333333'       # Set color of axes lines to white\nrcParams['axes.labelcolor'] = 'white'      # Set color of axes labels to white\nrcParams['xtick.color'] = 'white'          # Set color of x-axis ticks to white\nrcParams['ytick.color'] = 'white'          # Set color of y-axis ticks to white\nrcParams['text.color'] = 'white'           # Set text color to white\n\n# Open the raster file using rasterio\nwith rasterio.open(raster_path) as src:\n    # Set up colormap and normalization\n    cmap = plt.cm.Blues_r # Reverse the Blues colormap\n    cmap.set_bad(color='#333333')  # Set NaN values to be white\n    norm = plt.Normalize(vmin=-1000, vmax=20)\n\n    # Create a larger figure\n    plt.figure(figsize=(10.5, 8.5))\n    # Add \"Latitude\" and \"Longitude\" labels using plt.text\n    plt.text(0.5, -0.16, 'Longitude', transform=plt.gca().transAxes,\n             ha='center', color='white')\n    plt.text(-0.1, 0.5, 'Latitude', transform=plt.gca().transAxes,\n             va='center', rotation='vertical', color='white')\n\n    # Plot the raster data using rasterio's show function\n    rasterio.plot.show(src,\n                       cmap=cmap,\n                       norm=norm,\n                       title='Depth in the Santa Barbara Channel',\n                       origin='upper')\n                      \n\nplt.show()"
  },
  {
    "objectID": "posts/2022-12-19-climate-ai-debate/index.html",
    "href": "posts/2022-12-19-climate-ai-debate/index.html",
    "title": "Debating Nudging and AI for Climate",
    "section": "",
    "text": "This is a short podcast by myself and Lewis White where we debate using nudging and AI for climate solutions. We chose sides for each topic at random. This podcast was created for a final assignment for EDS 242 Ethics and Bias in Environmental Data Science – a course in UCSB’s Master’s of Environmental Data Science curriculum taught by Dena Montague. Intro music by Bonfire Records and moderation by Jessica French."
  },
  {
    "objectID": "posts/2022-12-19-climate-ai-debate/index.html#references",
    "href": "posts/2022-12-19-climate-ai-debate/index.html#references",
    "title": "Debating Nudging and AI for Climate",
    "section": "References",
    "text": "References\n\nAhmad, M. Usman, Afif Hanna, Ahmed-Zayn Mohamed, Alex Schlindwein, Caitlin Pley, Ingrid Bahner, Rahul Mhaskar, Gavin J. Pettigrew, and Tambi Jarmi. “A Systematic Review of Opt-out Versus Opt-in Consent on Deceased Organ Donation and Transplantation (2006-2016).” World Journal of Surgery 43, no. 12 (December 2019): 3161–71. https://doi.org/10.1007/s00268-019-05118-4.\nBartmann, Marius. “The Ethics of AI-Powered Climate Nudging—How Much AI Should We Use to Save the Planet?” Sustainability 14, no. 9 (January 2022): 5153. https://doi.org/10.3390/su14095153.\nBirdReturns. “BirdReturns.” Accessed December 7, 2022. https://birdreturns.org/. Clifford, Catherine. “More than 80% Say They’d Change Their Behavior to Fight Climate Change, but U.S. Conservatives Lag.” CNBC. Accessed December 7, 2022. https://www.cnbc.com/2021/09/14/climate-change-to-change-behavior-80percent-of-respondents-tell-pew.html.\nHe, Tianzhi, Farrokh Jazizadeh, and Laura Arpan. “AI-Powered Virtual Assistants Nudging Occupants for Energy Saving: Proactive Smart Speakers for HVAC Control.” Building Research & Information 50, no. 4 (May 19, 2022): 394–409. https://doi.org/10.1080/09613218.2021.2012119.\nMcGovern, Amy, Imme Ebert-Uphoff, David John Gagne, and Ann Bostrom. “Why We Need to Focus on Developing Ethical, Responsible, and Trustworthy Artificial Intelligence Approaches for Environmental Science.” Environmental Data Science 1 (2022): e6. https://doi.org/10.1017/eds.2022.5.\nNordgren, Anders. “Artificial Intelligence and Climate Change: Ethical Issues.” Journal of Information, Communication and Ethics in Society ahead-of-print, no. ahead-of-print (January 1, 2022). https://doi.org/10.1108/JICES-11-2021-0106.\nOpen Transcripts. “Harnessing Artificial Intelligence to Target Conservation Efforts - Carla Gomes.” Accessed December 7, 2022. http://opentranscripts.org/transcript/artificial-intelligence-to-target-conservation/.\nResources for the Future. “Nudging Behavior Toward Climate Solutions, with Elke Weber.” Accessed December 7, 2022."
  },
  {
    "objectID": "posts/2023-06-25-flourish-ocean-depth/index.html",
    "href": "posts/2023-06-25-flourish-ocean-depth/index.html",
    "title": "Learning Flourish to Visualize Ocean Depth Data",
    "section": "",
    "text": "Description:\nIn this post, I share how I used Flourish – a data visualization platform – to visualize ocean depth data in the Santa Barbara Channel. I used ocean depth data from the kelpGeoMod project data repository.\n\nSomething New\nI recently came upon a tweet about Flourish which is an online data visualization and storytelling platform. This was the first I’d heard of the platform, and I wanted to try it out as an alternative for other popular proprietary software such as Tableau. I pulled ocean depth data from my master’s capstone project, Developing a Data Pipeline for Kelp Forest Modeling (also known as kelpGeoMod), and started to explore. Here I will share how I created the visualization below.\n\n\n\nGetting the Data\nTo get data to visualize, I navigated back to my masters capstone project data repository and downloaded the ocean depth data in the Santa Barbara Channel. The original data source was the ETOPO Global Relief Model 2022.\n\n\n\n\n\n\n\nGetting Started in Flourish\nI then created a free account with Flourish. With the free version of my account, I have access to a limited set of features, while the full version offers additional functionalities. I then started exploring available features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlourish had numerous templates for visualizing data. I noticed, though, that it is not currently compatability with raster data, so I had to convert my raster data to vector data. This is not ideal, but it was quickly doable with this relatively small raster file.\n\n\nCode\n# Load necessary libraries\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\n\n# Set a data directory\n#dir &lt;- \"./data/\"\n\n# Load depth data\ndepth &lt;- rast(file.path(dir, \"depth.tif\"))\n\n# Vectorize\ndepth_vect &lt;- as.polygons(depth[[1]], \n                          dissolve = FALSE)\n\n# Convert to sf object\ndepth_sf &lt;- st_as_sf(depth_vect) %&gt;% \n  rename(\"depth\" = \"exportImage\") %&gt;% # rename column name\n  filter(depth &lt;= 0) %&gt;% # filter for depths less than zero\n  mutate(depth = round(depth, 3)) # round depth values\n\n# Write to GeoJSON file\n# st_write(obj = depth_sf, \n#          file.path(dir, \"depth.geojson\", \n#          driver = \"GeoJSON\"))\n\n\nI decided to choose the UK hex map template because I wanted to use try out the three-dimensional extrusion feature.\n\n\n\n\n\n\n\nMaking the Map\nOnce you get started, to make the map you’ll have to replace the template data with your own. Name your project, navigate to the data tab, then upload your own GeoJSON data.\n\n\n\n\n\nBe sure to select which columns go with which type of data. Flourish has helpful documentation hints denoted with question mark circles.\n\n\n\n\n\nWhen you’re ready, swap back to the preview mode and start messing with your map’s aesthetics.\n\n\n\n\n\nI chose to edit aesthetics such as the projection, background color, padding, palette, legend, the popup shape, title, and footer. I also added a screenreader description. You can adjust these settings however you would like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should have an export-ready three-dimensional image of ocean depth in the Santa Barbara Channel! Export and enjoy Flourishing!\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{windschitl2023,\n  author = {Windschitl, Elke},\n  title = {Learning {Flourish} to {Visualize} {Ocean} {Depth} {Data}},\n  date = {2023-06-25},\n  url = {https://elkewind.github.io/posts/2023-06-25-flourish-ocean-depth},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nWindschitl, Elke. 2023. “Learning Flourish to Visualize Ocean\nDepth Data.” June 25, 2023. https://elkewind.github.io/posts/2023-06-25-flourish-ocean-depth."
  },
  {
    "objectID": "posts/2023-03-29-art-and-science/index.html",
    "href": "posts/2023-03-29-art-and-science/index.html",
    "title": "Integrating Art into Science and Conservation",
    "section": "",
    "text": "Here I discuss how I integrate art into environmental science and conservation. I like to use creative and quantitative talents in parallel, and the balance between the two has ebbed and flowed over time."
  },
  {
    "objectID": "posts/2023-03-29-art-and-science/index.html#art-and-science",
    "href": "posts/2023-03-29-art-and-science/index.html#art-and-science",
    "title": "Integrating Art into Science and Conservation",
    "section": "Art and Science",
    "text": "Art and Science\nAt the time of writing this, I am a master’s student studying environmental data science at UC Santa Barbara (read my full bio here), but I am also a self-taught digital artist. Science and art have both played integral roles in my life over the past 6 years, and recently I have been exploring ways to integrate the two. In the past, these two realms of interest have been separate, but I have found that my art can supplement environmental science or vice versa to elevate communication and engagement.\nI began experimenting in digital illustration and design in 2017, and learned how to use Adobe Photoshop and Inkscape. Over time, I have progressed to primarily use Procreate and Canva. Additionally, I am an amateur photographer and love to photograph the natural world using a Canon t6i and Adobe Lightroom software.\nInspired by other data science/environmental science artists (such as Allison Horst – check her out!), I have worked to continue creating art alongside developing my quantitative skills. My favorite way to wrap artwork into science is by creating relevant ecological illustrations and adding them into presentations or other resources. Below are some examples of my artwork in relation to environmental science.\n\nHex Stickers:\nI am currently working on a collaborative capstone project modeling habitat suitability for kelp cultivation in the Santa Barbara Channel. I designed a hex sticker for our capstone group. (Unfamiliar with hex stickers? Check out these data science hex stickers created by others.)\n\n\n\n\nEnvironmental Non-Profit Social Media Campaign:\nIn 2022, I assisted with a social media campaign by The LENA Project to provide monthly information and suggestions on environmental related topics. Here are two of the designs I created for this campaign.\n\n\n\n\nEnvironmental Non-Profit Creative Project:\nIn 2021, I participated in (as well as volunteered for) Prompt for the Planet *Chapter 2 – an initiative to create art and write poetry to raise awareness about our changing planet.\n\n\n\nEnvironmental Non-Profit Fundraising:\nIn 2021 I designed a yard sign for The LENA Project that they sold as a fundraiser and as a campaign to promote unfertilized lawns. In 2023 I updated the design for the continuation of the campaign.\n\n\n\n\nJournalism:\nIn 2020, my art was sourced by The Lutrinae in an article about the Monterey Bay Aquarium reopening after shutting down during the pandemic.\n\n\n\nPhotography:\nI use photography as a way to document traveling and share my love for ecology and the natural world.\n\n\n\n\n\n\n\nBlogging:\nAll of my blog posts feature my own illustrations.\n\n\n\nAdditional Illustrations:\nSmaller illustrations have made their way into presentations, twitter posts, stickers, and more."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html",
    "href": "posts/2023-07-02-kelpGeoMod/index.html",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "",
    "text": "Below is the technical documentation for the Bren Master of Environmental Data Science capstone project Developing a Data Pipeline for Kelp Forest Modeling completed in 2023 by myself, Jessica French, Erika Egg, and Javier Patrón."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#abstract",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#abstract",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Abstract",
    "text": "Abstract\nGiant kelp (Macrocystis pyrifera) is an ecosystem engineer that creates complex vertical habitat by growing to approximately 50 m in dense forests. Healthy kelp forests are some of the most diverse ecosystems in the world that also protect coastlines from storms, provide nutrients to beaches, and giant kelp is a promising biofuel precursor that does not take up arable land or use freshwater to grow. Researchers are working to better understand nutrient utilization and cycling in this critical ecosystem and need comprehensive data on nutrient concentrations to further their research. Additionally, kelp aquaculture companies are working to show that giant kelp can be grown as a profitable biofuel precursor in the Santa Barbara Channel. In order to do this they need to grow kelp efficiently in areas that have suitable habitat. This project creates a synthesized data set that can be used and expanded on by researchers to make their data acquisition process more efficient. It also produces estimates of habitat suitability for giant kelp in the Santa Barbara Channel that kelp aquaculture organizations can use to supplement prior analyses and guide where to place future farms."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#executive-summary",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#executive-summary",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Executive Summary",
    "text": "Executive Summary\nGiant kelp (Macrocystis pyrifera) is a foundational species of canopy-forming kelp in the Santa Barbara Channel that provides the structure for some of the most diverse ecosystems in the world (Buschmann et al., 2007). Its tall stipes provide habitat for many species and protect coastlines from storms (Buschmann et al., 2007; Esgro & Ray, 2021). Declines in giant kelp abundance over the past decade have put these ecosystem services at risk and increased the urgency for research (Rogers-Bennett & Catton, 2019; Wernberg et al., 2013). It is also an attractive option for biofuel production because it can grow up to one meter per day and requires no fresh water or arable land (Cuba et al., 2022; Kerrison et al., 2015). \nThis project addressed the needs of two clients. The first was Ph.D student Natalie Dornan, who is researching nutrient utilization and cycling in giant kelp forests in the Santa Barbara Channel. Her goal is to create a nitrogen budget for the area to better understand the sources and sinks for nitrogen in the Santa Barbara Channel. To further her research she needs comprehensive data on nutrient concentrations in the Santa Barbara Channel. The second client was Ocean Rainforest, who is cultivating giant kelp in the Santa Barbara Channel to be used in biofuel production. They are working to prove that giant kelp can be a profitable biofuel precursor which means they need to be able to grow kelp efficiently. To do this they need to know where habitat is suitable for kelp. \nThere have been several long term monitoring efforts in the Santa Barbara Channel that have generated a lot of data on nutrient concentrations. This data is spread over many agency, organization, and research project websites, APIs, and data portals with each storing data in different file formats and providing access to the data in a slightly different way. In addition to access and file formats being inconsistent, observations are often at different spatial and temporal resolutions and in different data structures. For example one organization may provide point data collected quarterly in a text file while another provides raster images collected daily in a netCDF file. Putting this data into a common format so that all of the observations can be used together is tedious and time consuming. \nThis project addressed this problem by creating a synthesized data set of oceanographic factors that impact giant kelp growth designed to streamline research. Publicly available data on nutrient concentrations, sea surface temperature (SST), depth, seafloor habitat, and kelp coverage were obtained from the Santa Barbara Coastal Long Term Ecological Research (SBC LTER), National Oceanic and Atmospheric Administration (NOAA), United States Geological Survey (USGS), California Cooperative Oceanic Fisheries Investigations (CalCOFI), the California State Mapping Project, Earth Research Institute (ERI), National Aeronautics and Space Administration (NASA), and The Group for High Resolution Sea Surface Temperature (GHRSST) and compiled into one data set (Bell, Cavanaugh, Reuman, et al., 2021; Bell, Cavanaugh, & Siegel, 2023; CalCOFI Bottle Database, n.d.; Nearshore Benthic Habitat GIS for the Channel Islands Volume II - Mapped Areas, n.d.; Seafloor Mapping Lab at CSUMB: Data Library Southern California Data (Part II), n.d.; ERI, n.d.; Golden, 2013; JPL MUR MEaSUREs Project, 2015; NOAA National Centers for Environmental Information, 2022; Prouty & Baker, 2020a, 2020b; Washburn et al., 2022). All original data sets were vetted to ensure they had sufficient metadata, spatial coverage, and temporal coverage before being included in the project. Data sets were standardized with respect to resolution (spatial and temporal), units, extent, and coordinate reference system. \nThe standardized nutrient data, kelp area, kelp biomass (kelp biomass is derived from kelp area and does not represent separate observations), sea surface temperature, and depth were shared in both tabular (CSV) and image (raster) format. The first CSV contained the mean observed nutrient values and sea surface temperature values for each year and quarter with estimates of kelp area, kelp biomass, and depth added on. The raster format contained year and quarter mean raster bricks for each variable except depth and substrate which were considered constant. These raster bricks were used to create another CSV file that provided the same information in a more accessible format. By standardizing and combining the data it could be used together to address the need of Ocean Rainforest to know where habitat is most suitable for giant kelp in the Santa Barbara Channel. \nOcean Rainforest completed a habitat suitability analysis in 2018 for offshore locations (further than 3 nm from shore). As they move forward with placing additional kelp aquaculture farms in the Santa Barbara Channel they need an assessment of nearshore habitat suitability for giant kelp. To address this need, this project will produce updated estimates of habitat suitability for giant kelp with data from 2014 to 2022 and covering areas within 5 km of the Santa Barbara Coastline. \nTo model habitat suitability observations of phosphate and combined nitrate and nitrite for each year and quarter were interpolated using inverse distance weighting to generate a quarterly mean across all years for each nutrient. These estimates and depth were used to estimate habitat suitability for giant kelp in the Santa Barbara Channel using a maximum entropy species distribution modeling approach. \nThis resulted in quarterly estimates of habitat suitability for giant kelp on a scale of 0 to 1 that were filtered to locations that met the substrate needs of Ocean Rainforest. Additionally, the relative contribution of each variable to the estimates of habitat suitability (variable importance) was generated to guide future iterations of the model. \nThe synthesized data set created through this project will streamline research on nutrient cycling and utilization in kelp forests in the Santa Barbara Channel by making the data collected through various long term monitoring efforts available in one place. The model outputs will provide an updated estimate of habitat suitability for kelp in an area not covered by previous models. The variable importance will allow industry professionals and researchers to see what is most impacting giant kelp growth in the Santa Barbara Channel and help guide future research. This project was packaged into a well documented github repository with an accompanied google drive data hub to create a seamless pipeline for researchers and industry professionals to use in the future."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#problem-statement",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#problem-statement",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Problem Statement",
    "text": "Problem Statement\nGiant kelp (Macrocystis pyrifera) forests provide a wealth of ecosystem services such as protecting coastlines from storms, bringing nutrients to beaches, and creating habitat that increases biodiversity in the nearshore environment (Buschmann et al., 2007; Cuba et al., 2022; Esgro & Ray, 2021). Giant kelp has evolved to thrive in highly variable environments; it can withstand 15° C changes in sea surface temperatures, periods of limited nutrient availability, and even withstand  severe storms adapting and recovering quickly (Cavanaugh et al., 2019). However, increasing frequency and severity of marine heat waves, El Niño events, and other environmental disturbances are pushing kelp to the limits of what it can withstand (Esgro & Ray, 2021; Rogers-Bennett & Catton, 2019; Wernberg et al., 2013). This became evident when a record breaking marine heat wave between 2014 and 2016 added to a severe El Niño, combined with a plague of sea star wasting disease that allowed kelp’s main predators, sea urchins, to flourish (Cavanaugh et al., 2019; Rogers-Bennett & Catton, 2019). This resulted in the decimation of kelp forests along the California coast with areas losing up to 90% of their kelp canopy (Cavanaugh et al., 2019). \nThis has motivated researchers and the kelp aquaculture industry to gain a better understanding of kelp ecology in a changing climate and to identify locations that could support kelp restoration projects or kelp farms. The clients for this project are seeking to do just that. Ph.D student Natalie Dornan is studying nutrient cycling and utilization in kelp forests with the goal of developing a spatiotemporal model of nitrogen in the Santa Barbara Channel. Additionally, the pioneering blue growth company Ocean Rainforest is working to make giant kelp a profitable biofuel precursor that can be grown in the Santa Barbara Channel. In order to accomplish their goals, they need data on oceanographic factors that impact kelp growth in the Santa Barbara Channel and estimates of locations that have suitable habitat for giant kelp. \nThere are many factors that impact habitat suitability for giant kelp. Among these are nutrient concentrations for nitrate, nitrite, ammonium, and phosphorus, sea surface temperature, and substrate type (Brzezinksi et al., 2013; Buschmann et al., 2007; Cavanaugh et al., 2019; Peters et al., 2019). While the nutrient and temperature requirements of naturally occurring kelp and cultivated kelp are the same, the substrate needs are not. Kelp farms, such as those operated by Ocean Rainforest, need soft substrate to place their infrastructure on so that it does not disturb natural kelp habitat or protected rocky reef habitat.  Natural kelp typically attaches to and grows from rocky substrate. This creates a challenge in assessing where giant kelp habitat is suitable based solely on where it naturally occurs. \nData on oceanographic factors such as nitrogen and phosphorus concentrations, sea surface temperature, seafloor substrate, and ocean depth are available through several long term monitoring efforts in the Santa Barbara Channel. The challenge is that different data sources provide data in different formats, at different spatial and temporal resolutions, and stored and accessed in slightly different ways. Synthesizing the available data so that all of the observations can be used simultaneously will create the most complete picture possible of conditions in the Santa Barbara Channel and identify data gaps that can be addressed by future research. \nThe synthesized data set of oceanographic factors will then be used to meet the second challenge of identifying suitable habitat for giant kelp. Current models of habitat suitability that Ocean Rainforest uses were completed in 2018 and did not cover the nearshore environment. The model created through this project will provide predictions with data collected through 2022 and estimates of kelp habitat suitability within 5 km of shore. This updated model, when combined with maps of seafloor substrate, will provide needed information to the kelp aquaculture industry and researchers when exploring where to locate future projects."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#specific-objectives",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#specific-objectives",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Specific Objectives",
    "text": "Specific Objectives\n\nSynthesize currently available data on kelp forest distribution and oceanographic factors in the Santa Barbara Channel into one standardized data set that can be easily used, reused, and updated by researchers and kelp farm industry professionals. After downloading the data set researchers can easily incorporate additional variables and more current data to meet their research needs. \nCreate a model of giant kelp habitat suitability in the Santa Barbara Channel that will provide an update to analyses completed in 2018, in an area not covered previously, and will account for the differing substrate needs of naturally occurring and cultivated kelp."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-solution-design",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-solution-design",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Summary of Solution Design",
    "text": "Summary of Solution Design\n\nObjective 1: Synthesized Data Set\nData on kelp area, kelp biomass (derived from kelp area), depth, sea surface temperature (hereafter referred to as SST), nutrient concentrations, substrate, and regulatory boundaries were downloaded from open source data repositories and research projects. A summary of the source, original format, and final format is provided in Table S1. \nData that were collected over multiple years were filtered to observations over the 2014-2022 period. This time period was chosen because of the sharp declines in kelp coverage between 2014 and 2016 as well as 2022 being the most recent full year of data. All data were filtered spatially to observations made within the Santa Barbara Channel (Coordinates used to delimit the Santa Barbara Channel 33.85°- 34.59°N, 118.80°- 120.65°W) (Fig. 1). Only nutrient observations made in the top 10 m of water were included. Where multiple observations were made at different depths in the same location the mean of all points in the top 10 m of water were included. All data that did not have WGS 84 as the coordinate reference system were reprojected to WGS 84. Observations for variables that were spread across multiple data sets and/or multiple files were combined into one file. Multiple files from the same data set were combined into one file. This resulted in one file per data set for all variables that would be used in creating the synthesized data set in subsequent steps. \n\n\n\nFigure 1: Map of the Santa Barbara Channel. The blue box outlines the following coordinates 33.85°- 34.59°N, 118.80°- 120.65°W\n\n\nNutrient observations taken on different temporal scales were aggregated to year and quarter (Quarter 1 = Jan - Mar, Quarter 2 = Apr - Jun, Quarter 3 = Jul - Sep, Quarter 4 = Oct - Dec). Where the same point was sampled more than once in one quarter the mean of the values was reported for that point in that year and quarter. All nutrient data set files were then combined by stacking the rows of each data set together for CSV export. Additionally nutrient observations were converted from point format to raster format at a resolution of 0.008° by assigning the value at that point to the grid cell it intersects for continuity with other data sets. \nData on kelp area and kelp biomass were extracted from a netCDF file and assembled into a data frame. This data frame was then converted into two rasterStacks. Values within these rasterStacks were aggregated by sum from the original resolution of 30 m to roughly the desired resolution and then resampled to the exact resolution of 0.008° (approximately 1 km). The original temporal resolution was year and quarter and was not changed. Depth estimates as of 2022 were resampled from 15 arcsecond resolution to 0.008°. SST data were aggregated from daily estimates to year and quarter by taking the mean of all daily estimates that were within the quarter for each grid cell and resampled to 0.008° resolution using the nearest neighbor method. The combined nutrient point observations were intersected with the kelp area, kelp biomass, depth, and SST rasters to estimate values for these variables at each point and exported in CSV format. \nPoint observations of nutrients were interpolated using inverse distance weighting with an inverse distance power of 1. Measurements of nitrate and nitrite were combined by summing to create total nitrogen measurement. Points were aggregated over all years by quarter to maximize the number of observations available to estimate the nutrient concentration. This assumes that nutrient concentrations in the same location in different years would be roughly the same, and is a notable limitation of this method. For phosphate and nitrogen the maximum distance a cell could be from a data point and still have a value estimated (hereafter referred to as the maximum distance parameter) was set to 0.008° (approx. 10 km) (Brzezinksi et al., 2013; Peters et al., 2019). The maximum distance parameter for ammonium was set to 0.04° because it is more spatially variable than phosphate and nitrogen (Brzezinksi et al., 2013; Peters et al., 2019). Values were estimated for cells within 5 km (Fig. 2) of the Santa Barbara coastline at a resolution of 1 km, however the estimate could be based on point values that were further away, up to the maximum distance parameter set for the nutrient being estimated. \n\n\n\nFigure 2: Map of Interpolation Area. Interpolation was done within 5 km of the Santa Barbara Coast. The interpolated area is shown in green.\n\n\nYear and quarter raster data for nutrients, SST, kelp area, and kelp biomass were converted to tabular format by assigning the value of each grid cell to a latitude and longitude within that grid cell. Estimates of depth as of 2022 were converted to tabular format in the same way and joined to the nutrient, SST, kelp area, and kelp biomass data. This was exported as a single file in CSV format. \nSubstrate data stored in a shapefile were reclassified to rock substrate, soft substrate, mixed substrate, and anthropogenic substrate based on the reported induration or description of each polygon. The reclassified data were then converted to raster at a resolution of 0.00003° (approximately 3.3 m). Substrate data stored as Esri layer files were loaded into QGIS and saved as GeoTIFF files. These rasters (riginal resolutions of 2 m, 3 m, or 5 m) were resampled using the nearest neighbor method to 0.00003° resolution. The resulting files were combined and exported in GeoTIFF format. \nThe final synthesized data set was provided in CSV and GeoTIFF rasterStack formats. The CSV format contained one file with the observed nutrient concentrations and SST aggregated to year and quarter, with estimates of kelp area, kelp biomass, and depth added for each point as described above. The GeoTIFF format contains a series of raster bricks where each brick contains the measurements for one variable and each layer in a stack represents a year and quarter. The variables contained in the series are kelp area, kelp biomass, nutrients, and SST. This series of raster stacks was combined in CSV format where each row represents the value of a cell at a year and quarter and each column represents a variable. The combined substrate file is provided separately because the observations are categorical with discrete boundaries and could not be resampled to 0.008° resolution and maintain accuracy of all substrate categories (The soft substrate category covers much larger continuous areas compared to hard, mixed, and anthropogenic and was converted to  0.008° resolution for analysis).\nAn additional sandy-bottom substrate raster was created at the 0.008° resolution level for later analyses. This was done by first reclassifying to sandy (1) and non-sandy (0) substrates and aggregating to a near 0.008° resolution by mean. The raster was resampled to the mask to get a perfect resolution of 0.008° by the nearest neighbor method. Then the raster was reclassified so cells with values less than 1 due to containing non-sandy cells were assigned 0, and cells with values of 1 remained at 1 – the sandy-bottom substrate.\n\n\nObjective 2: Habitat Suitability Model \nOnce synthesized, this data set was used to model kelp habitat suitability in the Santa Barbara Channel via a maximum entropy species distribution model called Maxent (citation 12) – a pre-developed machine learning algorithm (Kass et al., 2023). This modeling approach was chosen because it allows the user to generate a predicted habitat suitability for a species based on continuous environmental variables, such as the GeoTIFF files created in the synthesized data set (Kass et al., 2023; Melo-Merino et al., 2020; Phillips et al., 2017; Watt, 2018). Although the input kelp area was remotely sensed, it did not fully cover the area of interest, and growing kelp that hadn’t reached the surface would not be detected (Cavanaugh et al., 2021). Therefore, it was assumed that the measurements of kelp area represented definitive presence but not definitive absence, and thus was appropriate for a presence-only model like Maxent (Kass et al., 2023; Watt, 2018).\nMaxent outputs a probability distribution heatmap of predicted habitat suitability for the species of interest (Elith et al., 2011; Kass et al., 2023; Melo-Merino et al., 2020). Habitat suitability for giant kelp was predicted for each quarter independently to account for drastic seasonal changes in ocean nutrient distribution (Brzezinksi et al., 2013; Buschmann et al., 2007; Peters et al., 2019). Predictions were generated for a 1 km resolution grid that extended 5 km from the Santa Barbara coastline. \nTo prepare the kelp occurrence data for the model, the kelp area was averaged over the time period of interest (2014-2022) by quarter. Then it was converted from a continuous raster to a data frame where the kelp area of each cell was assigned to a row and a point within the cell was assigned as the latitude and longitude (Watt, 2018). A value greater than zero was treated as an observation of presence. Additionally, the interpolated nitrogen, interpolated phosphate, and depth were combined in folders such that each tif file represented a variable and each folder represented a quarter. \nThe interactive web application Wallace was used to perform the initial runs for Maxent modeling and model selection (Kass et al., 2023). The application uses the maxent and ENMeval packages available in R to run different versions of the model and calculate evaluation metrics. Wallace also makes all code used in the modeling process available to download so that the process is fully reproducible. This reproducible code was saved and updated to be contained in the kelpGeoMod pipeline (Kass et al., 2023; Melo-Merino et al., 2020). \nIn order to find the model with the best performance, a k = 4 checkerboard spatial partition was used to first train then test the data. Linear, Quadratic, Hinge, and Product feature classes were allowed to be applied to the data and regularization multipliers between 0.5 and 4.5 at a step value of 0.2 were used. Clamping was not employed, so the model was not constrained to environmental values seen in the training data.\nThe model with the best predictive performance for all quarters based on minimizing the AIC  had a regularization parameter of 0.5 and allowed linear and quadratic feature classes to be applied to the data. The raw output of maxent modeling is a ratio of the probability density of covariates across the landscape of interest with kelp occurrence over the probability density of covariates across the whole landscape of interest (Elith et al., 2011).In this form the Maxent output is challenging to interpret intuitively so a cloglog transformation was applied to the model raw output so that it could be interpreted as predicted habitat suitability where each grid cell had a value between 0 and 1 (Elith et al., 2011; Kass et al., 2023). This type of transformation is recommended for interpretability in the Maxent and Wallace documentation (Kass et al., 2023; Phillips et al., 2017).\nThe outputs were exported as GeoTIFF files and used to create heat maps that combine predicted habitat suitability,  and substrate type in the Santa Barbara Channel. This will allow Ocean Rainforest and researchers to see areas where in the Santa Barbara Channel habitat is suitable for kelp where it does not occur naturally. Combining this with maps of seafloor substrate will allow Ocean Rainforest to identify potential areas for kelp farm placement that will not disturb existing kelp habitat and have the soft seafloor substrate that is required for farm infrastructure. Similarly it will allow researchers to identify areas of potential rocky reef habitat where kelp restoration projects are most likely to be successful. The maps created were exported as GeoTIFF files. \nAdditionally, metrics of feature importance were pulled from the model outputs. This information will be useful to Ocean Rainforest as they consider what variables are most important to have in a given location when siting kelp aquaculture farms."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#products-and-deliverables",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#products-and-deliverables",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Products and Deliverables",
    "text": "Products and Deliverables\n\nSynthesized Data Set\nThe synthesized data set was provided in two formats to maximize flexibility of use and to align with the various formats of the original data. The first is a CSV file that contains all of the observed nutrient values from the original data sets and observed SST with estimates of  kelp area, kelp biomass, and depth from the raster cell that the observation intersects aggregated to year and quarter. This data set brings together observations of nutrient concentrations from CalCOFI, ERI, SBC LTER, and USGS in a single file (Fig. 3) (CalCOFI Bottle Database, n.d.), ERI (ERI, n.d.), SBC LTER (Bell, Cavanaugh, Reuman, et al., 2021; Washburn et al., 2022). Adding estimates of kelp area,  kelp biomass, and depth will make it easier to investigate relationships between nutrient concentrations and kelp forest cover. This data set is available for viewing or to download at this link. \n\n\n\nFigure 3: Location of points in observed nutrient data set. Map depicting the locations that nutrient measurements were taken, points are colored by the organization or research project that collected the data.\n\n\nThe second format of the synthesized data set was a series of GeoTIFF raster stacks at 0.008° resolution. The data for each variable; kelp area (Fig. 4), kelp biomass, SST (Fig. 5), nitrogen (Fig. 6), ammonium, and phosphate, were represented by a series of raster bricks where each layer contained the estimates for each year and quarter.\n\n\n\nFigure 4: Kelp area raster brick. Diagram of kelp area raster brick, darker green indicates higher kelp area.\n\n\n\n\n\nFigure 5: SST raster brick. Diagram of SST raster brick. Temperatures range from 13 °C to 19° C with highest temperatures in red and lower temperatures in blue.\n\n\n\n\n\nFigure 6: Nitrogen raster brick. Diagram of nitrogen raster brick. Observations are depicted in red with darker colors showing higher nitrogen concentrations.\n\n\nIn addition to the series of raster bricks, estimates of depth (Fig. 7)  and substrate (Fig. 8) observations were provided as GeoTIFF raster layers. Depth was provided at a resolution of 0.008° and substrate at a resolution of 0.00003°.\n\n\n\nFigure 7: Depth layer. Depth layer as of 2022.\n\n\n\n\n\nFigure 8: Combined substrate layer. Substrate layer with different substrate classifications (soft, hard, mixed, and anthropogenic) shown in colors according to the attached legend.\n\n\n\n\nModel Outputs\nIn order to model habitat suitability for giant kelp within 5 km of the Santa Barbara Coastline, nutrient observations were interpolated as described above resulting in quarterly mean raster layers of nitrogen (Fig. 9), phosphate (Fig. 10), and ammonium (Fig. 11). A comparison of the root mean square error (RMSE) for each of these layers was estimated and is provided in tables (Table 1, Table 2, and Table 3). \n\n\n\nFigure 9: Quarterly interpolation of nitrogen. Result of nitrogen interpolation for each quarter that was used as inputs to maxent.\n\n\n\n\n\nTable 1: Performance of quarterly nitrogen interpolation. Performance of quarterly nitrogen interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\n\n\n\nFigure 10: Quarterly interpolation of phosphate. Result of phosphate interpolation for each quarter that was used as inputs to maxent.\n\n\n\n\n\nTable 2: Performance of quarterly phosphate interpolation. Performance of quarterly phosphate interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\n\n\n\nFigure 11: Quarterly interpolation of ammonium. Result of ammonium interpolation. These layers were not used in modeling.\n\n\n\n\n\nTable 3: Performance of quarterly phosphate interpolation. Performance of quarterly ammonium interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\nEstimates of habitat suitability for giant kelp were provided for each quarter at 0.008° resolution on a scale of 0 to 1. Estimates for each quarter were filtered to determine habitat suitability in areas that have soft substrate. A comparison of these estimates for each quarter are provided below (Fig. 12, Fig. 13, Fig. 14, Fig. 15). Additionally, variable importance was determined for each quarter and is shown in tables (Table 4, Table 5, Table 6, Table 7). The estimates of habitat suitability generally showed higher habitat suitability near the coast that was variable throughout the year. The variable importance showed that depth was primarily driving habitat suitability with nitrogen concentration being the second most important. \n\n\n\nFigure 12: Habitat suitability quarter 1. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 4: Variable importance of habitat suitability in quarter 1.\n\n\n\n\n\nFigure 13: Habitat suitability quarter 2. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 5: Variable importance of habitat suitability in quarter 2.\n\n\n\n\n\nFigure 14: Habitat suitability in quarter 3. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 6: Variable importance of habitat suitability in quarter 3.\n\n\n\n\n\nFigure 15: Habitat suitability quarter 4. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 7: Variable importance of habitat suitability in quarter 4.\n\n\n\n\nData pipeline\nIn order to make this project reproducible and easy for the clients to continue in the future, all of the code used to create each data product and the model results is provided in an open source GitHub repository (link). To aid others in navigating the project a comprehensive user guide (link) and project schematic were also created and made publicly available. The combination of the GitHub repository, user guide, and project schematic are the data pipeline that will make it possible for future users to use and build upon the project."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-testing",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-testing",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Summary of Testing",
    "text": "Summary of Testing\n\nData Testing\nTests were performed to ensure that each raster or shape met the following criteria after completing the data cleaning process established in the project repository:\n\n\n\n\n\n\n\nData type\nTest\n\n\n\n\nRaster\nCRS = WGS84\n\n\nRaster\nExtent = (xmin = -120.65, xmax = -118.80, ymin = 33.85, ymax = 34.59)\n\n\nRaster\nResolution = 0.008 x 0.008\n\n\nRaster\nOrigin = -0.002, 0.002\n\n\nRaster\nSST between 0° and 100° Celsius\n\n\nRaster\nNutrients &gt;= 0\n\n\nVector\nCRS = WGS84\n\n\nVector\nAt least one data point within (xmin = -120.65, xmax = -118.80, ymin = 33.85, ymax = 34.59)\n\n\nVector\nSST, between 0° and 100° Celsius\n\n\nVector\nNutrients &gt;= 0\n\n\n\nThese tests ensure the success of the data cleaning process and provide a way for future users to check that any data they update is compatible with the synthesized data set and existing code. \n\n\nModel \nThe root mean squared error (RMSE) of each interpolated layer used as inputs to the model was calculated to compare to the RMSE of the underlying data. The performance of the species distribution model was tested using the checkerboard 2 spatial partitioning method with the ENMeval package in R. This allowed the giant kelp occurrence data to be split into training and test data sets within the modeling process."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#user-documentaion",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#user-documentaion",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "User Documentaion",
    "text": "User Documentaion\nAll code used in this project was documented in R script and RMarkdown files. Each file contains information such as a description of the purpose of the code, the source of the data files used, and thorough code comments explaining each operation. Folders in our GitHub repository contain README.txt files with brief overviews of the contents within. \nRelevant metadata for each raw data file were compiled and used to create README.txt files for each data set used. Metadata includes information on the abstract, methods, spatial coverage/resolution, temporal coverage/resolution, descriptions for variables used, links to the original data source, and contact information for associated researchers. Attribute descriptions for each data set when applicable or useful were compiled and also added to the applicable README.txt file. For raw data sets, we did not include attribute information, as in most cases it could be found by looking at the original data sources and often there were many attributes not used in the scope of our project. A similar process was completed for all intermediate and analysis data created throughout the project. README.txt files are included in each data-related folder when applicable.\nA README.md for the overall project repository contains an overview of key information and summarizes how to use the repository. A user guide, in both pdf and txt format, is available here). This guide describes how the raw data types were accessed, cleaned, synthesized and prepared for modeling. It also provides guidance for users to add their own data and prepare it for Maxent modeling if they wish. Moreover, it describes the process for synthesizing the final data set and generating the final model and visualizations. This will ensure that future users will be able to incorporate their own data into the data set and explore their own models."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#archive-access",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#archive-access",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Archive Access",
    "text": "Archive Access\nAll of the data sets used in this project are open-source and available for public use through each research project, organization, and agency website, API, or website. The data products created through this project are also publicly available on Google Drive at this link. To facilitate data sharing and reuse, detailed README files with access and functionality information across all of our files were included. The final synthesized and standardized data sets, as well as the species distribution model outputs, were made available in both GeoTIFF and CSV formats. All code used to create the datasets and model outputs is available on the project’s GitHub repository available here. This combined with a comprehensive user guide and project schematic will make this project easily reproducible and flexible for future users. \nFurthermore, the final product and data set were published under the Creative Commons Zero (CC0) intellectual property laws to enable public use. This was done to promote transparency, facilitate data sharing, and enable external users, such as researchers, stakeholders, and kelp farmers, to access and use our project’s data."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#acknowledgements",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#acknowledgements",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe acknowledge the Bren School of Environmental Science & Management’s Master’s of Environmental Data Science program at the University of California Santa Barbara for funding this synthesis project. We would also like to express our gratitude to our clients, Courtney Schatzman from Ocean Rainforest and Natalie Dornan from UCSB interdepartmental Graduate Program in Marine Science (IGPMS), as well as Sidney Gerst and Kirby Bartlett for providing user information, Jeff Massen for user testing and being the main contact as a kelp farmer, and Daphne Virlar-Knight from NCEAS for helping us explore different options for MaxEnt modeling. We are also grateful to Tamma Carleton, Shubhi Sharma and Kevin Winner for their expertise and guidance in statistics and species distribution modeling. Lastly, we are grateful to Dr. Li Kui and Carrie Bretz for their assistance in accessing and processing data from the SBC LTER and Seafloor Mapping Lab at California State University, Monterey Bay."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#references",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#references",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "References",
    "text": "References\nBell, T., Cavanaugh, K., Reuman, D., Castorani, M., Sheppard, L., & Walter, J. (2021). SBC LTER: REEF: Macrocystis pyrifera biomass and environmental drivers in southern and central California (Version 1) [Data set]. Environmental Data. https://doi.org/10.6073/pasta/27e795dee803493140d6a7cdc3d23379\nBell, T., Cavanaugh, K., & Siegel, D. (2023). SBC LTER: Time series of quarterly NetCDF files of kelp biomass in the canopy from Landsat 5, 7 and 8, since 1984 (ongoing) (Version 20) [Data set]. Environmental Data Initiative. https://doi.org/10.6073/pasta/41f330ccf66fa8c05fc851862e69b1da\nBrzezinksi, M., Reed, D., Harrer, S., Rassweiler, A., Melack, J., Goodridge, B., & Dugan, J. (2013). Multiple Sources and Forms of Nitrogen Sustain Year-Round Kelp Growth on the Inner Continental Shelf of the Santa Barbara Channel. Oceanography, 26(3), 114–123. https://doi.org/10.5670/oceanog.2013.53\nBuschmann, A., Graham, M., & Vásquez, J. (2007). Global Ecology of the Giant Kelp Macrocystis (pp. 39–88). https://doi.org/10.1201/9781420050943.ch2\nCalCOFI Bottle Database. (n.d.). [Data set]. Retrieved June 8, 2023, from https://calcofi.org/data/oceanographic-data/bottle-database/\nCavanaugh, K. C., Bell, T., Costa, M., Eddy, N. E., Gendall, L., Gleason, M. G., Hessing-Lewis, M., Martone, R., McPherson, M., Pontier, O., Reshitnyk, L., Beas-Luna, R., Carr, M., Caselle, J. E., Cavanaugh, K. C., Flores Miller, R., Hamilton, S., Heady, W. N., Hirsh, H. K., … Schroeder, S. B. (2021). A Review of the Opportunities and Challenges for Using Remote Sensing for Management of Surface-Canopy Forming Kelps. Frontiers in Marine Science, 8. https://www.frontiersin.org/articles/10.3389/fmars.2021.753531\nCavanaugh, K. C., Reed, D. C., Bell, T. W., Castorani, M. C. N., & Beas-Luna, R. (2019). Spatial Variability in the Resistance and Resilience of Giant Kelp in Southern and Baja California to a Multiyear Heatwave. Frontiers in Marine Science, 6. https://www.frontiersin.org/articles/10.3389/fmars.2019.00413\nCuba, D., Guardia-Luzon, K., Cevallos, B., Ramos-Larico, S., Neira, E., Pons, A., & Avila-Peltroche, J. (2022). Ecosystem Services Provided by Kelp Forests of the Humboldt Current System: A Comprehensive Review. Coasts, 2(4), 259–277. https://doi.org/10.3390/coasts2040013\nElith, J., Phillips, S. J., Hastie, T., Dudík, M., Chee, Y. E., & Yates, C. J. (2011). A statistical explanation of MaxEnt for ecologists. Diversity and Distributions, 17(1), 43–57. https://doi.org/10.1111/j.1472-4642.2010.00725.x\nERI. (n.d.). Plumes & Blooms [Data set]. Retrieved June 8, 2023, from http://www.oceancolor.ucsb.edu/plumes_and_blooms/\nEsgro, M., & Ray, J. (2021). For Protecting and Restoring California’s Kelp Forests (p. 19).\nGolden, N. E. (2013). California State Waters Map Series Data Catalog: U.S. Geological Survey Data Series 781 [Data set]. U.S. Geological Survey. https://doi.org/10.3133/ds781\nJPL MUR MEaSUREs Project. (2015). GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (4.1) [Data set]. https://doi.org/10.5067/GHGMR-4FJ04\nKass, J. M., Pinilla-Buitrago, G. E., Paz, A., Johnson, B. A., Grisales-Betancur, V., Meenan, S. I., Attali, D., Broennimann, O., Galante, P. J., Maitner, B. S., Owens, H. L., Varela, S., Aiello-Lammens, M. E., Merow, C., Blair, M. E., & Anderson, R. P. (2023). wallace 2: A shiny app for modeling species niches and distributions redesigned to facilitate expansion via module contributions. Ecography, 2023(3), e06547. https://doi.org/10.1111/ecog.06547\nKerrison, P. D., Stanley, M. S., Edwards, M. D., Black, K. D., & Hughes, A. D. (2015). The cultivation of European kelp for bioenergy: Site and species selection. Biomass & Bioenergy, 80, 229–242. https://doi.org/10.1016/j.biombioe.2015.04.035\nMelo-Merino, S. M., Reyes-Bonilla, H., & Lira-Noriega, A. (2020). Ecological niche models and species distribution models in marine environments: A literature review and spatial analysis of evidence. Ecological Modelling, 415, 108837. https://doi.org/10.1016/j.ecolmodel.2019.108837\nNearshore Benthic Habitat GIS for the Channel Islands Volume II - Mapped Areas. (n.d.). [Data set]. Retrieved June 8, 2023, from https://pubs.usgs.gov/of/2005/1170/catalog.html\nNOAA National Centers for Environmental Information. (2022). ETOPO 2022 15 Arc-Second Global Relief Model. NOAA National Centers for Environmental Information [Data set]. DOI: https://doi.org/10.25921/fd45-gt74\nOpenAI. “ChatGPT.” https://openai.com/blog/chat-gpt/\nPeters, J. R., Reed, D. C., & Burkepile, D. E. (2019). Climate and fishing drive regime shifts in consumer-mediated nutrient cycling in kelp forests. Global Change Biology, 25(9), 3179–3192. https://doi.org/10.1111/gcb.14706\nPhillips, S. J., Anderson, R. P., Dudík, M., Schapire, R. E., & Blair, M. E. (2017). Opening the black box: An open-source release of Maxent. Ecography, 40(7), 887–893. https://doi.org/10.1111/ecog.03049\nProuty, N. G., & Baker, M. C. (2020a). CTD profiles and discrete water-column measurements collected off California and Oregon during NOAA cruise RL-19-05 (USGS field activity 2019-672-FA) from October to November 2019 (ver. 2.0, July 2022) [Data set]. U.S. Geological Survey. https://doi.org/10.5066/P9JKYWQU\nProuty, N. G., & Baker, M. C. (2020b). CTD profiles and discrete water-column measurements collected off California and Oregon during NOAA cruise SH-18-12 (USGS field activity 2018-663-FA) from October to November 2018 (ver. 3.0, July 2022) [Data set]. U.S. Geological Survey. https://doi.org/10.5066/P99MJ096\nRogers-Bennett, L., & Catton, C. A. (2019). Marine heat wave and multiple stressors tip bull kelp forest to sea urchin barrens. Scientific Reports, 9(1), Article 1. https://doi.org/10.1038/s41598-019-51114-y\nSeafloor Mapping Lab at CSUMB: Data Library Southern California Data (Part II). (n.d.). [Data set]. Retrieved June 8, 2023, fromhttp://seafloor.otterlabs.org/SFMLwebDATA_s.htm\nWashburn, L., Brzezinksi, M., Carlson, C., & Siegel, D. (2022). SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000 (Version 27) [Data set]. Environmental Data Initiative. https://doi.org/10.6073/pasta/8b74750eed1af2b987e02b4b466e12e7\nWatt, D. (2018, October 9). Preparing Data for MaxEnt Species Distribution Modeling Using R. Azavea. https://www.azavea.com/blog/2018/10/09/preparing-data-for-maxent-species-distribution-modeling-using-r/\nWernberg, T., Smale, D. A., Tuya, F., Thomsen, M. S., Langlois, T. J., de Bettignies, T., Bennett, S., & Rousseaux, C. S. (2013). An extreme climatic event alters marine ecosystem structure in a global biodiversity hotspot. Nature Climate Change, 3(1), Article 1. https://doi.org/10.1038/nclimate1627"
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#appendix-i-supplemental-figures-and-tables",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#appendix-i-supplemental-figures-and-tables",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Appendix I: Supplemental Figures and Tables",
    "text": "Appendix I: Supplemental Figures and Tables\nTable S1. Summary of key information related to the raw data sets used\n\n\n\nData set name\nSource\nVariable\nOriginal Format\nFinal Format(s)\n\n\n\n\n\n\nFile\nResolution\nFile\nResolution\n\n\nSBC LTER: Time series of quarterly NetCDF files of kelp biomass in the canopy from Landsat 5, 7 and 8, since 1984 (ongoing)\nSanta Barbara Coastal Long Term Ecological Research\nKelp area/biomass\nnetCDF\n\n\n30 m x 30 m\nQuarterly\n\n\nSBC LTER: REEF Macrocystis pyrifera biomass and environmental drivers in southern and central California\nSanta Barbara Coastal Long Term Ecological Research\nNitrate\nCSV\n\n\nPoints\nQuarterly\n\n\nSBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles\nSanta Barbara Coastal Long Term Ecological Research\nNitrate + nitrite,\nPhosphate, Ammonium\n\n\nText\n\n\nPoints\nMonthly\n\n\nETOPO Global Relief Model 2022 (Bedrock 15 arcseconds)\nNational Oceanic and Atmospheric Administration\nOcean depth,\nGeoTIFF\n15 arcseconds\n\n\nWater-column environmental variables and accompanying discrete CTD measurements collected off California and Oregon during NOAA cruise SH-18-12\nUnited States Geological Survey\nNitrate + Nitrite, phosphate\n\n\nCSV\n\n\nPoints\nAnnual (measurements taken in fall)\n\n\nCalifornia Cooperative Oceanic Fisheries Investigations – Bottle Database\nCalifornia Cooperative Oceanic Fisheries Investigations\nNitrate, nitrite,\nNItrate + Nitrite ammonia, phosphate\nCSV\n\n\nPoints\nQuarterly\n\n\nPlumes and Blooms\nEarth Research Institute\nNitrite,\nNitrate + Nitrite , phosphate\nCSV\n\n\nPoints\nMonthly\n\n\nGHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1)\nNational Aeronautics and Space Administration and The Group for High Resolution Sea Surface Temperature\nSea surface temperature\nnetCDF\n\n\n0.01° x 0.01°\nDaily\n\n\nCalifornia State Waters Map Series Data Catalog\nUnited States Geological Survey\nSubstrate\nShape file\n\n\n\n\nNearshore Benthic Habitat GIS for the Channel Islands National Marine Sanctuary and Southern California Fisheries Reserves Volume II\nUnited States Geological Survey\nSubstrate\nShape file\n\n\n\n\nSouthern California Data\nCalifornia State Mapping Project\nSubstrate\nEsri layer\n\n\n\n2 m x 2 m\n3 m x 3 m\n5 m x 5 m\n\n\nCalifornia County Boundaries\nCalifornia Open Data Portal\nLand boundaries\nShape file\n\n\n\n\n\nTable S2. Capstone deliverables, descriptions and applications\n\n\n\n\n\n\n\n\nDeliverable\nDescription\nFile Name and Format\n\n\n\n\nSynthesized Data Set\nA CSV file containing all of the observed nutrient values from the original data sets, observed SST, and kelp area and biomass of the raster cell that the observation intersects aggregated to year and quarter.\n\nA series of GeoTIFF raster bricks containing kelp area, kelp biomass, SST, and nutrients for each year and quarter at 0.008° resolution.\n\n\nAn estimate of depth at 0.008° resolution as of 2022.\n\nA CSV file containing the values in the GeoTIFF raster bricks with each row representing a cell at one year and quarter.\n\n\n\nobserved-nutrients-synthesized.csv\n\n\n\n\nkelp-area-brick.tif\nkelp-biomass-brick.tif\nnitrate-nitrite-brick.tif\nphosphate-brick.tif\nsst-brick.tif\nammonium-brick.tif\ndepth.tif\n\nfull-synthesized.csv\n\n\nHabitat Suitability Map\nQuarterly estimates of habitat suitability for giant kelp in all substrate types and for soft substrate only.\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\n\n\nsubstrate-masked-brick.tif\n\n\n\n\n\n\nFigure S1: Project schematic"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "code",
    "section": "",
    "text": "Elke Windschitl\n\n\nEnvironmental Data Scientist\n\n\n\n\n\n\n\n\n\n\n\n.teal-bg-left { background-color: $dark-teal; padding-top: 20px; padding-bottom: 20px; padding-right: 20px; padding-left: 20px; border-radius: 0 25px 25px 0; position: relative; z-index: 3; display: flex; flex-direction: column; justify-content: center; align-items: center; margin-right: 40px; margin-top: 20px; transform: translateX(-100%); /* Start off-screen to the left / transition: transform 0.2s, opacity 0.2s; / Add smooth transitions for both transform and opacity */ }\n.teal-bg-left.slide-in { transform: translateX(0%); /* Move to the original position / opacity: 1; / Fade in */ }\n.teal-bg-left p { margin: 0 0; /* Add margin to separate the paragraphs / display: block; / Ensure the paragraphs display as block elements */ }\n.custom-circle-headshot { position: relative; padding-top: 45px; left: 23%; z-index: 4; /* Place the image above the background / transition: transform 0.2s; / Add a smooth transition effect */ }\n.custom-circle-headshot:hover { transform: scale(1.1); /* Increase the size on hover (1.1 times the original size) */ }\n@media (min-width: 768px) { .custom-circle-headshot { left: 30%; padding-top: 20px; } }\n.clickable-image { margin-bottom: 15px; z-index: 4; transition: transform 0.2s; }\n.clickable-image:hover { transform: scale(1.08); }\n@media (max-width: 768px) { .clickable-image { max-width: 100%; /* Make the image width 100% of its container / max-height: auto; / Allow the image to scale proportionally */ } }\n.modal { display: none; position: fixed; z-index: 4; padding: 100px; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0, 0, 0, 0.9); text-align: center; margin-top: 50px; }\n.close { position: absolute; top: 10px; right: 10px; color: white; font-size: 30px; cursor: pointer; }\n.modal-content { max-width: 50%; /* Adjust this value to make the modal narrower / max-height: 50%; / Adjust this value to make the modal shorter */ margin: 0 auto; }\n.teal-bg-right { background-color: $dark-teal; padding-top: 20px; padding-bottom: 20px; padding-right: 20px; padding-left: 20px; border-radius: 25px 0 0 25px; position: relative; z-index: 3; display: flex; justify-content: center; align-items: center; margin-left: 40px; margin-top: 0px; margin-bottom: 40px; transform: translateX(100%); /* Start off-screen to the left / transition: transform 0.2s, opacity 0.2s; / Add smooth transitions for both transform and opacity */ }\n@media (min-width: 768px) { .teal-bg-right { margin-top: 135px; margin-left: 0;/* Apply margin only on desktop screens */ margin-bottom: 10px; } }\n.teal-bg-right.slide-in { transform: translateX(0%); /* Move to the original position / opacity: 1; / Fade in */ }\n.teal-bg { background-color: $dark-teal; padding-top: 20px; padding-bottom: 20px; padding-right: 20px; padding-left: 20px; border-radius: 25px; margin-top: 30px; }\n.teal-bg2 { background-color: $dark-teal; border-radius: 15px; }\n.gray-bg { background-color: $light-gray; padding-left: 20px; border-radius: 25px; }\n.custom-banner { width: 100%; position: relative; top: -100px; left: 0; z-index: -1; padding: 0; margin: 0; }\n.custom-footer { width: 100%; position: relative; padding-bottom: 0; margin-bottom: 0; z-index: 2; }\n.footer-container { margin-bottom: -35px; /* Adjust this value to match the image height */ }\n@media (max-width: 768px) { .footer-container { margin-bottom: -20px; } }\n.button-container { display: flex; justify-content: center; align-items: center; margin-top: 25px; }\n.light-teal-button { background-color: $light-gray; color: $light-teal; border: 2px solid $light-teal; padding: 10px 20px; border-radius: 5px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; cursor: pointer; margin-right: 25px; margin-left: 25px; }\n.light-teal-button:hover { background-color: $light-teal; /* Change the fill color on hover / color: $light-gray; / Change the text color on hover */ }\n@media (max-width: 768px) { .button-container { flex-direction: column; /* Stack buttons vertically on mobile / text-align: center; / Center button text on mobile */ }\n.light-teal-button { margin: 10px 0; } }\n.menu { position: fixed; top: 50%; right: 10px; transform: translateY(-90%); background-color: $dark-teal; padding: 10px; border: 1px solid $dark-teal; border-radius: 5px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2); z-index: 4; }\n.menu li { margin: 5px 0; /* Adjust spacing between items */ list-style-type: none; }\n.menu a { text-decoration: none; color: $light-gray }\n@media (max-width: 768px) { .menu { display: none; /* Hide the menu completely on mobile */ } }\n.custom-list { list-style-type: disc; line-height: 1.2; padding-top: 0; padding-right: 10px; margin-top: 0; font-size: 14px; }\n.custom-list li { margin-bottom: 5px; padding-top: 0; margin-top: 0; font-size: 14px; }"
  }
]