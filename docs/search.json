[
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html",
    "href": "posts/2023-03-23-eel-sdm/index.html",
    "title": "Boosting to Predict Eel Distribution",
    "section": "",
    "text": "In this post, I attempt to reproduce work by Elith et al. 2008 [1] to model species distribution of short-finned eel (Anguilla australis) using Boosted Regression Trees. This analysis was created for an assignment for EDS 232 Machine Learning in Environmental Scinece – a course in UCSB’s Master’s of Environmental Data Science curriculum taught by Mateo Robbins.\nBoosting is a popular machine learning algorithm that builds models sequentially based on information learned from the previous model. Here, decision trees will be built in sequence using extreme gradient boosting to classify presence or absence of short-finned eel in a given location associated with environmental parameters such as temperature, slope, rainy days, etc. Elith et al. used package gbm in R, whereas I use a Tidymodels approach in R."
  },
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html#data",
    "href": "posts/2023-03-23-eel-sdm/index.html#data",
    "title": "Boosting to Predict Eel Distribution",
    "section": "Data",
    "text": "Data\nData labeled “model.data.csv” were retrieved from the supplemental information by Elith et al. 2008 and include the following variables:\n\n\n\nFigure 1: Table 1. from Elith et al. 2008 displaying the variables included in the analysis.\n\n\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(sjPlot)\nlibrary(pROC)\nlibrary(RColorBrewer)\n\neel_data <- eel_data_raw %>%\n  select(-Site) # remove site number from data frame\neel_data$Angaus <- as.factor(eel_data$Angaus) # set outcome vaiable as a factor\n\ntab_df(eel_data[1:5,],\n       title = \"Table. 1\")\n\n\n\n\nTable. 1\n\nAngaus\nSegSumT\nSegTSeas\nSegLowFlow\nDSDist\nDSMaxSlope\nUSAvgT\nUSRainDays\nUSSlope\nUSNative\nDSDam\nMethod\nLocSed\n\n\n0\n16.00\n-0.10\n1.04\n50.20\n0.57\n0.09\n2.47\n9.80\n0.81\n0\nelectric\n4.80\n\n\n1\n18.70\n1.51\n1.00\n132.53\n1.15\n0.20\n1.15\n8.30\n0.34\n0\nelectric\n2.00\n\n\n0\n18.30\n0.37\n1.00\n107.44\n0.57\n0.49\n0.85\n0.40\n0.00\n0\nspo\n1.00\n\n\n0\n16.70\n-3.80\n1.00\n166.82\n1.72\n0.90\n0.21\n0.40\n0.22\n1\nelectric\n4.00\n\n\n1\n17.20\n0.33\n1.00\n3.95\n1.15\n-1.20\n1.98\n21.90\n0.96\n0\nelectric\n4.70\n\n\n\n\n\n\nSplit and Resample\nI split the data from above into a training and test set 70/30, stratified by outcome score. I used 10-fold CV to resample the training set, stratified by Angaus.\n\n\nCode\n# Stratified sampling with the rsample package\nset.seed(123) # Set a seed for reproducibility\nsplit <- initial_split(data = eel_data, \n                       prop = .7, \n                       strata = \"Angaus\")\n\neel_train <- training(split) # Grab training data\neel_test  <- testing(split) # Grab test data\n\n# Set up cross validation stratified on Angaus\ncv_folds <- eel_train %>% \n  vfold_cv(v=10, strata = \"Angaus\")\n\n\n\n\nPreprocess\nI created a recipe to prepare the data for the XGBoost model. I was interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis.\n\n\nCode\n# Set up a recipe\neel_rec <- recipe(Angaus ~ ., data = eel_train) %>% \n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>% \n  prep(training = eel_train, retain = TRUE)\n\n# Bake to check recipe\nbaked_eel <- bake(eel_rec, eel_train)"
  },
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html#tuning-xgboost",
    "href": "posts/2023-03-23-eel-sdm/index.html#tuning-xgboost",
    "title": "Boosting to Predict Eel Distribution",
    "section": "Tuning XGBoost",
    "text": "Tuning XGBoost\n\nTune Learning Rate\nFirst I conducted tuning on just the learn_rate parameter.\n\n\nCode\neel_spec <- parsnip::boost_tree(mode = \"classification\",\n                                engine = \"xgboost\",\n                                trees = 3000,\n                                learn_rate = tune())\n\n\nI set up a grid to tune my model by using a range of learning rate parameter values.\n\n\nCode\n# Set up tuning grid\neel_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n# Set up workflow\nwf_eel_tune <- workflow() %>% \n  add_recipe(eel_rec) %>% \n  add_model(eel_spec)\n\ndoParallel::registerDoParallel()\nset.seed(123)\n\n# Tune\neel_rs <- tune_grid(\n  wf_eel_tune,\n  Angaus~.,\n  resamples = cv_folds,\n  grid = eel_grid\n)\n\n\n\n\nCode\n# Identify best values from the tuning process\neel_rs %>%\n  tune::show_best(metric = \"accuracy\") %>%\n  tab_df(title = \"Table 2\",\n         digits = 4,\n         footnote = \"Performance of the best models and the associated estimates for the learning rate parameter values.\",\n         show.footnote = TRUE)\n\n\n\n\nTable 2\n\nlearn_rate\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n0.1656\naccuracy\nbinary\n0.8313\n10\n0.0103\nPreprocessor1_Model17\n\n\n0.1035\naccuracy\nbinary\n0.8312\n10\n0.0092\nPreprocessor1_Model11\n\n\n0.3000\naccuracy\nbinary\n0.8298\n10\n0.0129\nPreprocessor1_Model30\n\n\n0.1449\naccuracy\nbinary\n0.8298\n10\n0.0110\nPreprocessor1_Model15\n\n\n0.2380\naccuracy\nbinary\n0.8298\n10\n0.0111\nPreprocessor1_Model24\n\n\nPerformance of the best models and the associated estimates for the learning rate parameter values.\n\n\n\n\n\nCode\neel_best_learn <- eel_rs %>%\n  tune::select_best(\"accuracy\")\n\n# eel_model <- eel_spec %>% \n#   finalize_model(eel_best_learn)\n\n\n\n\nTune Tree Parameters\nI created a new specification where I set the learning rate and tune the tree parameters.\n\n\nCode\neel_spec2 <- parsnip::boost_tree(mode = \"classification\",\n                                engine = \"xgboost\",\n                                trees = 3000,\n                                learn_rate = eel_best_learn$learn_rate,\n                                min_n = tune(),\n                                tree_depth = tune(),\n                                loss_reduction = tune()\n                                )\n\n\nI set up a tuning grid using grid_max_entropy() to get a representative sampling of the parameter space.\n\n\nCode\n# Define parameters to be tuned\neel_params <- dials::parameters(\n  min_n(),\n  tree_depth(),\n  loss_reduction()\n)\n\n# Set up grid\neel_grid2 <- dials::grid_max_entropy(eel_params, size = 30)\n\n# Set up workflow\nwf_eel_tune2 <- workflow() %>% \n  add_recipe(eel_rec) %>% \n  add_model(eel_spec2)\n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n# Tune\neel_rs2 <- tune_grid(\n  wf_eel_tune2,\n  Angaus~.,\n  resamples = cv_folds,\n  grid = eel_grid2\n)\n\n\n\n\nCode\n# Identify best values from the tuning process\neel_rs2 %>%\n  tune::show_best(metric = \"accuracy\") %>%\n  tab_df(title = \"Table 3\",\n         digits = 4,\n         footnote = \"Performance of the best models and the associated estimates for the tree parameter values.\",\n         show.footnote = TRUE)\n\n\n\n\nTable 3\n\nmin_n\ntree_depth\nloss_reduction\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n18\n7\n0.1440\naccuracy\nbinary\n0.8413\n10\n0.0102\nPreprocessor1_Model15\n\n\n3\n11\n0.0000\naccuracy\nbinary\n0.8385\n10\n0.0135\nPreprocessor1_Model08\n\n\n6\n15\n0.0000\naccuracy\nbinary\n0.8342\n10\n0.0136\nPreprocessor1_Model14\n\n\n3\n8\n0.0000\naccuracy\nbinary\n0.8342\n10\n0.0128\nPreprocessor1_Model18\n\n\n4\n5\n0.2273\naccuracy\nbinary\n0.8314\n10\n0.0135\nPreprocessor1_Model12\n\n\nPerformance of the best models and the associated estimates for the tree parameter values.\n\n\n\n\n\nCode\neel_best_trees <- eel_rs2 %>%\n  tune::select_best(\"accuracy\")\n\n# eel_model2 <- eel_spec2 %>% \n#   finalize_model(eel_best_trees)\n\n\n\n\nTune Stochastic Parameters\nI created another new specification where I set the learning rate and tree parameters and tune the stochastic parameters.\n\n\nCode\neel_spec3 <- parsnip::boost_tree(mode = \"classification\",\n                                engine = \"xgboost\",\n                                trees = 3000,\n                                learn_rate = eel_best_learn$learn_rate,\n                                min_n = eel_best_trees$min_n,\n                                tree_depth = eel_best_trees$tree_depth,\n                                mtry = tune(),                   \n                                loss_reduction = eel_best_trees$loss_reduction,\n                                sample_size = tune(),\n                                stop_iter = tune()\n                                )\n\n\nI set up a tuning grid using grid_max_entropy() again.\n\n\nCode\n# Define parameters to be tuned\neel_params2 <- dials::parameters(\n  finalize(mtry(),select(baked_eel,-Angaus)),\n  sample_size = sample_prop(c(.4, .9)),\n  stop_iter())\n\n# Set up grid\neel_grid3 <- dials::grid_max_entropy(eel_params2, size = 30)\n\n# Set up workflow\nwf_eel_tune3 <- workflow() %>% \n  add_recipe(eel_rec) %>% \n  add_model(eel_spec3)\n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n# Tune\neel_rs3 <- tune_grid(\n  wf_eel_tune3,\n  Angaus~.,\n  resamples = cv_folds,\n  grid = eel_grid3\n)\n\n\n\n\nCode\n# Identify best values from the tuning process\neel_rs3 %>%\n  tune::show_best(metric = \"accuracy\") %>%\n  tab_df(title = \"Table 4\",\n         digits = 4,\n         footnote = \"Performance of the best models and the associated estimates for the stochastic parameter values.\",\n         show.footnote = TRUE)\n\n\n\n\nTable 4\n\nmtry\nsample_size\nstop_iter\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n14\n0.7264\n13\naccuracy\nbinary\n0.8171\n10\n0.0122\nPreprocessor1_Model20\n\n\n9\n0.7459\n18\naccuracy\nbinary\n0.8141\n10\n0.0125\nPreprocessor1_Model26\n\n\n16\n0.8406\n7\naccuracy\nbinary\n0.8128\n10\n0.0119\nPreprocessor1_Model17\n\n\n6\n0.7097\n4\naccuracy\nbinary\n0.8127\n10\n0.0107\nPreprocessor1_Model09\n\n\n5\n0.8537\n4\naccuracy\nbinary\n0.8113\n10\n0.0102\nPreprocessor1_Model22\n\n\nPerformance of the best models and the associated estimates for the stochastic parameter values.\n\n\n\n\n\nCode\neel_best_stoch <- eel_rs3 %>%\n  tune::select_best(\"accuracy\")\n\neel_model3 <- eel_spec3 %>% \n  finalize_model(eel_best_stoch)"
  },
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html#finalize-workflow-and-make-final-prediction",
    "href": "posts/2023-03-23-eel-sdm/index.html#finalize-workflow-and-make-final-prediction",
    "title": "Boosting to Predict Eel Distribution",
    "section": "Finalize workflow and make final prediction",
    "text": "Finalize workflow and make final prediction\nI assembled my final workflow with all of my optimized parameters and did a final fit.\n\n\nCode\neel_final_spec <- parsnip::boost_tree(mode = \"classification\",\n                                engine = \"xgboost\",\n                                trees = 3000,\n                                learn_rate = eel_best_learn$learn_rate,\n                                min_n = eel_best_trees$min_n,\n                                tree_depth = eel_best_trees$tree_depth,\n                                mtry = eel_best_stoch$mtry,                   \n                                loss_reduction = eel_best_trees$loss_reduction,\n                                stop_iter = eel_best_stoch$stop_iter,\n                                sample_size = eel_best_stoch$sample_size\n                                )\n\n# Set up workflow\nwf_eel_final <- workflow() %>% \n  add_recipe(eel_rec) %>% \n  add_model(eel_final_spec)\n\nfinal_simple_fit <- wf_eel_final %>% # fit to just training data (need for later)\n  fit(data = eel_train)\n\nfinal_eel_fit <- last_fit(eel_final_spec, Angaus~., split) # does training fit then final prediction as well\n\n# Show predictions\nfinal_pred_tab <- as.data.frame(final_eel_fit$.predictions)\ntab_df(head(final_pred_tab),\n  title = \"Table 5\",\n  digits = 3,\n  footnote = \"Predictions of Angaus presence on test data.\",\n  show.footnote = TRUE)\n\n\n\n\nTable 5\n\n.pred_0\n.pred_1\n.row\n.pred_class\nAngaus\n.config\n\n\n0.995\n0.005\n1\n0\n0\nPreprocessor1_Model1\n\n\n0.083\n0.917\n2\n1\n1\nPreprocessor1_Model1\n\n\n0.629\n0.371\n3\n0\n0\nPreprocessor1_Model1\n\n\n0.670\n0.330\n4\n0\n0\nPreprocessor1_Model1\n\n\n0.796\n0.204\n9\n0\n0\nPreprocessor1_Model1\n\n\n0.296\n0.704\n10\n1\n1\nPreprocessor1_Model1\n\n\nPredictions of Angaus presence on test data.\n\n\n\n\n\nCode\nfinal_met_tab <- final_eel_fit$.metrics # Store metrics\ntab_df(final_met_tab,\n  title = \"Table 6\",\n  digits = 3,\n  footnote = \"Accuracy and area under ther receiver operator curve of the final fit.\",\n  show.footnote = TRUE)\n\n\n\n\nTable 6\n\n.metric\n.estimator\n.estimate\n.config\n\n\naccuracy\nbinary\n0.843853820598007\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.866325136612022\nPreprocessor1_Model1\n\n\nAccuracy and area under ther receiver operator curve of the final fit.\n\n\n\n\n\nCode\n# Bind predictions and original data\neel_test_rs <- cbind(eel_test, final_eel_fit$.predictions)\neel_test_rs <- eel_test_rs[,-1] # Remove duplicate column\n\n# Compute a confusion matrix\ncm<- eel_test_rs %>% yardstick::conf_mat(truth = Angaus, estimate = .pred_class) \n\nautoplot(cm, type = \"heatmap\") +\n  theme(axis.text.x = element_text(size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title = element_text(size = 14),\n        panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\")) +\n  labs(title = \"Figure 2: Confusion matrix of predictions on test data.\")\n\n\n\n\n\nCode\ntibble <- final_eel_fit %>% collect_metrics()\n\nfinal_eel_accuracy <- tibble %>%\n  filter(.metric == \"accuracy\") %>%\n  pull(.estimate)\n\nfinal_eel_auc <- tibble %>%\n  filter(.metric == \"roc_auc\") %>%\n  pull(.estimate)\n\n\n\n\nThe model had an accuracy of 0.84. The ROC area under the curve was 0.87. The rate of false negatives was 0.11, and the rate of false positives was 0.05."
  },
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html#fit-your-model-the-evaluation-data-and-compare-performance",
    "href": "posts/2023-03-23-eel-sdm/index.html#fit-your-model-the-evaluation-data-and-compare-performance",
    "title": "Boosting to Predict Eel Distribution",
    "section": "Fit your model the evaluation data and compare performance",
    "text": "Fit your model the evaluation data and compare performance\nI then fit my final model to the evaluation data set labeled “eel.eval.data.csv.”\n\n\nCode\n# Read in eval data\neval_dat <- eval_dat_raw %>% \n  rename(Angaus = Angaus_obs) %>% # rename to match previous data\n  mutate(Angaus = as_factor(Angaus)) # make outcome a factor\n\nprediction <- final_simple_fit %>% predict(new_data = eval_dat) # generate predictions\neval_dat_pred <- cbind(eval_dat, prediction)\n\n# Compare predicted classes to actual classes\ncorrect_predictions <- sum(eval_dat_pred$.pred_class == eval_dat_pred$Angaus)\n\n# Calculate accuracy\naccuracy <- correct_predictions / nrow(eval_dat_pred)\n\n# Calculate auc\neval_dat_pred$pred_num <- as.numeric(eval_dat_pred$.pred_class)\nauc <- auc(eval_dat_pred$Angaus, eval_dat_pred$pred_num)\n\n\nHow did my model perform on this data?\n\n\nThe model had an accuracy of 0.81 on these data, which isn't quite as good as the accuracy when applying the model to the testing data. However the difference is not too extreme and seems pretty good given that the dummy classifier would be 0.744. The model had an AUC of 0.66 which does not seem great.\n\n\nHow did my results compare to those of Elith et al.?\n\n\nThe model here does not do as well as the model in Elith et al. which found a model AUC of 0.858. My AUC of 0.66is fairly far off. I would guess that Elith et al. did more tuning to find the optimal values, where as I was more limited by computing power."
  },
  {
    "objectID": "posts/2023-03-23-eel-sdm/index.html#references",
    "href": "posts/2023-03-23-eel-sdm/index.html#references",
    "title": "Boosting to Predict Eel Distribution",
    "section": "References",
    "text": "References\n[1] Elith, J., Leathwick, J.R. and Hastie, T. (2008), A working guide to boosted regression trees. Journal of Animal Ecology, 77: 802-813. https://doi.org/10.1111/j.1365-2656.2008.01390.x"
  },
  {
    "objectID": "posts/2022-12-19-climate-ai-debate/index.html",
    "href": "posts/2022-12-19-climate-ai-debate/index.html",
    "title": "Debating Nudging and AI for Climate",
    "section": "",
    "text": "This is a short podcast by myself and Lewis White where we debate using nudging and AI for climate solutions. We chose sides for each topic at random. This podcast was created for a final assignment for EDS 242 Ethics and Bias in Environmental Data Science – a course in UCSB’s Master’s of Environmental Data Science curriculum taught by Dena Montague. Intro music by Bonfire Records and moderation by Jessica French."
  },
  {
    "objectID": "posts/2022-12-19-climate-ai-debate/index.html#references",
    "href": "posts/2022-12-19-climate-ai-debate/index.html#references",
    "title": "Debating Nudging and AI for Climate",
    "section": "References",
    "text": "References\n\nAhmad, M. Usman, Afif Hanna, Ahmed-Zayn Mohamed, Alex Schlindwein, Caitlin Pley, Ingrid Bahner, Rahul Mhaskar, Gavin J. Pettigrew, and Tambi Jarmi. “A Systematic Review of Opt-out Versus Opt-in Consent on Deceased Organ Donation and Transplantation (2006-2016).” World Journal of Surgery 43, no. 12 (December 2019): 3161–71. https://doi.org/10.1007/s00268-019-05118-4.\nBartmann, Marius. “The Ethics of AI-Powered Climate Nudging—How Much AI Should We Use to Save the Planet?” Sustainability 14, no. 9 (January 2022): 5153. https://doi.org/10.3390/su14095153.\nBirdReturns. “BirdReturns.” Accessed December 7, 2022. https://birdreturns.org/. Clifford, Catherine. “More than 80% Say They’d Change Their Behavior to Fight Climate Change, but U.S. Conservatives Lag.” CNBC. Accessed December 7, 2022. https://www.cnbc.com/2021/09/14/climate-change-to-change-behavior-80percent-of-respondents-tell-pew.html.\nHe, Tianzhi, Farrokh Jazizadeh, and Laura Arpan. “AI-Powered Virtual Assistants Nudging Occupants for Energy Saving: Proactive Smart Speakers for HVAC Control.” Building Research & Information 50, no. 4 (May 19, 2022): 394–409. https://doi.org/10.1080/09613218.2021.2012119.\nMcGovern, Amy, Imme Ebert-Uphoff, David John Gagne, and Ann Bostrom. “Why We Need to Focus on Developing Ethical, Responsible, and Trustworthy Artificial Intelligence Approaches for Environmental Science.” Environmental Data Science 1 (2022): e6. https://doi.org/10.1017/eds.2022.5.\nNordgren, Anders. “Artificial Intelligence and Climate Change: Ethical Issues.” Journal of Information, Communication and Ethics in Society ahead-of-print, no. ahead-of-print (January 1, 2022). https://doi.org/10.1108/JICES-11-2021-0106.\nOpen Transcripts. “Harnessing Artificial Intelligence to Target Conservation Efforts - Carla Gomes.” Accessed December 7, 2022. http://opentranscripts.org/transcript/artificial-intelligence-to-target-conservation/.\nResources for the Future. “Nudging Behavior Toward Climate Solutions, with Elke Weber.” Accessed December 7, 2022."
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html",
    "href": "posts/2023-01-13-fpar-lai/index.html",
    "title": "Exploring FPAR and LAI",
    "section": "",
    "text": "The purpose of this blog is to explore ways to use the MCD15A3H Version 6.1 data product produced by MODIS instruments on board the National Aeronautics and Space Administration (NASA)’s Terra and Aqua satellites. MCD15A3H contains data on Fraction of Photosynthetically Active Radiation (FPAR) and Leaf Area Index (LAI), both dimensionless characteristics of plant canopy structure. FPAR refers to the fraction of incoming solar radiation (400−700 nm) that is absorbed by the green entities of a plant canopy, and LAI refers to the amount of leaf material in a plant canopy that is estimated as the one-sided green leaf area per unit ground surface area in a broadleaf canopy and as the one−half of the total needle surface area per unit ground area in a coniferous canopy.\nIn this blog, we present examples for accessing the data product using Google Earth Engine and reading FPAR and LAI data into a Jupyter notebook. We provide code to access the bands and create histogram and time series plots as well as two use case examples to compare and contrast these metrics. Others can extend this analysis to evaluate differences between LAI and FPAR in order to identify areas of low LAI and high FPAR – indicating highly efficient canopies – or high LAI and low FPAR – indicating less efficient canopies. However, these calculations are beyond the scope of this blog and not illustrated here. Instead, we focus on how FPAR and LAI, separately, have spatially changed over time in the Lacandon Jungle.\n\n\n\nThe Lacandon Jungle is Maya land and is one of the most biodiverse ecosystems in the world (Levinson 2017). This area is of interest because it is experiencing significant tropical deforestation due to slash-and-burn farming, logging, and cattle raising (Levinson 2017). From 2000 to 2012, 6 percent of the total forest area was lost, or around 500 million trees and more than 32 million tons of biomass (Soberanes 2018). Whatsmore, large climate variability (like the rate of occurrence of events of drought) has affected the jungle’s role in directly influencing the local, regional, and even global climate (O’Brien 2008)."
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html#dataset-description",
    "href": "posts/2023-01-13-fpar-lai/index.html#dataset-description",
    "title": "Exploring FPAR and LAI",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe MCD15A3H (Version 6.1) data product observes vegetation canopy structure and soil patterns via Moderate Resolution Imaging Spectroradiometer (MODIS) sensors on the Terra and Aqua satellites, and is published and mantained by NASA since July 4, 2002 (Myneni, Knyazikhin, & Park 2021). Terra’s orbit around the Earth is set so that it covers the Equator from north to south in the morning at an altitude of 698 km and Aqua covers it from south to north in the afternoon at an altitude of 705 km. Thus, this data product has a global spatial extent and generated at a 500 meter spatial resolution in a Sinusoidal projection. In addition, the MCD15A3H data product is generated at two temporal resolutions: a 8-day compositing period and a 4-day compositing period (Myneni, Knyazikhin, & Park 2021). Here, we use the a 4-day composite data product.\nThe file format of this data product is HDF-EOS (Hierarchical Data Format - Earth Observing System). This is a self-describing data format that is used for NASA EOS satellites, which include Terra, Aqua, and Aura. Beyond the HDF metadata, there is also an ECS .met file (XML format) available containing a portion of the HDF metadata.\nThe MCD15A3H data product can be retrieved via NASA Earthdata Search, USGS EarthExplorer, OPeNDAP, and Google Earth Engine. Here, we retrieve the data product via Google Earth Engine given the API’s effectiveness to access, manipulate, and visualize freely available geospatial data from several national agencies and universities without a browser.\nThe dataset has two data layers reflecting data quality at the pixel level: FparLai_QC and FparExtra_QC. FparLai_QC indicates the quality of the LAI/FPAR algorithm execution. The main situations where data quality may be impacted are: 1) if there are dense canopies, reflectances become saturated and may not properly represent changes in canopy properties or 2) the sun-sensor geometry is collected badly/is too uncertain. If either of these cause the main algorithm to not work properly, then a backup algorithm is used. The best result is the main method being used with no saturation, but if the main method is able to be used despite saturation, this data is still considered “good, very useable.” Cases where a pixel is not able to be produced using either method are also indicated. This information and more can all be found in FparLai_QC, represented as a bit-string. FparExtra_QC includes extra information that could be impacting quality, such as snow/ice presence, aerosol levels, more specific cloud aspects, and land information, also represented as a bit-string. These bit-string variables provide quality information for both FPAR and LAI measurements. Fill values are used within the FPAR and LAI data when biophysical estimates are not able to be generated by an algorithm, or these situations may also be seen represented as missing values, both of which could impact the results of data analysis. In the data for the regions we work with here, only None values are present for FPAR and LAI, which we drop from the data for our tutorial.\nUseful data quality links for further exploration:\n\nThe ArcGIS MODIS-VIIRS Python Toolbox can be used to help decode the data quality layers.\nAdditional information on issues with data by sensor, satellite, and collection version is available by NASA.\nThe user guide also contains more in-depth on the data quality information overviewed here. The data quality information above was summarized from this guide."
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html#dataset-inputoutput",
    "href": "posts/2023-01-13-fpar-lai/index.html#dataset-inputoutput",
    "title": "Exploring FPAR and LAI",
    "section": "Dataset Input/Output",
    "text": "Dataset Input/Output\nUse the code below to import all packages for analysis in this notebook and authenticate and initialize Google Earth Engine:\n\n# Import packages\nimport ee\nimport geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n\n\n# Authenticate and initialize GEE\n# ee.Authenticate()\nee.Initialize()\n\nSet geometry and reference systems parameters:\n\n# Set region of interest\nPOI_jungle = ee.Geometry.Point(-91.59522999999996, 16.75) # point for Lacandon Jungle\nscale = 10000  # scale in meters\n\n# Set coordinate reference system\ncrs_4326 = 'EPSG:4326'\n\nImport the MCD15A3H product using Google Earth Engine:\n\n# Load MCD15A3H product for FPAR and LAI data\ngdat = ee.ImageCollection('MODIS/061/MCD15A3H')"
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html#metadata-display-and-basic-visualization",
    "href": "posts/2023-01-13-fpar-lai/index.html#metadata-display-and-basic-visualization",
    "title": "Exploring FPAR and LAI",
    "section": "Metadata Display and Basic Visualization",
    "text": "Metadata Display and Basic Visualization\nBelow we explore the parameters available for this data product, time series plots for FPAR and LAI in the jungle and the desert, and histograms of FPAR and LAI in the jungle and later the desert. We chose to look at time series plot to better understand the seasonality of these metrics, and the differences between our two areas of interest. We decided to create histograms to also better understand differences between our two regions of interest and to understand the spread of our data.\nUse the code below to view metadata and metadata parameters of MCD15A3H product:\n\n# Display metadata of MCD15A3H product\nfirst = gdat.first() # pull first image\nbands = first.bandNames() # pull band names/variables\nstr(bands.getInfo()) # view metadata\n\n\"['Fpar', 'Lai', 'FparLai_QC', 'FparExtra_QC', 'FparStdDev', 'LaiStdDev']\"\n\n\nTo reiterate and consolidate what each of these represent:\nFpar - the Fraction of Photosynthetically Active Radiation (FPAR) values\nLai - the Leaf Area Index (LAI) values\nFparLai_QC - bit-strings containing key quality information, such as algorithm used and overall quality of measurement\nFparExtra_QC - bit-strings containing extra quality information that may also affect results\nFparStdDev - standard deviations for each FPAR value\nLaiStdDev - standard deviations for each LAI value\nNow, use the code below to make basic time series and histogram plots of FPAR and LAI data for the region of interest:\n\nFraction of Photosynthetically Active Radiation in Lacandon Jungle\nFirst, create data frame to use for data visualization:\n\n# Create data frame for FPAR variable in Lacandon Jungle\nfparJ = gdat.select('Fpar') # select FPAR band name/variable\nfpar_tsJ = fparJ.getRegion(POI_jungle, scale).getInfo() # extract data\ndf_fparJ = pd.DataFrame(fpar_tsJ).dropna() # save data frame\n\n# Tidy data frame\nheaders_1 = df_fparJ.loc[0] # extract headers\ndf_fparJ = pd.DataFrame(df_fparJ.values[1:], columns = headers_1) # add headers\nprint(df_fparJ) # view data frame \n\n# Convert time to datetime\ndf_fparJ['datetime'] = pd.to_datetime(df_fparJ['time'], unit = 'ms')\n\n0             id  longitude  latitude           time Fpar\n0     2002_07_04 -91.583243  16.75358  1025740800000   87\n1     2002_07_08 -91.583243  16.75358  1026086400000   54\n2     2002_07_12 -91.583243  16.75358  1026432000000   40\n3     2002_07_16 -91.583243  16.75358  1026777600000   73\n4     2002_07_20 -91.583243  16.75358  1027123200000   65\n...          ...        ...       ...            ...  ...\n1914  2023_06_02 -91.583243  16.75358  1685664000000   80\n1915  2023_06_06 -91.583243  16.75358  1686009600000   83\n1916  2023_06_10 -91.583243  16.75358  1686355200000   82\n1917  2023_06_14 -91.583243  16.75358  1686700800000   79\n1918  2023_06_18 -91.583243  16.75358  1687046400000   73\n\n[1919 rows x 5 columns]\n\n\nNow, let’s make a time series plot:\n\n# Plot time series for FPAR variable in Lacandon Jungle\nplt.figure(figsize = (10, 6), dpi = 300) # create figure; set size and resolution (dpi)\nplt.plot(df_fparJ['datetime'], df_fparJ['Fpar']) # add data to plot\nplt.title('Fraction of Photosynthetically Active Radiation in Lacandon Jungle (FPAR), 2002 to 2022', fontsize = 14) # add title to plot\nplt.xlabel('Year', fontsize = 12) # add x label to plot\nplt.ylabel('FPAR (%)', fontsize = 12) # add y label to plot\n\nText(0, 0.5, 'FPAR (%)')\n\n\n\n\n\nAnd let’s make a histogram plot:\n\n# Plot histogram for FPAR variable in Lacandon Jungle\nfig, ax = plt.subplots(figsize = (10, 6), dpi = 300) # create figure; set size and resolution (dpi)\nn, bins, patches = ax.hist(x = df_fparJ['Fpar'], bins = 'auto') # add histogram to plot\nplt.title('Fraction of Photosynthetically Active Radiation (FPAR) in Lacandon Jungle, 2002 to 2022', fontsize = 14) # add title to plot\nplt.xlabel('FPAR (%)', fontsize = 12) # add x label to plot\nplt.ylabel('Count', fontsize = 12) # add y label to plot\nax.yaxis.set_minor_locator(AutoMinorLocator()) # set automatic tick selection for y-axis\nax.xaxis.set_minor_locator(AutoMinorLocator()) # set automatic tick selection for x-axis\nax.tick_params(which = 'major', length = 7) # set major ticks\nax.tick_params(which = 'minor', length = 4) # set minor ticks\n\n\n\n\nFrom our time series and histogram we see that FPAR in the jungle is fairly high on average, but seemingly seasonal with high variablility. The data have a long left tail.\n\n\nLeaf Area Index in Lacandon Jungle\nNow, we repeat this same process for Leaf Area Index!\nFirst, create data frame to use for data visualization:\n\n# Create data frame for LAI variable in Lacandon Jungle\nlaiJ = gdat.select('Lai') # select LAI band name/variable\nlai_tsJ = laiJ.getRegion(POI_jungle, scale).getInfo() # extract data\ndf_laiJ = pd.DataFrame(lai_tsJ).dropna() # save data frame\n\n# Tidy data frame\nheaders_2 = df_laiJ.loc[0] # extract headers\ndf_laiJ = pd.DataFrame(df_laiJ.values[1:], columns = headers_2) # add headers\nprint(df_laiJ) # view data frame \n\n# Convert time to datetime\ndf_laiJ['datetime'] = pd.to_datetime(df_laiJ['time'], unit = 'ms')\n\n0             id  longitude  latitude           time Lai\n0     2002_07_04 -91.583243  16.75358  1025740800000  57\n1     2002_07_08 -91.583243  16.75358  1026086400000  28\n2     2002_07_12 -91.583243  16.75358  1026432000000  14\n3     2002_07_16 -91.583243  16.75358  1026777600000  41\n4     2002_07_20 -91.583243  16.75358  1027123200000  30\n...          ...        ...       ...            ...  ..\n1914  2023_06_02 -91.583243  16.75358  1685664000000  49\n1915  2023_06_06 -91.583243  16.75358  1686009600000  48\n1916  2023_06_10 -91.583243  16.75358  1686355200000  51\n1917  2023_06_14 -91.583243  16.75358  1686700800000  47\n1918  2023_06_18 -91.583243  16.75358  1687046400000  40\n\n[1919 rows x 5 columns]\n\n\nNext, let’s make a time series plot:\n\n# Plot time series for LAI variable in Lacandon Jungle\nplt.figure(figsize = (10, 6), dpi = 300) # create figure; set size and resolution (dpi)\nplt.plot(df_laiJ['datetime'], df_laiJ['Lai']) # add data to plot\nplt.title('Leaf Area Index in Lacandon Jungle, 2002 to 2022', fontsize = 14) # add title to plot\nplt.xlabel('Year', fontsize = 12) # add x label to plot\nplt.ylabel('Leaf Area Index (m²/m²)', fontsize = 12) # add y label to plot\n\nText(0, 0.5, 'Leaf Area Index (m²/m²)')\n\n\n\n\n\nAnd let’s make a histogram plot:\n\n# Plot histogram for LAI variable in Lacandon Jungle\nfig, ax = plt.subplots(figsize = (10, 6), dpi = 300) # create figure; set size and resolution (dpi)\nn, bins, patches = ax.hist(x = df_laiJ['Lai'], bins = 'auto') # add histogram to plot\nplt.title('Leaf Area Index in Lacandon Jungle, 2002 to 2022', fontsize = 14) # add title to plot\nplt.xlabel('Leaf Area Index (m²/m²)', fontsize = 12) # add x label to plot\nplt.ylabel('Count', fontsize = 12) # add y label to plot\nax.yaxis.set_minor_locator(AutoMinorLocator()) # set automatic tick selection for y-axis\nax.xaxis.set_minor_locator(AutoMinorLocator()) # set automatic tick selection for x-axis\nax.tick_params(which = 'major', length = 7) # set major ticks\nax.tick_params(which = 'minor', length = 4) # set minor ticks\n\n\n\n\nFrom our time series and histogram we see that LAI in the jungle reaches up to 60%, with high variablility. The data have a long left tail."
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html#use-case-examples",
    "href": "posts/2023-01-13-fpar-lai/index.html#use-case-examples",
    "title": "Exploring FPAR and LAI",
    "section": "Use Case Examples",
    "text": "Use Case Examples\n\nSummary\nLet’s now focus on the Lacandon Jungle. Here, we show use case examples for FPAR and LAI during two time periods of interest, 2010 to 2012 and 2020 to 2022. We also include time series plots to show how the FPAR and LAI change day to day over time.\nThis analysis is useful to evaluate whether jungle productivity and area over time have spatially changed. FPAR is a proxy variable for productivity, as it measures the ratio of light entering a photosynthetic system to the amount of light absorbed and reflected in that system. LAI captures changes in tree canopies over time and the potential for gas exchange and light absorption. For a more complete analysis of productivity, LAI can be evaluated alongside FPAR to understand the potential for light absorption versus the true rate of absorption. Differences between LAI and FPAR could be calculated to identify areas of low LAI and high FPAR -- indicating highly efficient canopies -- or high LAI and low FPAR -- indicating less efficient canopies. These calculations are beyond the scope of our notebook though and not illustrated here. FPAR and LAI can also be used to identify areas of deforestation through time.\nThese tools can be used by a number of stakeholders including resource managers, climate scientists, and concerned citizens. Anyone interested in understanding how vegetation and its productivity is changing over time or space might use FPAR and LAI. Outputs from mapping and plotting satellite observations may be used in conservation, resoration, climate models, resource management, or policy evaluation.\n\n\nFPAR in Lacandon Jungle\nLet’s define the time periods of interest and select the band name:\n\n# Select FPAR band name/variable\ngee1 = gdat.filter(ee.Filter.date('2010-11-01', '2012-11-01')).select('Fpar').mean() # select for time period of interest 1\ngee2 = gdat.filter(ee.Filter.date('2020-11-01', '2022-11-01')).select('Fpar').mean() # select for time period of interest 2\n\nNow, let’s create a basemap, add the layer for the mean FPAR from November 2010 to November 2012, and add the layer for the mean FPAR from November 2020 (left) to November 2022 (right):\n\n# Create basemap with spatial parameters for Lacandon Jungle\nMap = geemap.Map(center = [16.75, -91.59522999999996], zoom = 12)\n\n\n# Define palette\npalette = ['#fffff9', '#d7eba8', '#addd8e',\n          '#78c679', '#41ab5d', '#238443', '#005a32']\n\n# Define visual parameters\nvisParams = {'bands': ['Fpar'], # select band/variable\n             'min': 0, # set minimum parameter\n             'max': 100, # set maximum parameter\n             'palette': palette} # set palette\n\n# Define color bar\ncolors = visParams['palette'] # set colors from visual parameters\nvmin = visParams['min'] # set minimum from visual parameters\nvmax = visParams['max'] # set maximum from visual parameters\n\n# Add layer for time period of interest 1 to the left tile\nleft  = geemap.ee_tile_layer(gee1, visParams, 'Mean FPAR (%) in Lacandon Jungle from 2010 to 2012')\n\n# Add layer for time period of interest 2 to the right tile\nright = geemap.ee_tile_layer(gee2, visParams, 'Mean FPAR (%) in Lacandon Jungle from 2020 to 2022')\n\n# Add tiles to the map\nMap.split_map(left, right)\n\n# Add color bar\nMap.add_colorbar_branca(colors = colors, \n                        vmin = vmin, \n                        vmax = vmax)\nMap # view map\n\n\n\n\nFinally, let’s make an interactive time series map for FPAR from November 2010 (left) to November 2022 (right):\n\n\n\n\n\n\nNote\n\n\n\nNOTE: This analysis was completed as a final project for the course EDS 220 Working with Environmental Data at UCSB, and the original notebook can be found here. I’m still troubleshooting how to integrate these maps into my Quarto website. You’ll notice that the first map has a rendered layer, but the following three maps aren’t appearing as I expected. If anyone has encountered this themselves or has thoughts on a potential solution, drop me an issue here.\n\n\n\n# Create basemap with spatial parameters for Lacandon Jungle\nFPARMap = geemap.Map(center = [16.75, -91.59522999999996], zoom = 9)\n\n\n# Import collection of images from 2010 to 2022\ncollection = gdat.filter(ee.Filter.date('2010-11-01', '2022-11-01')).select('Fpar')\n\n# Set first image in collection of images from 2010 to 2022\nfirst_image = collection.first()\n\n# Add layer with first image\nFPARMap.addLayer(first_image, visParams, \"FPAR (%) in Lacandon Jungle from 2010 to 2022\")\n\n# Add all other images in collection of images from 2010 to 2022\nimage = collection.toBands()\n\n# Add layer with all other images\nFPARMap.addLayer(image, {}, \"Time series\", False)\n\n# Add labels \nlabels = collection.aggregate_array(\"system:index\").getInfo()\n\n# Add time slider\nFPARMap.add_time_slider(collection, visParams, labels = labels, time_interval = 1)\n\n# Add color legend\nFPARMap.add_colorbar_branca(colors = colors, \n                        vmin = vmin, \n                        vmax = vmax)\n\nFPARMap # view map\n\n\n\n\n\n\nLAI in Lacandon Jungle\nNow, we repeat this same process for Leaf Area Index!\nWe will be using the same time periods of interest, but need to reselect the band and recalculate the means for LAI:\n\n# Select LAI band name/variable\ngee3 = gdat.filter(ee.Filter.date('2010-11-01', '2012-11-01')).select('Lai').mean() # select for time period of interest 1\ngee4 = gdat.filter(ee.Filter.date('2020-11-01', '2022-11-01')).select('Lai').mean() # select for time period of interest 2\n\nWe will use the same palette as before, but will set new visual parameters to adjust for now working with LAI:\n\n# Define visual parameters\nvisParams2 = {'bands': ['Lai'], # select band/variable\n             'min': 0, # set minimum parameter\n             'max': 100, # set maximum parameter\n             'palette': palette} # set palette\n\nNow, let’s create a basemap, add the layer for the mean LAI from November 2010 to November 2012, and add the layer for the mean LAI from November 2020 (left) to November 2022 (right):\n\n# Create basemap with spatial parameters for Lacandon Jungle\nMap2 = geemap.Map(center = [16.75, -91.59522999999996], zoom = 12)\n\n# Define color bar\ncolors2 = visParams2['palette'] # set colors from visual parameters\nvmin2 = visParams2['min'] # set minimum from visual parameters\nvmax2 = visParams2['max'] # set maximum from visual parameters\n\n# Add layer for time period of interest 1 to the left tile\nleft2  = geemap.ee_tile_layer(gee3, visParams2, 'Mean LAI (m²/m²) in Lacandon Jungle from 2010 to 2012')\n\n# Add layer for time period of interest 2 to the right tile\nright2 = geemap.ee_tile_layer(gee4, visParams2, 'Mean LAI (m²/m²) in Lacandon Jungle from 2020 to 2022')\n\n# Add tiles to the map\nMap2.split_map(left2, right2)\n\n# Add color bar\nMap2.add_colorbar_branca(colors = colors2, \n                        vmin = vmin2, \n                        vmax = vmax2)\nMap2 # view map\n\n\n\n\nLastly, we will also make an interactive time series map for LAI from November 2010 (left) to November 2022 (right):\n\n# Create basemap with spatial parameters for Lacandon Jungle\nLAIMap = geemap.Map(center = [16.75, -91.59522999999996], zoom = 10)\n\n# Import collection of images from 2010 to 2022\ncollection2 = gdat.filter(ee.Filter.date('2010-11-01', '2022-11-01')).select('Lai')\n\n# Set first image in collection of images from 2010 to 2022\nfirst_image2 = collection2.first()\n\n# Add layer with first image\nLAIMap.addLayer(first_image2, visParams2, \"LAI (m²/m²) in Lacandon Jungle from 2010 to 2022\")\n\n# Add all other images in collection of images from 2010 to 2022\nimage2 = collection2.toBands()\n\n# Add layer with all other images\nLAIMap.addLayer(image2, {}, \"Time series\", False)\n\n# Add labels \nlabels2 = collection2.aggregate_array(\"system:index\").getInfo()\n\n# Add time slider\nLAIMap.add_time_slider(collection2, visParams2, labels = labels2, time_interval = 1)\n\n# Add color legend\nLAIMap.add_colorbar_branca(colors = colors2, \n                        vmin = vmin2, \n                        vmax = vmax2)\n\nLAIMap # view map\n\n\n\n\n\n\nDiscussion\nFPAR is a parameter for modeling ecosystem productivity, and climate and land cover changes (like deforestation) affects FPAR variation (Peng et al. 2012). Here, we see little change between the two time periods on a broad level, but when zoomed in to random pixels we see slight changes in small areas. This applies to both FPAR and LAI and both the jungle and desert.The aggregated mean FPAR plots for the periods of interest show that, at randomly-selected points, the percentage observed typically increases from 2010 - 2012 to 2020 - 2022. While grassroots and community-led afforestation efforts have occured in the last decade to protect and preserve forest area (Soberanes 2018), we cannot assign changes in FPAR variation to land cover without introducing and controling for meteorological variables like temperature and accumulated precipitation, to consider disturbances like drought.\nWhen looking at LAI averaged over our two year period of 2010-2012 vs. the two year period of 2020-2022, we do not see any major differences. This could imply that the aforementioned afforestation efforts in the last decade either had no discernable major effects or that the tail end of the aforemented deforestion from 2000 to 2012 was less intense and allowed some recovery. We could extend these theories to further analysis by taking averages over more time periods (other two year periods, or longer periods encompassing our current periods) and seeing if any major differences are visible. If data quality was impacting the results, perhaps using longer time period averages could help mitigate these effects.\nWe do not calculate the difference in LAI and FPAR, but a user could use the maps to identify areas with high LAI and low FPAR or low LAI and high FPAR. Next steps for a more complex analysis would be to run these raster algebra calculations and re-plot."
  },
  {
    "objectID": "posts/2023-01-13-fpar-lai/index.html#references",
    "href": "posts/2023-01-13-fpar-lai/index.html#references",
    "title": "Exploring FPAR and LAI",
    "section": "References",
    "text": "References\n\nMyneni, R., Knyazikhin, Y., Park, T. (2021). MODIS/Terra+Aqua Leaf Area Index/FPAR 8-Day L4 Global 500m SIN Grid V061 [Data set]. NASA EOSDIS Land Processes DAAC. Accessed 2022-11-14 from https://doi.org/10.5067/MODIS/MCD15A2H.061\nO’Brien, Karen L. 1995. “Deforestation and Climate Change in the Selva Lacandona of Chiapas, Mexico: Some Preliminary Results.” Norsk Geografisk Tidsskrift - Norwegian Journal of Geography 49 (3): 105–22. https://doi.org/10.1080/00291959508543416.\nHoffner, Erik. 2018. “Illegal Cattle Ranching Deforests Mexico’s Massive Lacandon Jungle.” Mongabay Environmental News. March 14, 2018. https://news.mongabay.com/2018/03/illegal-cattle-ranching-deforests-mexicos-massive-lacandon-jungle/.\nLevinson, Jonathan. 2017. “Communities in Mexico Step up to Protect a Disappearing Forest.” Mongabay Environmental News. March 16, 2017. https://news.mongabay.com/2017/03/communities-in-mexico-step-up-to-protect-a-disappearing-forest/.\nPeng, Dailiang, Bing Zhang, Liangyun Liu, Hongliang Fang, Dongmei Chen, Yong Hu, and Lingling Liu. 2012. “Characteristics and Drivers of Global NDVI‐Based FPAR from 1982 to 2006.” Global Biogeochemical Cycles 26 (3). https://doi.org/10.1029/2011gb004060."
  },
  {
    "objectID": "posts/2023-06-25-flourish-ocean-depth/index.html",
    "href": "posts/2023-06-25-flourish-ocean-depth/index.html",
    "title": "Learning Flourish to Visualize Ocean Depth Data",
    "section": "",
    "text": "Description:\nIn this post, I share how I used Flourish – a data visualization platform – to visualize ocean depth data in the Santa Barbara Channel. I used ocean depth data from the kelpGeoMod project data repository.\n\nSomething New\nI recently came upon a tweet about Flourish which is an online data visualization and storytelling platform. This was the first I’d heard of the platform, and I wanted to try it out as an alternative for other popular proprietary software such as Tableau. I pulled ocean depth data from my master’s capstone project, Developing a Data Pipeline for Kelp Forest Modeling (also known as kelpGeoMod), and started to explore. Here I will share how I created the visualization below.\n\n\n\nGetting the Data\nTo get data to visualize, I navigated back to my masters capstone project data repository and downloaded the ocean depth data in the Santa Barbara Channel. The original data source was the ETOPO Global Relief Model 2022.\n\n\n\n\n\n\n\nGetting Started in Flourish\nI then created a free account with Flourish. With the free version of my account, I have access to a limited set of features, while the full version offers additional functionalities. I then started exploring available features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlourish had numerous templates for visualizing data. I noticed, though, that it is not currently compatability with raster data, so I had to convert my raster data to vector data. This is not ideal, but it was quickly doable with this relatively small raster file.\n\n\nCode\n# Load necessary libraries\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\n\n# Set a data directory\n#dir <- \"./data/\"\n\n# Load depth data\ndepth <- rast(file.path(dir, \"depth.tif\"))\n\n# Vectorize\ndepth_vect <- as.polygons(depth[[1]], \n                          dissolve = FALSE)\n\n# Convert to sf object\ndepth_sf <- st_as_sf(depth_vect) %>% \n  rename(\"depth\" = \"exportImage\") %>% # rename column name\n  filter(depth <= 0) %>% # filter for depths less than zero\n  mutate(depth = round(depth, 3)) # round depth values\n\n# Write to GeoJSON file\n# st_write(obj = depth_sf, \n#          file.path(dir, \"depth.geojson\", \n#          driver = \"GeoJSON\"))\n\n\nI decided to choose the UK hex map template because I wanted to use try out the three-dimensional extrusion feature.\n\n\n\n\n\n\n\nMaking the Map\nOnce you get started, to make the map you’ll have to replace the template data with your own. Name your project, navigate to the data tab, then upload your own GeoJSON data.\n\n\n\n\n\nBe sure to select which columns go with which type of data. Flourish has helpful documentation hints denoted with question mark circles.\n\n\n\n\n\nWhen you’re ready, swap back to the preview mode and start messing with your map’s aesthetics.\n\n\n\n\n\nI chose to edit aesthetics such as the projection, background color, padding, palette, legend, the popup shape, title, and footer. I also added a screenreader description. You can adjust these settings however you would like.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow you should have an export-ready three-dimensional image of ocean depth in the Santa Barbara Channel! Export and enjoy Flourishing!\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{windschitl2023,\n  author = {Elke Windschitl},\n  title = {Learning {Flourish} to {Visualize} {Ocean} {Depth} {Data}},\n  date = {2023-06-25},\n  url = {https://elkewind.github.io/posts/2023-06-25-flourish-ocean-depth},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nElke Windschitl. 2023. “Learning Flourish to Visualize Ocean Depth\nData.” June 25, 2023. https://elkewind.github.io/posts/2023-06-25-flourish-ocean-depth."
  },
  {
    "objectID": "posts/2019-12-01-bobwhite-aru-research/index.html",
    "href": "posts/2019-12-01-bobwhite-aru-research/index.html",
    "title": "Evaluating the Use of Autonomous Recording Units for Monitoring Northern Bobwhite Coveys",
    "section": "",
    "text": "This is a 2019 research poster on evaluating the use of autonomous recording units (ARUs) for monitoring Northern bobwhite coveys. This research was done in the Janke lab in collaboration with the Iowa DNR with the goal of furthering the National Bobwhite Conservation Initiative. I presented this research at the Iowa State University College of Agriculture and Life Sciences ‘Science with Practice’ Poster Presentation (April 2019), the Iowa State University Honors Poster Presentation (December 2019), and the National Conference on Undergraduate Research (April 2021).\n\n\n\n\n\n\n\n\nProcessing Audio in Raven Pro\n\n\n\n\n\nCitationBibTeX citation:@online{windschitl,adamjanke,kylayuza-pate2019,\n  author = {Elke Windschitl, Adam Janke, Kyla Yuza-Pate},\n  title = {Evaluating the {Use} of {Autonomous} {Recording} {Units} for\n    {Monitoring} {Northern} {Bobwhite} {Coveys}},\n  date = {2019-12-01},\n  url = {https://elkewind.github.io/posts/2019-12-01-bobwhite-aru-research},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nElke Windschitl, Adam Janke, Kyla Yuza-Pate. 2019. “Evaluating the\nUse of Autonomous Recording Units for Monitoring Northern Bobwhite\nCoveys.” December 1, 2019. https://elkewind.github.io/posts/2019-12-01-bobwhite-aru-research."
  },
  {
    "objectID": "posts/2023-03-29-art-and-science/index.html",
    "href": "posts/2023-03-29-art-and-science/index.html",
    "title": "Integrating Art into Science and Conservation",
    "section": "",
    "text": "Here I discuss how I integrate art into environmental science and conservation. I like to use creative and quantitative talents in parallel, and the balance between the two has ebbed and flowed over time."
  },
  {
    "objectID": "posts/2023-03-29-art-and-science/index.html#art-and-science",
    "href": "posts/2023-03-29-art-and-science/index.html#art-and-science",
    "title": "Integrating Art into Science and Conservation",
    "section": "Art and Science",
    "text": "Art and Science\nAt the time of writing this, I am a master’s student studying environmental data science at UC Santa Barbara (read my full bio here), but I am also a self-taught digital artist. Science and art have both played integral roles in my life over the past 6 years, and recently I have been exploring ways to integrate the two. In the past, these two realms of interest have been separate, but I have found that my art can supplement environmental science or vice versa to elevate communication and engagement.\nI began experimenting in digital illustration and design in 2017, and learned how to use Adobe Photoshop and Inkscape. Over time, I have progressed to primarily use Procreate and Canva. Additionally, I am an amateur photographer and love to photograph the natural world using a Canon t6i and Adobe Lightroom software.\nInspired by other data science/environmental science artists (such as Allison Horst – check her out!), I have worked to continue creating art alongside developing my quantitative skills. My favorite way to wrap artwork into science is by creating relevant ecological illustrations and adding them into presentations or other resources. Below are some examples of my artwork in relation to environmental science.\n\nHex Stickers:\nI am currently working on a collaborative capstone project modeling habitat suitability for kelp cultivation in the Santa Barbara Channel. I designed a hex sticker for our capstone group. (Unfamiliar with hex stickers? Check out these data science hex stickers created by others.)\n\n\n\n\nEnvironmental Non-Profit Social Media Campaign:\nIn 2022, I assisted with a social media campaign by The LENA Project to provide monthly information and suggestions on environmental related topics. Here are two of the designs I created for this campaign.\n\n\n\n\nEnvironmental Non-Profit Creative Project:\nIn 2021, I participated in (as well as volunteered for) Prompt for the Planet *Chapter 2 – an initiative to create art and write poetry to raise awareness about our changing planet.\n\n\n\nEnvironmental Non-Profit Fundraising:\nIn 2021 I designed a yard sign for The LENA Project that they sold as a fundraiser and as a campaign to promote unfertilized lawns. In 2023 I updated the design for the continuation of the campaign.\n\n\n\n\nJournalism:\nIn 2020, my art was sourced by The Lutrinae in an article about the Monterey Bay Aquarium reopening after shutting down during the pandemic.\n\n\n\nPhotography:\nI use photography as a way to document traveling and share my love for ecology and the natural world.\n\n\n\n\n\n\n\nBlogging:\nAll of my blog posts feature my own illustrations.\n\n\n\nAdditional Illustrations:\nSmaller illustrations have made their way into presentations, twitter posts, stickers, and more."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html",
    "href": "posts/2023-07-02-kelpGeoMod/index.html",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "",
    "text": "Below is the technical documentation for the Bren Master of Environmental Data Science capstone project Developing a Data Pipeline for Kelp Forest Modeling completed in 2023 by myself, Jessica French, Erika Egg, and Javier Patrón."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#abstract",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#abstract",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Abstract",
    "text": "Abstract\nGiant kelp (Macrocystis pyrifera) is an ecosystem engineer that creates complex vertical habitat by growing to approximately 50 m in dense forests. Healthy kelp forests are some of the most diverse ecosystems in the world that also protect coastlines from storms, provide nutrients to beaches, and giant kelp is a promising biofuel precursor that does not take up arable land or use freshwater to grow. Researchers are working to better understand nutrient utilization and cycling in this critical ecosystem and need comprehensive data on nutrient concentrations to further their research. Additionally, kelp aquaculture companies are working to show that giant kelp can be grown as a profitable biofuel precursor in the Santa Barbara Channel. In order to do this they need to grow kelp efficiently in areas that have suitable habitat. This project creates a synthesized data set that can be used and expanded on by researchers to make their data acquisition process more efficient. It also produces estimates of habitat suitability for giant kelp in the Santa Barbara Channel that kelp aquaculture organizations can use to supplement prior analyses and guide where to place future farms."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#executive-summary",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#executive-summary",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Executive Summary",
    "text": "Executive Summary\nGiant kelp (Macrocystis pyrifera) is a foundational species of canopy-forming kelp in the Santa Barbara Channel that provides the structure for some of the most diverse ecosystems in the world (Buschmann et al., 2007). Its tall stipes provide habitat for many species and protect coastlines from storms (Buschmann et al., 2007; Esgro & Ray, 2021). Declines in giant kelp abundance over the past decade have put these ecosystem services at risk and increased the urgency for research (Rogers-Bennett & Catton, 2019; Wernberg et al., 2013). It is also an attractive option for biofuel production because it can grow up to one meter per day and requires no fresh water or arable land (Cuba et al., 2022; Kerrison et al., 2015). \nThis project addressed the needs of two clients. The first was Ph.D student Natalie Dornan, who is researching nutrient utilization and cycling in giant kelp forests in the Santa Barbara Channel. Her goal is to create a nitrogen budget for the area to better understand the sources and sinks for nitrogen in the Santa Barbara Channel. To further her research she needs comprehensive data on nutrient concentrations in the Santa Barbara Channel. The second client was Ocean Rainforest, who is cultivating giant kelp in the Santa Barbara Channel to be used in biofuel production. They are working to prove that giant kelp can be a profitable biofuel precursor which means they need to be able to grow kelp efficiently. To do this they need to know where habitat is suitable for kelp. \nThere have been several long term monitoring efforts in the Santa Barbara Channel that have generated a lot of data on nutrient concentrations. This data is spread over many agency, organization, and research project websites, APIs, and data portals with each storing data in different file formats and providing access to the data in a slightly different way. In addition to access and file formats being inconsistent, observations are often at different spatial and temporal resolutions and in different data structures. For example one organization may provide point data collected quarterly in a text file while another provides raster images collected daily in a netCDF file. Putting this data into a common format so that all of the observations can be used together is tedious and time consuming. \nThis project addressed this problem by creating a synthesized data set of oceanographic factors that impact giant kelp growth designed to streamline research. Publicly available data on nutrient concentrations, sea surface temperature (SST), depth, seafloor habitat, and kelp coverage were obtained from the Santa Barbara Coastal Long Term Ecological Research (SBC LTER), National Oceanic and Atmospheric Administration (NOAA), United States Geological Survey (USGS), California Cooperative Oceanic Fisheries Investigations (CalCOFI), the California State Mapping Project, Earth Research Institute (ERI), National Aeronautics and Space Administration (NASA), and The Group for High Resolution Sea Surface Temperature (GHRSST) and compiled into one data set (Bell, Cavanaugh, Reuman, et al., 2021; Bell, Cavanaugh, & Siegel, 2023; CalCOFI Bottle Database, n.d.; Nearshore Benthic Habitat GIS for the Channel Islands Volume II - Mapped Areas, n.d.; Seafloor Mapping Lab at CSUMB: Data Library Southern California Data (Part II), n.d.; ERI, n.d.; Golden, 2013; JPL MUR MEaSUREs Project, 2015; NOAA National Centers for Environmental Information, 2022; Prouty & Baker, 2020a, 2020b; Washburn et al., 2022). All original data sets were vetted to ensure they had sufficient metadata, spatial coverage, and temporal coverage before being included in the project. Data sets were standardized with respect to resolution (spatial and temporal), units, extent, and coordinate reference system. \nThe standardized nutrient data, kelp area, kelp biomass (kelp biomass is derived from kelp area and does not represent separate observations), sea surface temperature, and depth were shared in both tabular (CSV) and image (raster) format. The first CSV contained the mean observed nutrient values and sea surface temperature values for each year and quarter with estimates of kelp area, kelp biomass, and depth added on. The raster format contained year and quarter mean raster bricks for each variable except depth and substrate which were considered constant. These raster bricks were used to create another CSV file that provided the same information in a more accessible format. By standardizing and combining the data it could be used together to address the need of Ocean Rainforest to know where habitat is most suitable for giant kelp in the Santa Barbara Channel. \nOcean Rainforest completed a habitat suitability analysis in 2018 for offshore locations (further than 3 nm from shore). As they move forward with placing additional kelp aquaculture farms in the Santa Barbara Channel they need an assessment of nearshore habitat suitability for giant kelp. To address this need, this project will produce updated estimates of habitat suitability for giant kelp with data from 2014 to 2022 and covering areas within 5 km of the Santa Barbara Coastline. \nTo model habitat suitability observations of phosphate and combined nitrate and nitrite for each year and quarter were interpolated using inverse distance weighting to generate a quarterly mean across all years for each nutrient. These estimates and depth were used to estimate habitat suitability for giant kelp in the Santa Barbara Channel using a maximum entropy species distribution modeling approach. \nThis resulted in quarterly estimates of habitat suitability for giant kelp on a scale of 0 to 1 that were filtered to locations that met the substrate needs of Ocean Rainforest. Additionally, the relative contribution of each variable to the estimates of habitat suitability (variable importance) was generated to guide future iterations of the model. \nThe synthesized data set created through this project will streamline research on nutrient cycling and utilization in kelp forests in the Santa Barbara Channel by making the data collected through various long term monitoring efforts available in one place. The model outputs will provide an updated estimate of habitat suitability for kelp in an area not covered by previous models. The variable importance will allow industry professionals and researchers to see what is most impacting giant kelp growth in the Santa Barbara Channel and help guide future research. This project was packaged into a well documented github repository with an accompanied google drive data hub to create a seamless pipeline for researchers and industry professionals to use in the future."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#problem-statement",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#problem-statement",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Problem Statement",
    "text": "Problem Statement\nGiant kelp (Macrocystis pyrifera) forests provide a wealth of ecosystem services such as protecting coastlines from storms, bringing nutrients to beaches, and creating habitat that increases biodiversity in the nearshore environment (Buschmann et al., 2007; Cuba et al., 2022; Esgro & Ray, 2021). Giant kelp has evolved to thrive in highly variable environments; it can withstand 15° C changes in sea surface temperatures, periods of limited nutrient availability, and even withstand  severe storms adapting and recovering quickly (Cavanaugh et al., 2019). However, increasing frequency and severity of marine heat waves, El Niño events, and other environmental disturbances are pushing kelp to the limits of what it can withstand (Esgro & Ray, 2021; Rogers-Bennett & Catton, 2019; Wernberg et al., 2013). This became evident when a record breaking marine heat wave between 2014 and 2016 added to a severe El Niño, combined with a plague of sea star wasting disease that allowed kelp’s main predators, sea urchins, to flourish (Cavanaugh et al., 2019; Rogers-Bennett & Catton, 2019). This resulted in the decimation of kelp forests along the California coast with areas losing up to 90% of their kelp canopy (Cavanaugh et al., 2019). \nThis has motivated researchers and the kelp aquaculture industry to gain a better understanding of kelp ecology in a changing climate and to identify locations that could support kelp restoration projects or kelp farms. The clients for this project are seeking to do just that. Ph.D student Natalie Dornan is studying nutrient cycling and utilization in kelp forests with the goal of developing a spatiotemporal model of nitrogen in the Santa Barbara Channel. Additionally, the pioneering blue growth company Ocean Rainforest is working to make giant kelp a profitable biofuel precursor that can be grown in the Santa Barbara Channel. In order to accomplish their goals, they need data on oceanographic factors that impact kelp growth in the Santa Barbara Channel and estimates of locations that have suitable habitat for giant kelp. \nThere are many factors that impact habitat suitability for giant kelp. Among these are nutrient concentrations for nitrate, nitrite, ammonium, and phosphorus, sea surface temperature, and substrate type (Brzezinksi et al., 2013; Buschmann et al., 2007; Cavanaugh et al., 2019; Peters et al., 2019). While the nutrient and temperature requirements of naturally occurring kelp and cultivated kelp are the same, the substrate needs are not. Kelp farms, such as those operated by Ocean Rainforest, need soft substrate to place their infrastructure on so that it does not disturb natural kelp habitat or protected rocky reef habitat.  Natural kelp typically attaches to and grows from rocky substrate. This creates a challenge in assessing where giant kelp habitat is suitable based solely on where it naturally occurs. \nData on oceanographic factors such as nitrogen and phosphorus concentrations, sea surface temperature, seafloor substrate, and ocean depth are available through several long term monitoring efforts in the Santa Barbara Channel. The challenge is that different data sources provide data in different formats, at different spatial and temporal resolutions, and stored and accessed in slightly different ways. Synthesizing the available data so that all of the observations can be used simultaneously will create the most complete picture possible of conditions in the Santa Barbara Channel and identify data gaps that can be addressed by future research. \nThe synthesized data set of oceanographic factors will then be used to meet the second challenge of identifying suitable habitat for giant kelp. Current models of habitat suitability that Ocean Rainforest uses were completed in 2018 and did not cover the nearshore environment. The model created through this project will provide predictions with data collected through 2022 and estimates of kelp habitat suitability within 5 km of shore. This updated model, when combined with maps of seafloor substrate, will provide needed information to the kelp aquaculture industry and researchers when exploring where to locate future projects."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#specific-objectives",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#specific-objectives",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Specific Objectives",
    "text": "Specific Objectives\n\nSynthesize currently available data on kelp forest distribution and oceanographic factors in the Santa Barbara Channel into one standardized data set that can be easily used, reused, and updated by researchers and kelp farm industry professionals. After downloading the data set researchers can easily incorporate additional variables and more current data to meet their research needs. \nCreate a model of giant kelp habitat suitability in the Santa Barbara Channel that will provide an update to analyses completed in 2018, in an area not covered previously, and will account for the differing substrate needs of naturally occurring and cultivated kelp."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-solution-design",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-solution-design",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Summary of Solution Design",
    "text": "Summary of Solution Design\n\nObjective 1: Synthesized Data Set\nData on kelp area, kelp biomass (derived from kelp area), depth, sea surface temperature (hereafter referred to as SST), nutrient concentrations, substrate, and regulatory boundaries were downloaded from open source data repositories and research projects. A summary of the source, original format, and final format is provided in Table S1. \nData that were collected over multiple years were filtered to observations over the 2014-2022 period. This time period was chosen because of the sharp declines in kelp coverage between 2014 and 2016 as well as 2022 being the most recent full year of data. All data were filtered spatially to observations made within the Santa Barbara Channel (Coordinates used to delimit the Santa Barbara Channel 33.85°- 34.59°N, 118.80°- 120.65°W) (Fig. 1). Only nutrient observations made in the top 10 m of water were included. Where multiple observations were made at different depths in the same location the mean of all points in the top 10 m of water were included. All data that did not have WGS 84 as the coordinate reference system were reprojected to WGS 84. Observations for variables that were spread across multiple data sets and/or multiple files were combined into one file. Multiple files from the same data set were combined into one file. This resulted in one file per data set for all variables that would be used in creating the synthesized data set in subsequent steps. \n\n\n\nFigure 1: Map of the Santa Barbara Channel. The blue box outlines the following coordinates 33.85°- 34.59°N, 118.80°- 120.65°W\n\n\nNutrient observations taken on different temporal scales were aggregated to year and quarter (Quarter 1 = Jan - Mar, Quarter 2 = Apr - Jun, Quarter 3 = Jul - Sep, Quarter 4 = Oct - Dec). Where the same point was sampled more than once in one quarter the mean of the values was reported for that point in that year and quarter. All nutrient data set files were then combined by stacking the rows of each data set together for CSV export. Additionally nutrient observations were converted from point format to raster format at a resolution of 0.008° by assigning the value at that point to the grid cell it intersects for continuity with other data sets. \nData on kelp area and kelp biomass were extracted from a netCDF file and assembled into a data frame. This data frame was then converted into two rasterStacks. Values within these rasterStacks were aggregated by sum from the original resolution of 30 m to roughly the desired resolution and then resampled to the exact resolution of 0.008° (approximately 1 km). The original temporal resolution was year and quarter and was not changed. Depth estimates as of 2022 were resampled from 15 arcsecond resolution to 0.008°. SST data were aggregated from daily estimates to year and quarter by taking the mean of all daily estimates that were within the quarter for each grid cell and resampled to 0.008° resolution using the nearest neighbor method. The combined nutrient point observations were intersected with the kelp area, kelp biomass, depth, and SST rasters to estimate values for these variables at each point and exported in CSV format. \nPoint observations of nutrients were interpolated using inverse distance weighting with an inverse distance power of 1. Measurements of nitrate and nitrite were combined by summing to create total nitrogen measurement. Points were aggregated over all years by quarter to maximize the number of observations available to estimate the nutrient concentration. This assumes that nutrient concentrations in the same location in different years would be roughly the same, and is a notable limitation of this method. For phosphate and nitrogen the maximum distance a cell could be from a data point and still have a value estimated (hereafter referred to as the maximum distance parameter) was set to 0.008° (approx. 10 km) (Brzezinksi et al., 2013; Peters et al., 2019). The maximum distance parameter for ammonium was set to 0.04° because it is more spatially variable than phosphate and nitrogen (Brzezinksi et al., 2013; Peters et al., 2019). Values were estimated for cells within 5 km (Fig. 2) of the Santa Barbara coastline at a resolution of 1 km, however the estimate could be based on point values that were further away, up to the maximum distance parameter set for the nutrient being estimated. \n\n\n\nFigure 2: Map of Interpolation Area. Interpolation was done within 5 km of the Santa Barbara Coast. The interpolated area is shown in green.\n\n\nYear and quarter raster data for nutrients, SST, kelp area, and kelp biomass were converted to tabular format by assigning the value of each grid cell to a latitude and longitude within that grid cell. Estimates of depth as of 2022 were converted to tabular format in the same way and joined to the nutrient, SST, kelp area, and kelp biomass data. This was exported as a single file in CSV format. \nSubstrate data stored in a shapefile were reclassified to rock substrate, soft substrate, mixed substrate, and anthropogenic substrate based on the reported induration or description of each polygon. The reclassified data were then converted to raster at a resolution of 0.00003° (approximately 3.3 m). Substrate data stored as Esri layer files were loaded into QGIS and saved as GeoTIFF files. These rasters (riginal resolutions of 2 m, 3 m, or 5 m) were resampled using the nearest neighbor method to 0.00003° resolution. The resulting files were combined and exported in GeoTIFF format. \nThe final synthesized data set was provided in CSV and GeoTIFF rasterStack formats. The CSV format contained one file with the observed nutrient concentrations and SST aggregated to year and quarter, with estimates of kelp area, kelp biomass, and depth added for each point as described above. The GeoTIFF format contains a series of raster bricks where each brick contains the measurements for one variable and each layer in a stack represents a year and quarter. The variables contained in the series are kelp area, kelp biomass, nutrients, and SST. This series of raster stacks was combined in CSV format where each row represents the value of a cell at a year and quarter and each column represents a variable. The combined substrate file is provided separately because the observations are categorical with discrete boundaries and could not be resampled to 0.008° resolution and maintain accuracy of all substrate categories (The soft substrate category covers much larger continuous areas compared to hard, mixed, and anthropogenic and was converted to  0.008° resolution for analysis).\nAn additional sandy-bottom substrate raster was created at the 0.008° resolution level for later analyses. This was done by first reclassifying to sandy (1) and non-sandy (0) substrates and aggregating to a near 0.008° resolution by mean. The raster was resampled to the mask to get a perfect resolution of 0.008° by the nearest neighbor method. Then the raster was reclassified so cells with values less than 1 due to containing non-sandy cells were assigned 0, and cells with values of 1 remained at 1 – the sandy-bottom substrate.\n\n\nObjective 2: Habitat Suitability Model \nOnce synthesized, this data set was used to model kelp habitat suitability in the Santa Barbara Channel via a maximum entropy species distribution model called Maxent (citation 12) – a pre-developed machine learning algorithm (Kass et al., 2023). This modeling approach was chosen because it allows the user to generate a predicted habitat suitability for a species based on continuous environmental variables, such as the GeoTIFF files created in the synthesized data set (Kass et al., 2023; Melo-Merino et al., 2020; Phillips et al., 2017; Watt, 2018). Although the input kelp area was remotely sensed, it did not fully cover the area of interest, and growing kelp that hadn’t reached the surface would not be detected (Cavanaugh et al., 2021). Therefore, it was assumed that the measurements of kelp area represented definitive presence but not definitive absence, and thus was appropriate for a presence-only model like Maxent (Kass et al., 2023; Watt, 2018).\nMaxent outputs a probability distribution heatmap of predicted habitat suitability for the species of interest (Elith et al., 2011; Kass et al., 2023; Melo-Merino et al., 2020). Habitat suitability for giant kelp was predicted for each quarter independently to account for drastic seasonal changes in ocean nutrient distribution (Brzezinksi et al., 2013; Buschmann et al., 2007; Peters et al., 2019). Predictions were generated for a 1 km resolution grid that extended 5 km from the Santa Barbara coastline. \nTo prepare the kelp occurrence data for the model, the kelp area was averaged over the time period of interest (2014-2022) by quarter. Then it was converted from a continuous raster to a data frame where the kelp area of each cell was assigned to a row and a point within the cell was assigned as the latitude and longitude (Watt, 2018). A value greater than zero was treated as an observation of presence. Additionally, the interpolated nitrogen, interpolated phosphate, and depth were combined in folders such that each tif file represented a variable and each folder represented a quarter. \nThe interactive web application Wallace was used to perform the initial runs for Maxent modeling and model selection (Kass et al., 2023). The application uses the maxent and ENMeval packages available in R to run different versions of the model and calculate evaluation metrics. Wallace also makes all code used in the modeling process available to download so that the process is fully reproducible. This reproducible code was saved and updated to be contained in the kelpGeoMod pipeline (Kass et al., 2023; Melo-Merino et al., 2020). \nIn order to find the model with the best performance, a k = 4 checkerboard spatial partition was used to first train then test the data. Linear, Quadratic, Hinge, and Product feature classes were allowed to be applied to the data and regularization multipliers between 0.5 and 4.5 at a step value of 0.2 were used. Clamping was not employed, so the model was not constrained to environmental values seen in the training data.\nThe model with the best predictive performance for all quarters based on minimizing the AIC  had a regularization parameter of 0.5 and allowed linear and quadratic feature classes to be applied to the data. The raw output of maxent modeling is a ratio of the probability density of covariates across the landscape of interest with kelp occurrence over the probability density of covariates across the whole landscape of interest (Elith et al., 2011).In this form the Maxent output is challenging to interpret intuitively so a cloglog transformation was applied to the model raw output so that it could be interpreted as predicted habitat suitability where each grid cell had a value between 0 and 1 (Elith et al., 2011; Kass et al., 2023). This type of transformation is recommended for interpretability in the Maxent and Wallace documentation (Kass et al., 2023; Phillips et al., 2017).\nThe outputs were exported as GeoTIFF files and used to create heat maps that combine predicted habitat suitability,  and substrate type in the Santa Barbara Channel. This will allow Ocean Rainforest and researchers to see areas where in the Santa Barbara Channel habitat is suitable for kelp where it does not occur naturally. Combining this with maps of seafloor substrate will allow Ocean Rainforest to identify potential areas for kelp farm placement that will not disturb existing kelp habitat and have the soft seafloor substrate that is required for farm infrastructure. Similarly it will allow researchers to identify areas of potential rocky reef habitat where kelp restoration projects are most likely to be successful. The maps created were exported as GeoTIFF files. \nAdditionally, metrics of feature importance were pulled from the model outputs. This information will be useful to Ocean Rainforest as they consider what variables are most important to have in a given location when siting kelp aquaculture farms."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#products-and-deliverables",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#products-and-deliverables",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Products and Deliverables",
    "text": "Products and Deliverables\n\nSynthesized Data Set\nThe synthesized data set was provided in two formats to maximize flexibility of use and to align with the various formats of the original data. The first is a CSV file that contains all of the observed nutrient values from the original data sets and observed SST with estimates of  kelp area, kelp biomass, and depth from the raster cell that the observation intersects aggregated to year and quarter. This data set brings together observations of nutrient concentrations from CalCOFI, ERI, SBC LTER, and USGS in a single file (Fig. 3) (CalCOFI Bottle Database, n.d.), ERI (ERI, n.d.), SBC LTER (Bell, Cavanaugh, Reuman, et al., 2021; Washburn et al., 2022). Adding estimates of kelp area,  kelp biomass, and depth will make it easier to investigate relationships between nutrient concentrations and kelp forest cover. This data set is available for viewing or to download at this link. \n\n\n\nFigure 3: Location of points in observed nutrient data set. Map depicting the locations that nutrient measurements were taken, points are colored by the organization or research project that collected the data.\n\n\nThe second format of the synthesized data set was a series of GeoTIFF raster stacks at 0.008° resolution. The data for each variable; kelp area (Fig. 4), kelp biomass, SST (Fig. 5), nitrogen (Fig. 6), ammonium, and phosphate, were represented by a series of raster bricks where each layer contained the estimates for each year and quarter.\n\n\n\nFigure 4: Kelp area raster brick. Diagram of kelp area raster brick, darker green indicates higher kelp area.\n\n\n\n\n\nFigure 5: SST raster brick. Diagram of SST raster brick. Temperatures range from 13 °C to 19° C with highest temperatures in red and lower temperatures in blue.\n\n\n\n\n\nFigure 6: Nitrogen raster brick. Diagram of nitrogen raster brick. Observations are depicted in red with darker colors showing higher nitrogen concentrations.\n\n\nIn addition to the series of raster bricks, estimates of depth (Fig. 7)  and substrate (Fig. 8) observations were provided as GeoTIFF raster layers. Depth was provided at a resolution of 0.008° and substrate at a resolution of 0.00003°.\n\n\n\nFigure 7: Depth layer. Depth layer as of 2022.\n\n\n\n\n\nFigure 8: Combined substrate layer. Substrate layer with different substrate classifications (soft, hard, mixed, and anthropogenic) shown in colors according to the attached legend.\n\n\n\n\nModel Outputs\nIn order to model habitat suitability for giant kelp within 5 km of the Santa Barbara Coastline, nutrient observations were interpolated as described above resulting in quarterly mean raster layers of nitrogen (Fig. 9), phosphate (Fig. 10), and ammonium (Fig. 11). A comparison of the root mean square error (RMSE) for each of these layers was estimated and is provided in tables (Table 1, Table 2, and Table 3). \n\n\n\nFigure 9: Quarterly interpolation of nitrogen. Result of nitrogen interpolation for each quarter that was used as inputs to maxent.\n\n\n\n\n\nTable 1: Performance of quarterly nitrogen interpolation. Performance of quarterly nitrogen interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\n\n\n\nFigure 10: Quarterly interpolation of phosphate. Result of phosphate interpolation for each quarter that was used as inputs to maxent.\n\n\n\n\n\nTable 2: Performance of quarterly phosphate interpolation. Performance of quarterly phosphate interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\n\n\n\nFigure 11: Quarterly interpolation of ammonium. Result of ammonium interpolation. These layers were not used in modeling.\n\n\n\n\n\nTable 3: Performance of quarterly phosphate interpolation. Performance of quarterly ammonium interpolation relative to the RMSE of the underlying data. A negative value indicates the RMSE of the interpolation was higher than the underlying data.\n\n\nEstimates of habitat suitability for giant kelp were provided for each quarter at 0.008° resolution on a scale of 0 to 1. Estimates for each quarter were filtered to determine habitat suitability in areas that have soft substrate. A comparison of these estimates for each quarter are provided below (Fig. 12, Fig. 13, Fig. 14, Fig. 15). Additionally, variable importance was determined for each quarter and is shown in tables (Table 4, Table 5, Table 6, Table 7). The estimates of habitat suitability generally showed higher habitat suitability near the coast that was variable throughout the year. The variable importance showed that depth was primarily driving habitat suitability with nitrogen concentration being the second most important. \n\n\n\nFigure 12: Habitat suitability quarter 1. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 4: Variable importance of habitat suitability in quarter 1.\n\n\n\n\n\nFigure 13: Habitat suitability quarter 2. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 5: Variable importance of habitat suitability in quarter 2.\n\n\n\n\n\nFigure 14: Habitat suitability in quarter 3. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 6: Variable importance of habitat suitability in quarter 3.\n\n\n\n\n\nFigure 15: Habitat suitability quarter 4. Estimates of habitat suitability for kelp. Full results are shown on the left and results filtered to soft substrate are shown on the right.\n\n\n\n\n\nTable 7: Variable importance of habitat suitability in quarter 4.\n\n\n\n\nData pipeline\nIn order to make this project reproducible and easy for the clients to continue in the future, all of the code used to create each data product and the model results is provided in an open source GitHub repository (link). To aid others in navigating the project a comprehensive user guide (link) and project schematic were also created and made publicly available. The combination of the GitHub repository, user guide, and project schematic are the data pipeline that will make it possible for future users to use and build upon the project."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-testing",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#summary-of-testing",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Summary of Testing",
    "text": "Summary of Testing\n\nData Testing\nTests were performed to ensure that each raster or shape met the following criteria after completing the data cleaning process established in the project repository:\n\n\n\n\n\n\n\nData type\nTest\n\n\n\n\nRaster\nCRS = WGS84\n\n\nRaster\nExtent = (xmin = -120.65, xmax = -118.80, ymin = 33.85, ymax = 34.59)\n\n\nRaster\nResolution = 0.008 x 0.008\n\n\nRaster\nOrigin = -0.002, 0.002\n\n\nRaster\nSST between 0° and 100° Celsius\n\n\nRaster\nNutrients >= 0\n\n\nVector\nCRS = WGS84\n\n\nVector\nAt least one data point within (xmin = -120.65, xmax = -118.80, ymin = 33.85, ymax = 34.59)\n\n\nVector\nSST, between 0° and 100° Celsius\n\n\nVector\nNutrients >= 0\n\n\n\nThese tests ensure the success of the data cleaning process and provide a way for future users to check that any data they update is compatible with the synthesized data set and existing code. \n\n\nModel \nThe root mean squared error (RMSE) of each interpolated layer used as inputs to the model was calculated to compare to the RMSE of the underlying data. The performance of the species distribution model was tested using the checkerboard 2 spatial partitioning method with the ENMeval package in R. This allowed the giant kelp occurrence data to be split into training and test data sets within the modeling process."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#user-documentaion",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#user-documentaion",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "User Documentaion",
    "text": "User Documentaion\nAll code used in this project was documented in R script and RMarkdown files. Each file contains information such as a description of the purpose of the code, the source of the data files used, and thorough code comments explaining each operation. Folders in our GitHub repository contain README.txt files with brief overviews of the contents within. \nRelevant metadata for each raw data file were compiled and used to create README.txt files for each data set used. Metadata includes information on the abstract, methods, spatial coverage/resolution, temporal coverage/resolution, descriptions for variables used, links to the original data source, and contact information for associated researchers. Attribute descriptions for each data set when applicable or useful were compiled and also added to the applicable README.txt file. For raw data sets, we did not include attribute information, as in most cases it could be found by looking at the original data sources and often there were many attributes not used in the scope of our project. A similar process was completed for all intermediate and analysis data created throughout the project. README.txt files are included in each data-related folder when applicable.\nA README.md for the overall project repository contains an overview of key information and summarizes how to use the repository. A user guide, in both pdf and txt format, is available here). This guide describes how the raw data types were accessed, cleaned, synthesized and prepared for modeling. It also provides guidance for users to add their own data and prepare it for Maxent modeling if they wish. Moreover, it describes the process for synthesizing the final data set and generating the final model and visualizations. This will ensure that future users will be able to incorporate their own data into the data set and explore their own models."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#archive-access",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#archive-access",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Archive Access",
    "text": "Archive Access\nAll of the data sets used in this project are open-source and available for public use through each research project, organization, and agency website, API, or website. The data products created through this project are also publicly available on Google Drive at this link. To facilitate data sharing and reuse, detailed README files with access and functionality information across all of our files were included. The final synthesized and standardized data sets, as well as the species distribution model outputs, were made available in both GeoTIFF and CSV formats. All code used to create the datasets and model outputs is available on the project’s GitHub repository available here. This combined with a comprehensive user guide and project schematic will make this project easily reproducible and flexible for future users. \nFurthermore, the final product and data set were published under the Creative Commons Zero (CC0) intellectual property laws to enable public use. This was done to promote transparency, facilitate data sharing, and enable external users, such as researchers, stakeholders, and kelp farmers, to access and use our project’s data."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#acknowledgements",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#acknowledgements",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe acknowledge the Bren School of Environmental Science & Management’s Master’s of Environmental Data Science program at the University of California Santa Barbara for funding this synthesis project. We would also like to express our gratitude to our clients, Courtney Schatzman from Ocean Rainforest and Natalie Dornan from UCSB interdepartmental Graduate Program in Marine Science (IGPMS), as well as Sidney Gerst and Kirby Bartlett for providing user information, Jeff Massen for user testing and being the main contact as a kelp farmer, and Daphne Virlar-Knight from NCEAS for helping us explore different options for MaxEnt modeling. We are also grateful to Tamma Carleton, Shubhi Sharma and Kevin Winner for their expertise and guidance in statistics and species distribution modeling. Lastly, we are grateful to Dr. Li Kui and Carrie Bretz for their assistance in accessing and processing data from the SBC LTER and Seafloor Mapping Lab at California State University, Monterey Bay."
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#references",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#references",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "References",
    "text": "References\nBell, T., Cavanaugh, K., Reuman, D., Castorani, M., Sheppard, L., & Walter, J. (2021). SBC LTER: REEF: Macrocystis pyrifera biomass and environmental drivers in southern and central California (Version 1) [Data set]. Environmental Data. https://doi.org/10.6073/pasta/27e795dee803493140d6a7cdc3d23379\nBell, T., Cavanaugh, K., & Siegel, D. (2023). SBC LTER: Time series of quarterly NetCDF files of kelp biomass in the canopy from Landsat 5, 7 and 8, since 1984 (ongoing) (Version 20) [Data set]. Environmental Data Initiative. https://doi.org/10.6073/pasta/41f330ccf66fa8c05fc851862e69b1da\nBrzezinksi, M., Reed, D., Harrer, S., Rassweiler, A., Melack, J., Goodridge, B., & Dugan, J. (2013). Multiple Sources and Forms of Nitrogen Sustain Year-Round Kelp Growth on the Inner Continental Shelf of the Santa Barbara Channel. Oceanography, 26(3), 114–123. https://doi.org/10.5670/oceanog.2013.53\nBuschmann, A., Graham, M., & Vásquez, J. (2007). Global Ecology of the Giant Kelp Macrocystis (pp. 39–88). https://doi.org/10.1201/9781420050943.ch2\nCalCOFI Bottle Database. (n.d.). [Data set]. Retrieved June 8, 2023, from https://calcofi.org/data/oceanographic-data/bottle-database/\nCavanaugh, K. C., Bell, T., Costa, M., Eddy, N. E., Gendall, L., Gleason, M. G., Hessing-Lewis, M., Martone, R., McPherson, M., Pontier, O., Reshitnyk, L., Beas-Luna, R., Carr, M., Caselle, J. E., Cavanaugh, K. C., Flores Miller, R., Hamilton, S., Heady, W. N., Hirsh, H. K., … Schroeder, S. B. (2021). A Review of the Opportunities and Challenges for Using Remote Sensing for Management of Surface-Canopy Forming Kelps. Frontiers in Marine Science, 8. https://www.frontiersin.org/articles/10.3389/fmars.2021.753531\nCavanaugh, K. C., Reed, D. C., Bell, T. W., Castorani, M. C. N., & Beas-Luna, R. (2019). Spatial Variability in the Resistance and Resilience of Giant Kelp in Southern and Baja California to a Multiyear Heatwave. Frontiers in Marine Science, 6. https://www.frontiersin.org/articles/10.3389/fmars.2019.00413\nCuba, D., Guardia-Luzon, K., Cevallos, B., Ramos-Larico, S., Neira, E., Pons, A., & Avila-Peltroche, J. (2022). Ecosystem Services Provided by Kelp Forests of the Humboldt Current System: A Comprehensive Review. Coasts, 2(4), 259–277. https://doi.org/10.3390/coasts2040013\nElith, J., Phillips, S. J., Hastie, T., Dudík, M., Chee, Y. E., & Yates, C. J. (2011). A statistical explanation of MaxEnt for ecologists. Diversity and Distributions, 17(1), 43–57. https://doi.org/10.1111/j.1472-4642.2010.00725.x\nERI. (n.d.). Plumes & Blooms [Data set]. Retrieved June 8, 2023, from http://www.oceancolor.ucsb.edu/plumes_and_blooms/\nEsgro, M., & Ray, J. (2021). For Protecting and Restoring California’s Kelp Forests (p. 19).\nGolden, N. E. (2013). California State Waters Map Series Data Catalog: U.S. Geological Survey Data Series 781 [Data set]. U.S. Geological Survey. https://doi.org/10.3133/ds781\nJPL MUR MEaSUREs Project. (2015). GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (4.1) [Data set]. https://doi.org/10.5067/GHGMR-4FJ04\nKass, J. M., Pinilla-Buitrago, G. E., Paz, A., Johnson, B. A., Grisales-Betancur, V., Meenan, S. I., Attali, D., Broennimann, O., Galante, P. J., Maitner, B. S., Owens, H. L., Varela, S., Aiello-Lammens, M. E., Merow, C., Blair, M. E., & Anderson, R. P. (2023). wallace 2: A shiny app for modeling species niches and distributions redesigned to facilitate expansion via module contributions. Ecography, 2023(3), e06547. https://doi.org/10.1111/ecog.06547\nKerrison, P. D., Stanley, M. S., Edwards, M. D., Black, K. D., & Hughes, A. D. (2015). The cultivation of European kelp for bioenergy: Site and species selection. Biomass & Bioenergy, 80, 229–242. https://doi.org/10.1016/j.biombioe.2015.04.035\nMelo-Merino, S. M., Reyes-Bonilla, H., & Lira-Noriega, A. (2020). Ecological niche models and species distribution models in marine environments: A literature review and spatial analysis of evidence. Ecological Modelling, 415, 108837. https://doi.org/10.1016/j.ecolmodel.2019.108837\nNearshore Benthic Habitat GIS for the Channel Islands Volume II - Mapped Areas. (n.d.). [Data set]. Retrieved June 8, 2023, from https://pubs.usgs.gov/of/2005/1170/catalog.html\nNOAA National Centers for Environmental Information. (2022). ETOPO 2022 15 Arc-Second Global Relief Model. NOAA National Centers for Environmental Information [Data set]. DOI: https://doi.org/10.25921/fd45-gt74\nOpenAI. “ChatGPT.” https://openai.com/blog/chat-gpt/\nPeters, J. R., Reed, D. C., & Burkepile, D. E. (2019). Climate and fishing drive regime shifts in consumer-mediated nutrient cycling in kelp forests. Global Change Biology, 25(9), 3179–3192. https://doi.org/10.1111/gcb.14706\nPhillips, S. J., Anderson, R. P., Dudík, M., Schapire, R. E., & Blair, M. E. (2017). Opening the black box: An open-source release of Maxent. Ecography, 40(7), 887–893. https://doi.org/10.1111/ecog.03049\nProuty, N. G., & Baker, M. C. (2020a). CTD profiles and discrete water-column measurements collected off California and Oregon during NOAA cruise RL-19-05 (USGS field activity 2019-672-FA) from October to November 2019 (ver. 2.0, July 2022) [Data set]. U.S. Geological Survey. https://doi.org/10.5066/P9JKYWQU\nProuty, N. G., & Baker, M. C. (2020b). CTD profiles and discrete water-column measurements collected off California and Oregon during NOAA cruise SH-18-12 (USGS field activity 2018-663-FA) from October to November 2018 (ver. 3.0, July 2022) [Data set]. U.S. Geological Survey. https://doi.org/10.5066/P99MJ096\nRogers-Bennett, L., & Catton, C. A. (2019). Marine heat wave and multiple stressors tip bull kelp forest to sea urchin barrens. Scientific Reports, 9(1), Article 1. https://doi.org/10.1038/s41598-019-51114-y\nSeafloor Mapping Lab at CSUMB: Data Library Southern California Data (Part II). (n.d.). [Data set]. Retrieved June 8, 2023, fromhttp://seafloor.otterlabs.org/SFMLwebDATA_s.htm\nWashburn, L., Brzezinksi, M., Carlson, C., & Siegel, D. (2022). SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000 (Version 27) [Data set]. Environmental Data Initiative. https://doi.org/10.6073/pasta/8b74750eed1af2b987e02b4b466e12e7\nWatt, D. (2018, October 9). Preparing Data for MaxEnt Species Distribution Modeling Using R. Azavea. https://www.azavea.com/blog/2018/10/09/preparing-data-for-maxent-species-distribution-modeling-using-r/\nWernberg, T., Smale, D. A., Tuya, F., Thomsen, M. S., Langlois, T. J., de Bettignies, T., Bennett, S., & Rousseaux, C. S. (2013). An extreme climatic event alters marine ecosystem structure in a global biodiversity hotspot. Nature Climate Change, 3(1), Article 1. https://doi.org/10.1038/nclimate1627"
  },
  {
    "objectID": "posts/2023-07-02-kelpGeoMod/index.html#appendix-i-supplemental-figures-and-tables",
    "href": "posts/2023-07-02-kelpGeoMod/index.html#appendix-i-supplemental-figures-and-tables",
    "title": "Developing a Data Pipeline for Kelp Forest Modeling",
    "section": "Appendix I: Supplemental Figures and Tables",
    "text": "Appendix I: Supplemental Figures and Tables\nTable S1. Summary of key information related to the raw data sets used\n\n\n\nData set name\nSource\nVariable\nOriginal Format\nFinal Format(s)\n\n\n\n\n\n\nFile\nResolution\nFile\nResolution\n\n\nSBC LTER: Time series of quarterly NetCDF files of kelp biomass in the canopy from Landsat 5, 7 and 8, since 1984 (ongoing)\nSanta Barbara Coastal Long Term Ecological Research\nKelp area/biomass\nnetCDF\n\n\n30 m x 30 m\nQuarterly\n\n\nSBC LTER: REEF Macrocystis pyrifera biomass and environmental drivers in southern and central California\nSanta Barbara Coastal Long Term Ecological Research\nNitrate\nCSV\n\n\nPoints\nQuarterly\n\n\nSBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles\nSanta Barbara Coastal Long Term Ecological Research\nNitrate + nitrite,\nPhosphate, Ammonium\n\n\nText\n\n\nPoints\nMonthly\n\n\nETOPO Global Relief Model 2022 (Bedrock 15 arcseconds)\nNational Oceanic and Atmospheric Administration\nOcean depth,\nGeoTIFF\n15 arcseconds\n\n\nWater-column environmental variables and accompanying discrete CTD measurements collected off California and Oregon during NOAA cruise SH-18-12\nUnited States Geological Survey\nNitrate + Nitrite, phosphate\n\n\nCSV\n\n\nPoints\nAnnual (measurements taken in fall)\n\n\nCalifornia Cooperative Oceanic Fisheries Investigations – Bottle Database\nCalifornia Cooperative Oceanic Fisheries Investigations\nNitrate, nitrite,\nNItrate + Nitrite ammonia, phosphate\nCSV\n\n\nPoints\nQuarterly\n\n\nPlumes and Blooms\nEarth Research Institute\nNitrite,\nNitrate + Nitrite , phosphate\nCSV\n\n\nPoints\nMonthly\n\n\nGHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1)\nNational Aeronautics and Space Administration and The Group for High Resolution Sea Surface Temperature\nSea surface temperature\nnetCDF\n\n\n0.01° x 0.01°\nDaily\n\n\nCalifornia State Waters Map Series Data Catalog\nUnited States Geological Survey\nSubstrate\nShape file\n\n\n\n\nNearshore Benthic Habitat GIS for the Channel Islands National Marine Sanctuary and Southern California Fisheries Reserves Volume II\nUnited States Geological Survey\nSubstrate\nShape file\n\n\n\n\nSouthern California Data\nCalifornia State Mapping Project\nSubstrate\nEsri layer\n\n\n\n2 m x 2 m\n3 m x 3 m\n5 m x 5 m\n\n\nCalifornia County Boundaries\nCalifornia Open Data Portal\nLand boundaries\nShape file\n\n\n\n\n\nTable S2. Capstone deliverables, descriptions and applications\n\n\n\n\n\n\n\n\nDeliverable\nDescription\nFile Name and Format\n\n\n\n\nSynthesized Data Set\nA CSV file containing all of the observed nutrient values from the original data sets, observed SST, and kelp area and biomass of the raster cell that the observation intersects aggregated to year and quarter.\n\nA series of GeoTIFF raster bricks containing kelp area, kelp biomass, SST, and nutrients for each year and quarter at 0.008° resolution.\n\n\nAn estimate of depth at 0.008° resolution as of 2022.\n\nA CSV file containing the values in the GeoTIFF raster bricks with each row representing a cell at one year and quarter.\n\n\n\nobserved-nutrients-synthesized.csv\n\n\n\n\nkelp-area-brick.tif\nkelp-biomass-brick.tif\nnitrate-nitrite-brick.tif\nphosphate-brick.tif\nsst-brick.tif\nammonium-brick.tif\ndepth.tif\n\nfull-synthesized.csv\n\n\nHabitat Suitability Map\nQuarterly estimates of habitat suitability for giant kelp in all substrate types and for soft substrate only.\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\nmaxent-quarter-1-output.tif\n\n\nsubstrate-masked-brick.tif\n\n\n\n\n\n\nFigure S1: Project schematic"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elke Windschitl",
    "section": "",
    "text": "Welcome!\n\n\nA little bit about me:\nI recently earned a Master of Environmental Data Science degree from the Bren School of Environmental Science & Management at the University of California, Santa Barbara. I am passionate about sustainability and conservation, and I hope to build a career using data science tools to solve environmental problems.\n\n\n\nEducation\n\n\nMaster of Environmental Data Science Student\n\n\nUniversity of California Santa Barbara | June 2023\n\n\nBachelor of Science in Biology with Honors\n\n\nIowa State University | May 2020"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A bit more about who I am, what I do, where I’ve been, and where I’m headed\n\n\n\nFull BioHobbiesHomesWhat’s Next\n\n\n\nBiography\n\nI am a recent graduate of the Bren School of Environmental Science & Management at the University of California, Santa Barbara, where I earned a master’s degree in Environmental Data Science (MEDS). I also hold a BS in Biology from the College of Agriculture and Life Sciences at Iowa State University.\nPassionate about leveraging data science solutions to address environmental challenges, I am focused on applying my skills in geospatial analysis, environmental modeling, machine learning, and data visualization to the fields of natural resource management and sustainable food production.\nDuring my time at the Bren School, I completed a capstone project in collaboration with UCSB researchers and Ocean Rainforest Inc., developing a data pipeline for kelp forest modeling to inform restoration and aquaculture efforts. Prior to that, I gained valuable experience working with the Janke Lab and the Iowa Department of Natural Resources, where I assessed new technologies for monitoring local wildlife.\nBeyond my core areas of interest, I am an animal ecology nerd with a deep commitment to ocean conservation, climate solutions, and environmental justice. I am driven to build a career as a data scientist dedicated to finding innovative environmental solutions that contribute to a sustainable future for all.\nWith my interdisciplinary background and strong skill set, I am ready to make an impact in the field of environmental data science, bridging the gap between scientific research, technological advancements, and practical conservation strategies.\n\n\n\n\nMy Hobbies!\n\nWhat do I do outside of the office and classroom? I love hiking, biking, swimming, and generally spending time outdoors. You can frequently catch me reading a book, and it’s most likely a mystery/thriller novel (find me on Goodreads!) I spend a lot of time in the kitchen cooking and baking – I picked up these two hobbies during the COVID-19 pandemic. Last, I dabble in digital illustration, graphic design, and photography. I have been selling prints and stickers since 2017.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Homes\n\nI grew up in the state of Iowa, but I’ve moved around a lot in my adult life! I participated in multiple National Student Exchange programs, and I moved around for internships and fellowships. Check out all the places I’ve lived since I started college in 2016 – size indicates relatively how long I lived there:\n\n\n\n\n\n\nMap made using package leaflet and data from: https://simplemaps.com/data/us-cities\n\n\n\nWhat’s Next for Me?\n\nI am looking to start my career as an environmental data scientist! Broadly, I hope to find a role where I can blend my creative and quantitative talents to contribute to an organization or research group focused natural resource management. I am ideally looking for jobs in Seattle, Santa Barbara, or remote opportunities."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Developing a Data Pipeline for Kelp Forest Modeling\n\n\n\nMEDS\n\n\n\nA Bren capstone project\n\n\n\nErika Egg, Jessica French, Javier Patrón, Elke Windschitl\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Flourish to Visualize Ocean Depth Data\n\n\n\nData Visualization\n\n\n\nA brief tutorial on how to use Flourish to visualize depth data\n\n\n\nElke Windschitl\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Art into Science and Conservation\n\n\n\nArt\n\n\n\nHow I use creative skills in parallel with quantitative tasks\n\n\n\nElke Windschitl\n\n\nMar 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring FPAR and LAI\n\n\n\nMEDS\n\n\nPython\n\n\n\nAn exploration of FPAR and LAI using Google Earth Engine and Python\n\n\n\nElke Windschitl, Erika Egg, Alessandra Vidal Meza\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebating Nudging and AI for Climate\n\n\n\nMEDS\n\n\n\nA short podcast debate by Lewis White and Elke Windschitl\n\n\n\nElke Windschitl, Lewis White\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting to Predict Eel Distribution\n\n\n\nMEDS\n\n\nMachine Learning\n\n\nR\n\n\n\nA species distribution model example with eel\n\n\n\nElke Windschitl\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating the Use of Autonomous Recording Units for Monitoring Northern Bobwhite Coveys\n\n\n\nISU\n\n\n\nA research poster created for an undergraduate Honors capstone project\n\n\n\nElke Windschitl, Adam Janke, Kyla Yuza-Pate\n\n\nDec 1, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  }
]